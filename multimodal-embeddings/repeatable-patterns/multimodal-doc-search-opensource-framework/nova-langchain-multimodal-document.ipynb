{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a0b2a41-bebe-4f3a-b803-eadc275a916f",
   "metadata": {},
   "source": [
    "# Amazon Nova Multimodal Embeddings with LangChain\n",
    "Integrate Amazon Nova multimodal embedding models with LangChain for document processing and retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dc6ae2-b86d-449f-80e1-2a5af7da8dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-core langchain-community boto3 faiss-cpu --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e2067a-23c6-4480-bff5-e83b80423f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import base64\n",
    "from typing import List\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "from IPython.display import Image, display\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8080c0d3-3a50-4f3b-aec0-325744730063",
   "metadata": {},
   "source": [
    "## Custom Nova Embedding Class for LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab678794-538a-432b-9414-054ade37d191",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NovaMultimodalEmbeddings(Embeddings):\n",
    "    def __init__(self, model_id: str = 'amazon.nova-2-multimodal-embeddings-v1:0', region: str = 'us-east-1'):\n",
    "        self.model_id = model_id\n",
    "        self.bedrock = boto3.client(\"bedrock-runtime\", region_name=region)\n",
    "        self.dim = 3072\n",
    "    \n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Embed multiple text documents\"\"\"\n",
    "        embeddings = []\n",
    "        for text in texts:\n",
    "            embedding = self._embed_text(text)\n",
    "            embeddings.append(embedding)\n",
    "        return embeddings\n",
    "    \n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        \"\"\"Embed a single query text\"\"\"\n",
    "        return self._embed_text(text, \"GENERIC_RETRIEVAL\")\n",
    "    \n",
    "    def _embed_text(self, text: str, purpose=\"GENERIC_INDEX\") -> List[float]:\n",
    "        \"\"\"Generate embedding for text\"\"\"\n",
    "        request_body = {\n",
    "            \"taskType\": \"SINGLE_EMBEDDING\",\n",
    "            \"singleEmbeddingParams\": {\n",
    "                \"embeddingPurpose\": purpose,\n",
    "                \"embeddingDimension\": self.dim,\n",
    "                \"text\": {\"truncationMode\": \"NONE\", \"value\": text}\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        response = self.bedrock.invoke_model(\n",
    "            modelId=self.model_id,\n",
    "            body=json.dumps(request_body)\n",
    "        )\n",
    "        \n",
    "        result = json.loads(response['body'].read())\n",
    "        return result[\"embeddings\"][0][\"embedding\"]\n",
    "\n",
    "    def embed_image_query(self, image_path: str) -> List[float]:\n",
    "        \"\"\"Embed a single query image\"\"\"\n",
    "        return self.embed_image(image_path, \"GENERIC_RETRIEVAL\")\n",
    "    \n",
    "    def embed_image(self, image_path: str, purpose=\"GENERIC_INDEX\") -> List[float]:\n",
    "        \"\"\"Embed an image from file path\"\"\"\n",
    "        with open(image_path, 'rb') as f:\n",
    "            image_data = base64.b64encode(f.read()).decode('utf-8')\n",
    "        \n",
    "        request_body = {\n",
    "            \"taskType\": \"SINGLE_EMBEDDING\",\n",
    "            \"singleEmbeddingParams\": {\n",
    "                \"embeddingPurpose\": \"GENERIC_INDEX\",\n",
    "                \"embeddingDimension\": self.dim,\n",
    "                \"image\": {\n",
    "                    \"format\": \"png\",\n",
    "                    \"detailLevel\": \"DOCUMENT_IMAGE\",\n",
    "                    \"source\": {\"bytes\": image_data}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        response = self.bedrock.invoke_model(\n",
    "            modelId=self.model_id,\n",
    "            body=json.dumps(request_body)\n",
    "        )\n",
    "        \n",
    "        result = json.loads(response['body'].read())\n",
    "        return result[\"embeddings\"][0][\"embedding\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c198ee2f-9a6c-48c9-af84-2b8040d5859c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Nova embeddings\n",
    "embeddings = NovaMultimodalEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multimodal-section",
   "metadata": {},
   "source": [
    "## Multimodal Document Processing\n",
    "\n",
    "Demonstrate how to process documents containing both text and images using Nova's multimodal capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multimodal-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "class MultimodalDocument:\n",
    "    def __init__(self, text: str, image_path: str = None, metadata: dict = None):\n",
    "        self.text = text\n",
    "        self.image_path = image_path\n",
    "        self.metadata = metadata or {}\n",
    "    \n",
    "    def has_image(self) -> bool:\n",
    "        return self.image_path is not None and os.path.exists(self.image_path)\n",
    "\n",
    "class MultimodalVectorStore:\n",
    "    def __init__(self, embeddings: NovaMultimodalEmbeddings):\n",
    "        self.embeddings = embeddings\n",
    "        self.text_vectorstore = None\n",
    "        self.image_vectorstore = None\n",
    "        self.documents = []\n",
    "    \n",
    "    def add_documents(self, multimodal_docs: List[MultimodalDocument]):\n",
    "        \"\"\"Add multimodal documents to the vector store\"\"\"\n",
    "        text_docs = []\n",
    "        image_docs = []\n",
    "        \n",
    "        for i, doc in enumerate(multimodal_docs):\n",
    "            self.documents.append(doc)\n",
    "            \n",
    "            # Add text document\n",
    "            text_metadata = {**doc.metadata, \"doc_id\": i, \"type\": \"text\"}\n",
    "            text_docs.append(Document(page_content=doc.text, metadata=text_metadata))\n",
    "            \n",
    "            # Add image document if exists\n",
    "            if doc.has_image():\n",
    "                image_metadata = {**doc.metadata, \"doc_id\": i, \"type\": \"image\", \"image_path\": doc.image_path}\n",
    "                image_docs.append(Document(page_content=f\"Image: {doc.image_path}\", metadata=image_metadata))\n",
    "        \n",
    "        # Create text vector store\n",
    "        if text_docs:\n",
    "            self.text_vectorstore = FAISS.from_documents(text_docs, self.embeddings)\n",
    "        \n",
    "        # Create image vector store\n",
    "        if image_docs:\n",
    "            image_embeddings = [self.embeddings.embed_image(doc.metadata[\"image_path\"]) for doc in image_docs]\n",
    "            self.image_vectorstore = FAISS.from_embeddings(\n",
    "                [(doc.page_content, emb) for doc, emb in zip(image_docs, image_embeddings)],\n",
    "                self.embeddings\n",
    "            )\n",
    "    \n",
    "    def search_text(self, query: str, k: int = 3):\n",
    "        \"\"\"Search text content\"\"\"\n",
    "        if self.text_vectorstore:\n",
    "            return self.text_vectorstore.similarity_search(query, k=k)\n",
    "        return []\n",
    "    \n",
    "    def search_images(self, image_path: str, k: int = 3):\n",
    "        \"\"\"Search similar images\"\"\"\n",
    "        if self.image_vectorstore:\n",
    "            query_embedding = self.embeddings.embed_image_query(image_path)\n",
    "            return self.image_vectorstore.similarity_search_by_vector(query_embedding, k=k)\n",
    "        return []\n",
    "    \n",
    "    def multimodal_search(self, text_query: str = None, image_path: str = None, k: int = 5):\n",
    "        \"\"\"Combined text and image search\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        if text_query:\n",
    "            text_results = self.search_text(text_query, k=k//2 + 1)\n",
    "            results.extend([(doc, \"text\") for doc in text_results])\n",
    "        \n",
    "        if image_path:\n",
    "            image_results = self.search_images(image_path, k=k//2 + 1)\n",
    "            results.extend([(doc, \"image\") for doc in image_results])\n",
    "        \n",
    "        return results[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-sample-images",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample multimodal documents\n",
    "# Note: In a real scenario, you would have actual image files\n",
    "\n",
    "sample_multimodal_docs = [\n",
    "    MultimodalDocument(\n",
    "        text=\"We present Amazon Nova Premier, our most capable multimodal foundation model and teacher for model distillation.\",\n",
    "        image_path= \"./images/nova-premier-pdf-screenshot.png\",\n",
    "        metadata={\"source\": \"nova_premier_model_card\", \"category\": \"ai\"}\n",
    "    ),\n",
    "    MultimodalDocument(\n",
    "        text=\"Vector databases enable semantic search capabilities\",\n",
    "        metadata={\"source\": \"vector_info\", \"category\": \"ai\"}\n",
    "    ),\n",
    "    MultimodalDocument(\n",
    "        text=\"Amazon S3 provides scalable object storage for data backup and archiving.\",\n",
    "        metadata={\"source\": \"aws_s3\", \"category\": \"storage\"}\n",
    "    ),\n",
    "    MultimodalDocument(\n",
    "        text=\"Amazon SageMaker AI is a fully managed machine learning (ML) service. \",\n",
    "        image_path=\"./images/sagemaker-ai-workflow-screenshot.png\",\n",
    "        metadata={\"source\": \"amazon_sagemaker\", \"category\": \"ai\"}\n",
    "    ),\n",
    "    MultimodalDocument(\n",
    "        text=\"Introducing Amazon Nova foundation models: Frontier intelligence and industry leading price performance\",\n",
    "        metadata={\"source\": \"nova_blog\", \"category\": \"ai\"}\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Initialize multimodal vector store\n",
    "multimodal_store = MultimodalVectorStore(embeddings)\n",
    "multimodal_store.add_documents(sample_multimodal_docs)\n",
    "\n",
    "print(\"Multimodal vector store created with text documents\")\n",
    "print(f\"Added {len(sample_multimodal_docs)} multimodal documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multimodal-search-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate text search in multimodal store\n",
    "query = \"Amazon Nova\"\n",
    "text_results = multimodal_store.search_text(query, k=2)\n",
    "\n",
    "print(f\"Text search results for: '{query}'\\n\")\n",
    "for i, doc in enumerate(text_results):\n",
    "    print(f\"{i+1}. {doc.page_content}\")\n",
    "    print(f\"   Category: {doc.metadata.get('category', 'N/A')}\")\n",
    "    print(f\"   Source: {doc.metadata.get('source', 'N/A')}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multimodal-search-section",
   "metadata": {},
   "source": [
    "## Multimodal Search with Image Display\n",
    "\n",
    "Enhanced search functionality that displays retrieved images alongside text results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced-multimodal-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedMultimodalSearch:\n",
    "    def __init__(self, multimodal_store: MultimodalVectorStore):\n",
    "        self.multimodal_store = multimodal_store\n",
    "    \n",
    "    def search_and_display(self, text_query: str = None, image_query_path: str = None, k: int = 3):\n",
    "        \"\"\"Search and display results with images\"\"\"\n",
    "        results = self.multimodal_store.multimodal_search(text_query, image_query_path, k)\n",
    "        \n",
    "        print(f\"=== Multimodal Search Results ===\")\n",
    "        if text_query:\n",
    "            print(f\"Text Query: '{text_query}'\")\n",
    "        if image_query_path:\n",
    "            print(f\"Image Query: {image_query_path}\")\n",
    "            display(Image(filename=image_query_path, width=400))\n",
    "        print(f\"Found {len(results)} results\\n\")\n",
    "        \n",
    "        for i, (doc, search_type) in enumerate(results):\n",
    "            print(f\"--- Result {i+1} [{search_type.upper()}] ---\")\n",
    "            print(f\"Content: {doc.page_content}\")\n",
    "            print(f\"Source: {doc.metadata.get('source', 'N/A')}\")\n",
    "            print(f\"Category: {doc.metadata.get('category', 'N/A')}\")\n",
    "            \n",
    "            # Display image if available\n",
    "            if 'image_path' in doc.metadata:\n",
    "                image_path = doc.metadata['image_path']\n",
    "                try:\n",
    "                    display(Image(filename=image_path, width=400))\n",
    "                except:\n",
    "                    print(f\"Could not display image: {image_path}\")\n",
    "            if \"Image:\" in doc.page_content:\n",
    "                image_path = doc.page_content.replace(\"Image: \", \"\")\n",
    "                try:\n",
    "                    display(Image(filename=image_path, width=400))\n",
    "                except:\n",
    "                    print(f\"Could not display image: {image_path}\")\n",
    "            \n",
    "            # Get original document for additional context\n",
    "            doc_id = doc.metadata.get('doc_id')\n",
    "            if doc_id is not None and doc_id < len(self.multimodal_store.documents):\n",
    "                original_doc = self.multimodal_store.documents[doc_id]\n",
    "                if original_doc.has_image() and 'image_path' not in doc.metadata:\n",
    "                    print(f\"Associated image: {original_doc.image_path}\")\n",
    "                    try:\n",
    "                        display(Image(filename=original_doc.image_path, width=400))\n",
    "                    except:\n",
    "                        print(f\"Could not display image: {original_doc.image_path}\")\n",
    "            \n",
    "            print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demo-enhanced-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create enhanced search instance\n",
    "enhanced_search = EnhancedMultimodalSearch(multimodal_store)\n",
    "\n",
    "# Demo: Search with text query and display associated images\n",
    "enhanced_search.search_and_display(text_query=\"Amazon Nova\", k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demo-image-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_image = \"./images/nova-mlp-pdf-screenshot.png\"\n",
    "if os.path.exists(query_image):\n",
    "    enhanced_search.search_and_display(image_query_path=query_image, k=1)\n",
    "else:\n",
    "    print(f\"Query image not found: {query_image}\")\n",
    "    print(\"Please ensure sample images exist in the ./images/ directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524249ab-8a36-4b25-b495-9019d8470af0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
