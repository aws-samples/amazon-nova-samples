{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a0b2a41-bebe-4f3a-b803-eadc275a916f",
   "metadata": {},
   "source": [
    "# Amazon Nova Multimodal Embeddings with LlamaIndex\n",
    "Integrate Amazon Nova multimodal embedding models with LlamaIndex for document processing and retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dc6ae2-b86d-449f-80e1-2a5af7da8dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index llama-index-vector-stores-faiss boto3 faiss-cpu --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e2067a-23c6-4480-bff5-e83b80423f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import base64\n",
    "from typing import List, Any\n",
    "from llama_index.core.embeddings import BaseEmbedding\n",
    "from llama_index.core import Document, VectorStoreIndex, StorageContext, Settings\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "from llama_index.core.schema import ImageDocument\n",
    "from IPython.display import Image, display\n",
    "import faiss\n",
    "import os\n",
    "\n",
    "# Disable LLM to avoid OpenAI API key requirement\n",
    "Settings.llm = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8080c0d3-3a50-4f3b-aec0-325744730063",
   "metadata": {},
   "source": [
    "## Custom Nova Embedding Class for LlamaIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab678794-538a-432b-9414-054ade37d191",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NovaMultimodalEmbedding(BaseEmbedding):\n",
    "    def __init__(self, model_id: str = 'amazon.nova-2-multimodal-embeddings-v1:0', region: str = 'us-east-1', **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        object.__setattr__(self, 'model_id', model_id)\n",
    "        object.__setattr__(self, 'region', region)\n",
    "        object.__setattr__(self, 'bedrock', boto3.client(\"bedrock-runtime\", region_name=region))\n",
    "        object.__setattr__(self, '_embed_dim', 3072)\n",
    "    \n",
    "    @classmethod\n",
    "    def class_name(cls) -> str:\n",
    "        return \"NovaMultimodalEmbedding\"\n",
    "    \n",
    "    def _get_query_embedding(self, query: str) -> List[float]:\n",
    "        return self._embed_text(query, \"GENERIC_RETRIEVAL\")\n",
    "    \n",
    "    def _get_text_embedding(self, text: str) -> List[float]:\n",
    "        return self._embed_text(text, \"GENERIC_INDEX\")\n",
    "    \n",
    "    async def _aget_query_embedding(self, query: str) -> List[float]:\n",
    "        return self._get_query_embedding(query)\n",
    "    \n",
    "    async def _aget_text_embedding(self, text: str) -> List[float]:\n",
    "        return self._get_text_embedding(text)\n",
    "    \n",
    "    def _embed_text(self, text: str, purpose: str = \"GENERIC_INDEX\") -> List[float]:\n",
    "        request_body = {\n",
    "            \"taskType\": \"SINGLE_EMBEDDING\",\n",
    "            \"singleEmbeddingParams\": {\n",
    "                \"embeddingPurpose\": purpose,\n",
    "                \"embeddingDimension\": self._embed_dim,\n",
    "                \"text\": {\"truncationMode\": \"NONE\", \"value\": text}\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        response = self.bedrock.invoke_model(\n",
    "            modelId=self.model_id,\n",
    "            body=json.dumps(request_body)\n",
    "        )\n",
    "        \n",
    "        result = json.loads(response['body'].read())\n",
    "        return result[\"embeddings\"][0][\"embedding\"]\n",
    "    \n",
    "    def embed_image(self, image_path: str, purpose: str = \"GENERIC_INDEX\") -> List[float]:\n",
    "        with open(image_path, 'rb') as f:\n",
    "            image_data = base64.b64encode(f.read()).decode('utf-8')\n",
    "        \n",
    "        request_body = {\n",
    "            \"taskType\": \"SINGLE_EMBEDDING\",\n",
    "            \"singleEmbeddingParams\": {\n",
    "                \"embeddingPurpose\": purpose,\n",
    "                \"embeddingDimension\": self._embed_dim,\n",
    "                \"image\": {\n",
    "                    \"format\": \"png\",\n",
    "                    \"detailLevel\": \"DOCUMENT_IMAGE\",\n",
    "                    \"source\": {\"bytes\": image_data}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        response = self.bedrock.invoke_model(\n",
    "            modelId=self.model_id,\n",
    "            body=json.dumps(request_body)\n",
    "        )\n",
    "        \n",
    "        result = json.loads(response['body'].read())\n",
    "        return result[\"embeddings\"][0][\"embedding\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c198ee2f-9a6c-48c9-af84-2b8040d5859c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Nova embeddings\n",
    "embeddings = NovaMultimodalEmbedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multimodal-section",
   "metadata": {},
   "source": [
    "## Multimodal Document Processing with LlamaIndex\n",
    "\n",
    "Create separate indexes for text and image content using LlamaIndex's native document types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multimodal-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalLlamaIndex:\n",
    "    def __init__(self, embedding_model: NovaMultimodalEmbedding):\n",
    "        self.embedding_model = embedding_model\n",
    "        self.text_index = None\n",
    "        self.image_index = None\n",
    "        self.documents = []\n",
    "    \n",
    "    def add_documents(self, text_docs: List[str], image_paths: List[str] = None, metadata_list: List[dict] = None):\n",
    "        \"\"\"Add text and image documents to separate indexes\"\"\"\n",
    "        # Create text documents\n",
    "        documents = []\n",
    "        for i, text in enumerate(text_docs):\n",
    "            metadata = metadata_list[i] if metadata_list and i < len(metadata_list) else {}\n",
    "            metadata[\"doc_id\"] = i\n",
    "            doc = Document(text=text, metadata=metadata)\n",
    "            documents.append(doc)\n",
    "        \n",
    "        # Create text index\n",
    "        faiss_index = faiss.IndexFlatIP(self.embedding_model._embed_dim)\n",
    "        vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "        storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "        self.text_index = VectorStoreIndex.from_documents(\n",
    "            documents, storage_context=storage_context, embed_model=self.embedding_model\n",
    "        )\n",
    "        \n",
    "        # Create image index if images provided\n",
    "        if image_paths:\n",
    "            image_documents = []\n",
    "            for i, img_path in enumerate(image_paths):\n",
    "                if os.path.exists(img_path):\n",
    "                    metadata = metadata_list[i] if metadata_list and i < len(metadata_list) else {}\n",
    "                    metadata[\"doc_id\"] = i\n",
    "                    metadata[\"image_path\"] = img_path\n",
    "                    # Create embedding manually for images\n",
    "                    embedding = self.embedding_model.embed_image(img_path)\n",
    "                    doc = Document(text=f\"Image: {img_path}\", metadata=metadata, embedding=embedding)\n",
    "                    image_documents.append(doc)\n",
    "            \n",
    "            if image_documents:\n",
    "                faiss_index_img = faiss.IndexFlatIP(self.embedding_model._embed_dim)\n",
    "                vector_store_img = FaissVectorStore(faiss_index=faiss_index_img)\n",
    "                storage_context_img = StorageContext.from_defaults(vector_store=vector_store_img)\n",
    "                self.image_index = VectorStoreIndex.from_documents(\n",
    "                    image_documents, storage_context=storage_context_img, embed_model=self.embedding_model\n",
    "                )\n",
    "    \n",
    "    def search_text(self, query: str, k: int = 3):\n",
    "        \"\"\"Search text content\"\"\"\n",
    "        if self.text_index:\n",
    "            retriever = self.text_index.as_retriever(similarity_top_k=k)\n",
    "            return retriever.retrieve(query)\n",
    "        return []\n",
    "    \n",
    "    def search_images(self, image_path: str, k: int = 3):\n",
    "        \"\"\"Search similar images using image query\"\"\"\n",
    "        if self.image_index and os.path.exists(image_path):\n",
    "            query_embedding = self.embedding_model.embed_image(image_path, \"GENERIC_RETRIEVAL\")\n",
    "            retriever = self.image_index.as_retriever(similarity_top_k=k)\n",
    "            # Use text query as placeholder since we're using embedding directly\n",
    "            return retriever.retrieve(f\"Image query: {image_path}\")\n",
    "        return []\n",
    "    \n",
    "    def multimodal_search(self, text_query: str = None, image_path: str = None, k: int = 5):\n",
    "        \"\"\"Combined text and image search\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        if text_query and self.text_index:\n",
    "            text_results = self.search_text(text_query, k//2 + 1)\n",
    "            results.extend([(node, \"text\") for node in text_results])\n",
    "        \n",
    "        if image_path and self.image_index:\n",
    "            image_results = self.search_images(image_path, k//2 + 1)\n",
    "            results.extend([(node, \"image\") for node in image_results])\n",
    "        \n",
    "        return results[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-sample-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data\n",
    "sample_texts = [\n",
    "    \"We present Amazon Nova Premier, our most capable multimodal foundation model and teacher for model distillation.\",\n",
    "    \"Vector databases enable semantic search capabilities\",\n",
    "    \"Amazon S3 provides scalable object storage for data backup and archiving.\",\n",
    "    \"Amazon SageMaker AI is a fully managed machine learning (ML) service.\",\n",
    "    \"Introducing Amazon Nova foundation models: Frontier intelligence and industry leading price performance\"\n",
    "]\n",
    "\n",
    "sample_images = [\n",
    "    \"./images/nova-premier-pdf-screenshot.png\",\n",
    "    None,  # No image for vector database text\n",
    "    None,  # No image for S3 text\n",
    "    \"./images/sagemaker-ai-workflow-screenshot.png\",\n",
    "    None   # No image for Nova blog text\n",
    "]\n",
    "\n",
    "sample_metadata = [\n",
    "    {\"source\": \"nova_premier_model_card\", \"category\": \"ai\"},\n",
    "    {\"source\": \"vector_info\", \"category\": \"ai\"},\n",
    "    {\"source\": \"aws_s3\", \"category\": \"storage\"},\n",
    "    {\"source\": \"amazon_sagemaker\", \"category\": \"ai\"},\n",
    "    {\"source\": \"nova_blog\", \"category\": \"ai\"}\n",
    "]\n",
    "\n",
    "# Filter out None images\n",
    "valid_images = [img for img in sample_images if img is not None]\n",
    "\n",
    "# Initialize multimodal index\n",
    "multimodal_index = MultimodalLlamaIndex(embeddings)\n",
    "multimodal_index.add_documents(sample_texts, valid_images, sample_metadata)\n",
    "\n",
    "print(f\"Created LlamaIndex with {len(sample_texts)} text documents and {len(valid_images)} image documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "text-search-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate text search\n",
    "query = \"Amazon Nova\"\n",
    "text_results = multimodal_index.search_text(query, k=2)\n",
    "\n",
    "print(f\"Text search results for: '{query}'\\n\")\n",
    "for i, node in enumerate(text_results):\n",
    "    print(f\"{i+1}. {node.text}\")\n",
    "    print(f\"   Score: {node.score:.4f}\")\n",
    "    print(f\"   Category: {node.metadata.get('category', 'N/A')}\")\n",
    "    print(f\"   Source: {node.metadata.get('source', 'N/A')}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demo-query-engine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Use LlamaIndex query engine for natural language responses\n",
    "if multimodal_index.text_index:\n",
    "    query_engine = multimodal_index.text_index.as_query_engine(llm=None)\n",
    "    response = query_engine.query(\"What is Amazon Nova?\")\n",
    "    print(f\"Query: What is Amazon Nova?\")\n",
    "    print(f\"Response: {response}\")\n",
    "    print(f\"Source nodes: {len(response.source_nodes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced-search-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedLlamaIndexSearch:\n",
    "    def __init__(self, multimodal_index: MultimodalLlamaIndex):\n",
    "        self.multimodal_index = multimodal_index\n",
    "    \n",
    "    def search_and_display(self, text_query: str = None, image_query_path: str = None, k: int = 3):\n",
    "        \"\"\"Search and display results with images\"\"\"\n",
    "        results = self.multimodal_index.multimodal_search(text_query, image_query_path, k)\n",
    "        \n",
    "        print(f\"=== LlamaIndex Multimodal Search Results ===\")\n",
    "        if text_query:\n",
    "            print(f\"Text Query: '{text_query}'\")\n",
    "        if image_query_path:\n",
    "            print(f\"Image Query: {image_query_path}\")\n",
    "            if os.path.exists(image_query_path):\n",
    "                display(Image(filename=image_query_path, width=400))\n",
    "        print(f\"Found {len(results)} results\\n\")\n",
    "        \n",
    "        for i, (node, search_type) in enumerate(results):\n",
    "            print(f\"--- Result {i+1} [{search_type.upper()}] ---\")\n",
    "            print(f\"Content: {node.text}\")\n",
    "            print(f\"Score: {node.score:.4f}\")\n",
    "            print(f\"Source: {node.metadata.get('source', 'N/A')}\")\n",
    "            print(f\"Category: {node.metadata.get('category', 'N/A')}\")\n",
    "            \n",
    "            # Display image if available\n",
    "            if 'image_path' in node.metadata:\n",
    "                image_path = node.metadata['image_path']\n",
    "                if os.path.exists(image_path):\n",
    "                    print(f\"Associated image: {image_path}\")\n",
    "                    display(Image(filename=image_path, width=400))\n",
    "                else:\n",
    "                    print(f\"Image not found: {image_path}\")\n",
    "            \n",
    "            print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demo-enhanced-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create enhanced search instance\n",
    "enhanced_search = EnhancedLlamaIndexSearch(multimodal_index)\n",
    "\n",
    "# Demo: Search with text query and display associated images\n",
    "enhanced_search.search_and_display(text_query=\"Amazon Nova\", k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demo-image-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Image-based search if images exist\n",
    "query_image = \"./images/nova-mlp-pdf-screenshot.png\"\n",
    "if os.path.exists(query_image):\n",
    "    enhanced_search.search_and_display(image_query_path=query_image, k=1)\n",
    "else:\n",
    "    print(f\"Query image not found: {query_image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05a2553-70b3-40ea-bdcf-61852be5fd87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
