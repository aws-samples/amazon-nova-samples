{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-lingual News Event Clustering with Amazon Nova\n",
    "\n",
    "This solution demonstrates how to cluster related news stories from multiple languages into coherent events using Amazon Nova Multimodal Embedding model. The notebook processes a diverse dataset of news articles in German, Spanish, and English, automatically grouping similar stories and generating meaningful event summaries.\n",
    "\n",
    "## Key Features:\n",
    "- **Multi-language support**: Handles news articles in German (deu), Spanish (spa), and English (eng)\n",
    "- **Semantic clustering**: Uses Amazon Nova embeddings to group semantically similar articles\n",
    "- **Automated summarization**: Generates event descriptions using Amazon Nova's language model\n",
    "- **Visual analysis**: Provides 2D visualization of article clusters and event distributions\n",
    "\n",
    "## Use Cases:\n",
    "- News aggregation and organization\n",
    "- Multi-lingual content analysis\n",
    "- Event detection and monitoring\n",
    "- Information overload reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from datetime import datetime, timedelta\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsEventProcessor:\n",
    "    def __init__(self, region_name=\"us-east-1\"):\n",
    "        self.bedrock = boto3.client('bedrock-runtime', region_name=region_name)\n",
    "        self.embedding_model = 'amazon.nova-2-multimodal-embeddings-v1:0'\n",
    "        self.llm_model = \"us.amazon.nova-lite-v1:0\"\n",
    "        \n",
    "    def get_embedding(self, text):\n",
    "        request_body = {\n",
    "            \"taskType\": \"SINGLE_EMBEDDING\",\n",
    "            \"singleEmbeddingParams\": {\n",
    "                \"embeddingDimension\": 1024,\n",
    "                \"embeddingPurpose\": \"CLUSTERING\",\n",
    "                \"text\": {\"truncationMode\": \"END\", \"value\": text}\n",
    "            }\n",
    "        }\n",
    "        response = self.bedrock.invoke_model(modelId=self.embedding_model, body=json.dumps(request_body))\n",
    "        return np.array(json.loads(response['body'].read())[\"embeddings\"][0][\"embedding\"])\n",
    "    \n",
    "    def get_batch_embeddings(self, texts):\n",
    "        return np.array([self.get_embedding(text) for text in texts])\n",
    "    \n",
    "    def generate_event_summary(self, articles):\n",
    "        sample_text = \"\\n\\n\".join([article[:300] for article in articles[:3]])\n",
    "        prompt = f\"\"\"Analyze these news articles and extract the main topic category and description.\n",
    "\n",
    "Articles:\n",
    "{sample_text}\"\"\"\n",
    "        \n",
    "        tools = [{\n",
    "            \"toolSpec\": {\n",
    "                \"name\": \"extract_topic\",\n",
    "                \"description\": \"Extract topic category and description from news articles\",\n",
    "                \"inputSchema\": {\n",
    "                    \"json\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"category\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Main topic category (max 3 words)\"\n",
    "                            },\n",
    "                            \"description\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Brief description (1-2 sentences)\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"category\", \"description\"]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }]\n",
    "        \n",
    "        request_body = {\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": [{\"text\": prompt}]}],\n",
    "            \"toolConfig\": {\"tools\": tools, \"toolChoice\": {\"tool\": {\"name\": \"extract_topic\"}}},\n",
    "            \"inferenceConfig\": {\"maxTokens\": 150, \"temperature\": 0.1}\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = self.bedrock.invoke_model(modelId=self.llm_model, body=json.dumps(request_body))\n",
    "            result = json.loads(response['body'].read())\n",
    "            tool_use = result['output']['message']['content'][0]['toolUse']\n",
    "            return tool_use['input']\n",
    "        except:\n",
    "            return {\"category\": \"Unknown Topic\", \"description\": \"Unable to analyze articles\"}\n",
    "\n",
    "processor = NewsEventProcessor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load News Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load public dataset\n",
    "# Dataset source: https://github.com/aws-samples/news-clustering-and-summarization/tree/main\n",
    "print(\"Loading public dataset...\")\n",
    "with open('public_dataset.json', 'r', encoding='utf-8') as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "# Extract articles and create categories from cluster information\n",
    "articles = [item['text'] for item in dataset[:100]]  # Limit for demo\n",
    "categories = [f\"Cluster_{item['cluster']}\" for item in dataset[:100]]\n",
    "\n",
    "# Create dataset table\n",
    "df = pd.DataFrame({\n",
    "    'Article_ID': [item['id'] for item in dataset[:100]],\n",
    "    'Title': [item['title'] for item in dataset[:100]],\n",
    "    'Category': categories,\n",
    "    'Language': [item['lang'] for item in dataset[:100]],\n",
    "    'Length': [len(item['text']) for item in dataset[:100]],\n",
    "    'Preview': [item['text'][:150] + '...' if len(item['text']) > 150 else item['text'] for item in dataset[:100]]\n",
    "})\n",
    "\n",
    "print(f\"Loaded {len(articles)} articles\")\n",
    "print(f\"Languages: {sorted(set([item['lang'] for item in dataset[:100]]))}\")\n",
    "print(f\"Unique clusters: {len(set(categories))}\")\n",
    "print(\"\\nDataset Overview:\")\n",
    "display(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings and cluster\n",
    "print(\"Generating embeddings...\")\n",
    "embeddings = processor.get_batch_embeddings(articles)\n",
    "\n",
    "n_events = len(set(categories))\n",
    "kmeans = KMeans(n_events, random_state=42)\n",
    "event_labels = kmeans.fit_predict(embeddings)\n",
    "event_sizes = Counter(event_labels)\n",
    "\n",
    "print(f\"Detected {n_events} topics: {dict(event_sizes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Event Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create event summaries\n",
    "event_summaries = {}\n",
    "for event_id in range(n_events):\n",
    "    event_articles = [articles[i] for i in range(len(articles)) if event_labels[i] == event_id]\n",
    "    if event_articles:\n",
    "        summary = processor.generate_event_summary(event_articles)\n",
    "        event_summaries[event_id] = summary\n",
    "        print(f\"Event {event_id}: {summary['category']} ({len(event_articles)} articles)\")\n",
    "        print(f\"  Description: {summary['description']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize events by category\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, n_events))\n",
    "\n",
    "# Event clusters by category\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "embeddings_2d = pca.fit_transform(embeddings)\n",
    "\n",
    "for event_id in range(n_events):\n",
    "    mask = event_labels == event_id\n",
    "    category = event_summaries[event_id]['category'] if event_id in event_summaries else f\"Event {event_id}\"\n",
    "    ax1.scatter(embeddings_2d[mask, 0], embeddings_2d[mask, 1], \n",
    "               c=[colors[event_id]], label=category, alpha=0.7, s=60)\n",
    "\n",
    "ax1.set_title('News Event Clusters by Category')\n",
    "ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Category distribution\n",
    "categories_list = [event_summaries[i]['category'] for i in range(n_events) if i in event_summaries]\n",
    "event_counts = [event_sizes[i] for i in range(n_events)]\n",
    "\n",
    "bars = ax2.bar(range(len(categories_list)), event_counts[:len(categories_list)], color=colors[:len(categories_list)])\n",
    "ax2.set_title('Event Coverage by Category')\n",
    "ax2.set_xlabel('Categories')\n",
    "ax2.set_ylabel('Articles')\n",
    "ax2.set_xticks(range(len(categories_list)))\n",
    "ax2.set_xticklabels(categories_list, rotation=45, ha='right')\n",
    "\n",
    "for bar, count in zip(bars, event_counts[:len(categories_list)]):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "             str(count), ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final report with structured data\n",
    "event_data = [(i, event_sizes[i], event_summaries.get(i, {})) for i in range(n_events)]\n",
    "event_data.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"NEWS EVENT SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total articles: {len(articles)}\")\n",
    "print(f\"Events detected: {n_events}\")\n",
    "\n",
    "print(\"\\nTOP EVENTS BY CATEGORY:\")\n",
    "for rank, (event_id, count, summary) in enumerate(event_data, 1):\n",
    "    if summary:\n",
    "        category = summary.get('category', f'Event {event_id}')\n",
    "        description = summary.get('description', 'No description available')\n",
    "        print(f\"\\n{rank}. {category.upper()} ({count} articles)\")\n",
    "        print(f\"   📝 {description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
