{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Nova LLM-as-a-Judge Evaluation with Amazon SageMaker\n",
    "\n",
    "This notebook demonstrates how to use Amazon Nova LLM-as-a-Judge methodology to evaluate and compare the outputs of two different large language models using Amazon SageMaker Training Jobs. We'll compare responses from a Qwen2.5 1.5B model deployed on SageMaker against Claude 3.7 Sonnet via Amazon Bedrock.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The Amazon Nova LLM-as-a-Judge approach uses a powerful language model to evaluate the quality of responses from other models by comparing them side-by-side. This method provides:\n",
    "\n",
    "**Objective Comparison**: Systematic evaluation of model outputs\n",
    "\n",
    "**Scalable Assessment**: Automated evaluation of large datasets\n",
    "\n",
    "**Detailed Metrics**: Win rates, confidence intervals, and preference distributions\n",
    "\n",
    "**Cost-Effective**: More efficient than human evaluation for large-scale comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T14:20:00.576414Z",
     "iopub.status.busy": "2025-06-27T14:20:00.576104Z",
     "iopub.status.idle": "2025-06-27T14:20:00.582402Z",
     "shell.execute_reply": "2025-06-27T14:20:00.581558Z",
     "shell.execute_reply.started": "2025-06-27T14:20:00.576390Z"
    }
   },
   "source": [
    "## Prerequisites\n",
    "\n",
    "* AWS Account with SageMaker and Bedrock access\n",
    "* Appropriate IAM roles and permissions\n",
    "* SageMaker Studio or Jupyter environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T15:22:29.107240Z",
     "iopub.status.busy": "2025-06-27T15:22:29.106656Z",
     "iopub.status.idle": "2025-06-27T15:22:29.114508Z",
     "shell.execute_reply": "2025-06-27T15:22:29.113484Z",
     "shell.execute_reply.started": "2025-06-27T15:22:29.107207Z"
    }
   },
   "source": [
    "## Understanding Amazon Nova LLM-as-a-Judge Evaluation Metrics\n",
    "\n",
    "When using the Amazon Nova LLM-as-a-Judge framework to compare the outputs of two language models, a set of quantitative metrics is generated. These metrics help you objectively assess which model performs better and how reliable the evaluation is.\n",
    "\n",
    "---\n",
    "\n",
    "### Core Preference Metrics\n",
    "\n",
    "- **a_scores**  \n",
    "  The number of times Model A's response was preferred by the judge model over Model B.\n",
    "\n",
    "- **b_scores**  \n",
    "  The number of times Model B's response was preferred by the judge model over Model A.\n",
    "\n",
    "- **ties**  \n",
    "  The number of times the judge found both responses equally good or could not determine a preference.\n",
    "\n",
    "- **inference_error**  \n",
    "  The number of evaluation cases where the judge could not provide a valid judgment due to technical issues, malformed outputs, or other errors.\n",
    "\n",
    "---\n",
    "\n",
    "### Statistical Confidence Metrics\n",
    "\n",
    "- **winrate**  \n",
    "  The proportion of valid judgments in which Model B was preferred\n",
    "\n",
    "- **lower_rate**  \n",
    "  The lower bound of the 95% confidence interval for the winrate. This tells you the minimum likely winrate for Model B, accounting for statistical uncertainty.\n",
    "\n",
    "- **upper_rate**  \n",
    "  The upper bound of the 95% confidence interval for the winrate. This tells you the maximum likely winrate for Model B.\n",
    "\n",
    "- **score**  \n",
    "  An aggregate performance score, which in some contexts matches the number of Model B wins, but can be used for custom scoring depending on the evaluation setup.\n",
    "\n",
    "---\n",
    "\n",
    "### Standard Error Metrics\n",
    "\n",
    "- **a_scores_stderr, b_scores_stderr, ties_stderr, inference_error_stderr, score_stderr**  \n",
    "  These metrics reflect the standard error (uncertainty) of each corresponding count or score. Smaller values indicate more reliable results, while larger values suggest more variability or a need for a larger sample size.\n",
    "\n",
    "---\n",
    "\n",
    "### How to Interpret These Metrics\n",
    "\n",
    "- **Winrate and Confidence Intervals:**  \n",
    "  - If the winrate is significantly above 0.5 and the confidence interval does not include 0.5, Model B is statistically favored.\n",
    "  - If the winrate is below 0.5 and the confidence interval does not include 0.5, Model A is statistically favored.\n",
    "  - If the interval includes 0.5, results are inconclusive.\n",
    "\n",
    "- **Error Analysis:**  \n",
    "  - High inference_error or large standard errors indicate possible issues with the evaluation process or insufficient data.\n",
    "\n",
    "- **Preference Distribution:**  \n",
    "  - The balance of a_scores, b_scores, and ties provides a direct picture of model performance differences on your evaluation set.\n",
    "\n",
    "---\n",
    "\n",
    "### Example Metrics Output\n",
    "\n",
    "```json\n",
    "{\n",
    "\"a_scores\": 16.0,\n",
    "\"a_scores_stderr\": 0.03,\n",
    "\"b_scores\": 10.0,\n",
    "\"b_scores_stderr\": 0.09,\n",
    "\"ties\": 0.0,\n",
    "\"ties_stderr\": 0.0,\n",
    "\"inference_error\": 0.0,\n",
    "\"inference_error_stderr\": 0.0,\n",
    "\"score\": 10.0,\n",
    "\"score_stderr\": 0.09,\n",
    "\"winrate\": 0.38,\n",
    "\"lower_rate\": 0.23,\n",
    "\"upper_rate\": 0.56\n",
    "}\n",
    "```\n",
    "---\n",
    "\n",
    "These metrics, generated automatically during evaluation, provide a comprehensive, statistically rigorous summary of how two models compare on your chosen dataset. They enable you to make informed decisions about model selection and deployment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and Installation\n",
    "\n",
    "Set up the required dependencies and configure the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install python SDK\n",
    "!pip install --upgrade sagemaker "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries\n",
    "Import necessary Python packages and set up SageMaker session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.inputs import TrainingInput\n",
    "import boto3\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import boto3\n",
    "import json\n",
    "from sagemaker.huggingface import HuggingFacePredictor\n",
    "\n",
    "\n",
    "# Initialize SageMaker session\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T17:47:53.128111Z",
     "iopub.status.busy": "2025-06-26T17:47:53.127424Z",
     "iopub.status.idle": "2025-06-26T17:47:53.131058Z",
     "shell.execute_reply": "2025-06-26T17:47:53.130420Z",
     "shell.execute_reply.started": "2025-06-26T17:47:53.128079Z"
    }
   },
   "source": [
    "## Model Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy Qwen2.5 1.5B Instruct Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 deploy_sm_model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test out the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the predictor once\n",
    "predictor = HuggingFacePredictor(endpoint_name=\"qwen25-<endpoint_name_here>\")\n",
    "\n",
    "def generate_with_qwen25(prompt: str, max_tokens: int = 500, temperature: float = 0.9) -> str:\n",
    "    \"\"\"\n",
    "    Sends a prompt to the deployed Qwen2.5 model on SageMaker and returns the generated response.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The input prompt/question to send to the model.\n",
    "        max_tokens (int): Maximum number of tokens to generate.\n",
    "        temperature (float): Sampling temperature for generation.\n",
    "\n",
    "    Returns:\n",
    "        str: The model-generated text.\n",
    "    \"\"\"\n",
    "    response = predictor.predict({\n",
    "        \"inputs\": prompt,\n",
    "        \"parameters\": {\n",
    "            \"max_new_tokens\": max_tokens,\n",
    "            \"temperature\": temperature\n",
    "        }\n",
    "    })\n",
    "    return response[0][\"generated_text\"]\n",
    "\n",
    "answer = generate_with_qwen25(\"What is the Grotto at Notre Dame?\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bedrock Claude 3.7 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize Bedrock client once\n",
    "bedrock = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "\n",
    "# (Claude 3.7 Sonnet) model ID via Bedrock\n",
    "MODEL_ID = \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\"\n",
    "\n",
    "def generate_with_claude37(prompt: str, max_tokens: int = 512, temperature: float = 0.7, top_p: float = 0.9) -> str:\n",
    "    \"\"\"\n",
    "    Sends a prompt to the Claude 4-tier model via Amazon Bedrock and returns the generated response.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The user message or input prompt.\n",
    "        max_tokens (int): Maximum number of tokens to generate.\n",
    "        temperature (float): Sampling temperature for generation.\n",
    "        top_p (float): Top-p nucleus sampling.\n",
    "\n",
    "    Returns:\n",
    "        str: The text content generated by Claude.\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p\n",
    "    }\n",
    "\n",
    "    response = bedrock.invoke_model(\n",
    "        modelId=MODEL_ID,\n",
    "        body=json.dumps(payload),\n",
    "        contentType=\"application/json\",\n",
    "        accept=\"application/json\"\n",
    "    )\n",
    "\n",
    "    response_body = json.loads(response['body'].read())\n",
    "    return response_body[\"content\"][0][\"text\"]\n",
    "\n",
    "answer = generate_with_claude37(\"What is the Grotto at Notre Dame?\")\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T18:08:50.568465Z",
     "iopub.status.busy": "2025-06-26T18:08:50.567730Z",
     "iopub.status.idle": "2025-06-26T18:08:50.571471Z",
     "shell.execute_reply": "2025-06-26T18:08:50.570831Z",
     "shell.execute_reply.started": "2025-06-26T18:08:50.568435Z"
    }
   },
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "squad = load_dataset(\"squad\", split=\"train[:20]\")\n",
    "print(squad[3][\"question\"])\n",
    "print(squad[3][\"answers\"][\"text\"][0])\n",
    "\n",
    "questions = [squad[i][\"question\"] for i in range(6)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T14:24:25.671531Z",
     "iopub.status.busy": "2025-06-27T14:24:25.670633Z",
     "iopub.status.idle": "2025-06-27T14:24:25.674408Z",
     "shell.execute_reply": "2025-06-27T14:24:25.673720Z",
     "shell.execute_reply.started": "2025-06-27T14:24:25.671498Z"
    }
   },
   "source": [
    "### Generate Evaluation Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "output_path = \"llm_judge.jsonl\"\n",
    "\n",
    "with open(output_path, \"w\") as f:\n",
    "    for q in questions:\n",
    "        try:\n",
    "            response_a = generate_with_qwen25(q)\n",
    "        except Exception as e:\n",
    "            response_a = f\"[Qwen2.5 generation failed: {e}]\"\n",
    "        \n",
    "        try:\n",
    "            response_b = generate_with_claude37(q)\n",
    "        except Exception as e:\n",
    "            response_b = f\"[Claude 3.7 generation failed: {e}]\"\n",
    "\n",
    "        row = {\n",
    "            \"prompt\": q,\n",
    "            \"response_A\": response_a,\n",
    "            \"response_B\": response_b\n",
    "        }\n",
    "        f.write(json.dumps(row) + \"\\n\")\n",
    "\n",
    "print(f\"JSONL file created at: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Training Parameters\n",
    "Set up the necessary parameters for the training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please populate parameters\n",
    "\n",
    "your_bucket_name = \"\"\n",
    "assert your_bucket_name != \"\", \"PLEASE POPULATE YOUR BUCKET NAME ABOVE\"\n",
    "\n",
    "input_s3_uri = \"s3://{}/datasets/byo-datasets-dev/custom-llm-judge/llm_judge.jsonl\".format(your_bucket_name)\n",
    "output_s3_uri = \"s3://{}/evaluation-results/mcaiich/llm_judge/\".format(your_bucket_name) # Output data s3 location\n",
    "instance_type = \"ml.g5.24xlarge\"  \n",
    "job_name = \"nova-micro-gen-qa-eval-job\"\n",
    "recipe_path = \"<PATH-TO-RECIPE>/recipe.yaml\"\n",
    "image_uri = f\"708977205387.dkr.ecr.us-east-1.amazonaws.com/nova-evaluation-repo:SM-TJ-Eval-latest\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Data to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T15:02:33.242097Z",
     "iopub.status.busy": "2025-06-27T15:02:33.241445Z",
     "iopub.status.idle": "2025-06-27T15:02:33.247763Z",
     "shell.execute_reply": "2025-06-27T15:02:33.246950Z",
     "shell.execute_reply.started": "2025-06-27T15:02:33.242071Z"
    }
   },
   "source": [
    "## Grant S3 Permissions to the SageMaker Execution Role\n",
    "\n",
    "Before proceeding, make sure to grant the Execution Role direct `s3:PutObject` permissions for your S3 bucket prefix.\n",
    "\n",
    "**Steps:**\n",
    "- Go to the Execution Role (e.g., `AmazonSageMaker-ExecutionRole-...`) in the AWS IAM Console.\n",
    "- Attach the following inline policy:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"Effect\": \"Allow\",\n",
    "  \"Action\": [\n",
    "    \"s3:PutObject\",\n",
    "    \"s3:GetObject\",\n",
    "    \"s3:ListBucket\"\n",
    "  ],\n",
    "  \"Resource\": [\n",
    "    \"arn:aws:s3:::my-bucket-east\",\n",
    "    \"arn:aws:s3:::my-bucket-east/*\"\n",
    "  ]\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "def parse_s3_uri(s3_uri):\n",
    "    assert s3_uri.startswith(\"s3://\"), \"Invalid S3 URI\"\n",
    "    parts = s3_uri.replace(\"s3://\", \"\").split(\"/\", 1)\n",
    "    bucket = parts[0]\n",
    "    key = parts[1] if len(parts) > 1 else \"\"\n",
    "    return bucket, key\n",
    "\n",
    "def upload_to_s3(local_path, s3_uri):\n",
    "    \"\"\"\n",
    "    Upload evaluation data to S3 bucket using current role credentials.\n",
    "    \"\"\"\n",
    "    bucket, key = parse_s3_uri(s3_uri)\n",
    "    \n",
    "    s3 = boto3.client(\"s3\")\n",
    "    s3.upload_file(Filename=local_path, Bucket=bucket, Key=key)\n",
    "    print(f\"✅ Uploaded {local_path} to {s3_uri}\")\n",
    "\n",
    "# Example usage\n",
    "upload_to_s3(\n",
    "    \"llm_judge.jsonl\",\n",
    "    \"s3://{}/datasets/byo-datasets-dev/custom-llm-judge/llm_judge.jsonl\".format(your_bucket_name)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Training Input (Optional)\n",
    "Configure input data source for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) For bring your own dataset for evaluation\n",
    "evalInput = TrainingInput(\n",
    "    s3_data=input_s3_uri,\n",
    "    distribution='FullyReplicated',\n",
    "    s3_data_type='S3Prefix'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Amazon Nova LLM-as-a-Judge Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = PyTorch(\n",
    "    output_path=output_s3_uri,\n",
    "    base_job_name=job_name,\n",
    "    role=role,\n",
    "    instance_type=instance_type,\n",
    "    training_recipe=recipe_path,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    image_uri = image_uri,\n",
    "    disable_profiler=True,\n",
    "    debugger_hook_config=False,\n",
    ")\n",
    "\n",
    "estimator.fit(inputs={\"train\": evalInput})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Results From Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import tarfile\n",
    "\n",
    "def download_and_extract_job_output(training_job_name: str, output_s3_uri: str, download_dir: str = \"./output\"):\n",
    "    \"\"\"\n",
    "    Downloads the output.tar.gz of a SageMaker training job and extracts it locally.\n",
    "\n",
    "    Args:\n",
    "        training_job_name (str): Name of the SageMaker training job.\n",
    "        output_s3_uri (str): Base S3 URI where outputs are stored.\n",
    "        download_dir (str): Local directory to extract files into.\n",
    "    \"\"\"\n",
    "    # Build the full S3 path\n",
    "    s3_uri = f\"{output_s3_uri.rstrip('/')}/{training_job_name}/output/output.tar.gz\"\n",
    "    print(f\"Resolved S3 URI: {s3_uri}\")\n",
    "\n",
    "    # Parse bucket and key\n",
    "    def parse_s3_uri(s3_uri):\n",
    "        assert s3_uri.startswith(\"s3://\"), \"Invalid S3 URI\"\n",
    "        parts = s3_uri.replace(\"s3://\", \"\").split(\"/\", 1)\n",
    "        bucket = parts[0]\n",
    "        key = parts[1] if len(parts) > 1 else \"\"\n",
    "        return bucket, key\n",
    "\n",
    "    bucket, key = parse_s3_uri(s3_uri)\n",
    "\n",
    "    # Create S3 client\n",
    "    s3 = boto3.client(\"s3\")\n",
    "\n",
    "    # Create output directory\n",
    "    if not os.path.exists(download_dir):\n",
    "        os.makedirs(download_dir)\n",
    "\n",
    "    local_tar_path = os.path.join(download_dir, \"output.tar.gz\")\n",
    "\n",
    "    # Download file\n",
    "    print(\"Downloading...\")\n",
    "    s3.download_file(bucket, key, local_tar_path)\n",
    "    print(f\"Downloaded to {local_tar_path}\")\n",
    "\n",
    "    # Extract tar.gz\n",
    "    print(\"Extracting...\")\n",
    "    with tarfile.open(local_tar_path, \"r:gz\") as tar:\n",
    "        tar.extractall(path=download_dir)\n",
    "\n",
    "    print(f\"Extracted contents to {download_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_and_extract_job_output(\"nova-micro-gen-qa-eval-job-<DATE>\", output_s3_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T14:30:03.019249Z",
     "iopub.status.busy": "2025-06-27T14:30:03.018574Z",
     "iopub.status.idle": "2025-06-27T14:30:03.024708Z",
     "shell.execute_reply": "2025-06-27T14:30:03.023844Z",
     "shell.execute_reply.started": "2025-06-27T14:30:03.019225Z"
    }
   },
   "source": [
    "## Results Visualization\n",
    "\n",
    "Based on the evaluation results shown in the uploaded image, here's how to create comprehensive visualizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def plot_llm_judge_results(results):\n",
    "    \"\"\"\n",
    "    Plot LLM judge evaluation results from a JSON file.\n",
    "\n",
    "    Args:\n",
    "        json_file_path (str): Path to the JSON results file\n",
    "        save_plots (bool): Whether to save plots to files\n",
    "        output_dir (str): Directory to save plots (defaults to same dir as JSON file)\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing the plotted data for further analysis\n",
    "    \"\"\"\n",
    "\n",
    "    # Set style\n",
    "    plt.style.use(\"default\")\n",
    "    sns.set_palette(\"husl\")\n",
    "\n",
    "    # Create figure with subplots\n",
    "    fig = plt.figure(figsize=(16, 12))\n",
    "\n",
    "    # 1. Score Distribution Bar Chart\n",
    "    ax1 = plt.subplot(2, 3, 1)\n",
    "    scores = {\n",
    "        \"A Scores\": results[\"a_scores\"],\n",
    "        \"B Scores\": results[\"b_scores\"],\n",
    "        \"Ties\": results[\"ties\"],\n",
    "        \"Inference Errors\": results[\"inference_error\"],\n",
    "    }\n",
    "\n",
    "    bars = ax1.bar(\n",
    "        scores.keys(),\n",
    "        scores.values(),\n",
    "        color=[\"#FF6B6B\", \"#4ECDC4\", \"#45B7D1\", \"#FFA07A\"],\n",
    "    )\n",
    "    ax1.set_title(\"Score Distribution\", fontsize=14, fontweight=\"bold\")\n",
    "    ax1.set_ylabel(\"Count\")\n",
    "\n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, scores.values()):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(\n",
    "            bar.get_x() + bar.get_width() / 2.0,\n",
    "            height + height * 0.01,\n",
    "            f\"{int(value)}\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "    # 2. Win Rate with Confidence Interval\n",
    "    ax2 = plt.subplot(2, 3, 2)\n",
    "    winrate = results[\"winrate\"]\n",
    "    lower_rate = results[\"lower_rate\"]\n",
    "    upper_rate = results[\"upper_rate\"]\n",
    "\n",
    "    # Create horizontal bar for winrate\n",
    "    ax2.barh([\"Win Rate\"], [winrate], color=\"#4ECDC4\", alpha=0.7, height=0.3)\n",
    "\n",
    "    # Add confidence interval\n",
    "    ax2.errorbar(\n",
    "        [winrate],\n",
    "        [\"Win Rate\"],\n",
    "        xerr=[[winrate - lower_rate], [upper_rate - winrate]],\n",
    "        fmt=\"o\",\n",
    "        color=\"black\",\n",
    "        capsize=10,\n",
    "        capthick=2,\n",
    "    )\n",
    "\n",
    "    ax2.set_xlim(0, 1)\n",
    "    ax2.set_xlabel(\"Win Rate\")\n",
    "    ax2.set_title(\"B vs A Win Rate with 95% CI\", fontsize=14, fontweight=\"bold\")\n",
    "    ax2.axvline(\n",
    "        x=0.5, color=\"red\", linestyle=\"--\", alpha=0.7, label=\"50% (No preference)\"\n",
    "    )\n",
    "    ax2.legend()\n",
    "\n",
    "    # Add text annotation\n",
    "    ax2.text(\n",
    "        winrate,\n",
    "        0,\n",
    "        f\"{winrate:.3f}\\n[{lower_rate:.3f}, {upper_rate:.3f}]\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontweight=\"bold\",\n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8),\n",
    "    )\n",
    "\n",
    "    # 3. Preference Pie Chart (excluding inference errors)\n",
    "    ax3 = plt.subplot(2, 3, 3)\n",
    "    total_valid = results[\"a_scores\"] + results[\"b_scores\"] + results[\"ties\"]\n",
    "\n",
    "    if total_valid > 0:\n",
    "        pie_data = [results[\"a_scores\"], results[\"b_scores\"], results[\"ties\"]]\n",
    "        pie_labels = [\"A Preferred\", \"B Preferred\", \"Ties\"]\n",
    "        colors = [\"#FF6B6B\", \"#4ECDC4\", \"#45B7D1\"]\n",
    "\n",
    "        wedges, texts, autotexts = ax3.pie(\n",
    "            pie_data, labels=pie_labels, colors=colors, autopct=\"%1.1f%%\", startangle=90\n",
    "        )\n",
    "\n",
    "        # Make percentage text bold\n",
    "        for autotext in autotexts:\n",
    "            autotext.set_fontweight(\"bold\")\n",
    "            autotext.set_color(\"white\")\n",
    "\n",
    "    ax3.set_title(\n",
    "        \"Preference Distribution\\n(Valid Judgments Only)\",\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "    # 4. Comparison of A vs B Scores\n",
    "    ax4 = plt.subplot(2, 3, 4)\n",
    "    categories = [\"A Scores\", \"B Scores\"]\n",
    "    values = [results[\"a_scores\"], results[\"b_scores\"]]\n",
    "    colors = [\"#FF6B6B\", \"#4ECDC4\"]\n",
    "\n",
    "    bars = ax4.bar(categories, values, color=colors, alpha=0.8)\n",
    "    ax4.set_title(\"A vs B Score Comparison\", fontsize=14, fontweight=\"bold\")\n",
    "    ax4.set_ylabel(\"Score Count\")\n",
    "\n",
    "    # Add value labels\n",
    "    for bar, value in zip(bars, values):\n",
    "        height = bar.get_height()\n",
    "        ax4.text(\n",
    "            bar.get_x() + bar.get_width() / 2.0,\n",
    "            height + height * 0.01,\n",
    "            f\"{int(value)}\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "\n",
    "    # Add difference annotation\n",
    "    diff = abs(values[0] - values[1])\n",
    "    winner = \"A\" if values[0] > values[1] else \"B\"\n",
    "    ax4.text(\n",
    "        0.5,\n",
    "        max(values) * 0.8,\n",
    "        f\"{winner} leads by {int(diff)}\",\n",
    "        ha=\"center\",\n",
    "        transform=ax4.transData,\n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"yellow\", alpha=0.7),\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "    # 5. Win Rate Visualization (Gauge-like)\n",
    "    ax5 = plt.subplot(2, 3, 5)\n",
    "\n",
    "    # Create a semi-circular gauge\n",
    "    theta = np.linspace(0, np.pi, 100)\n",
    "    r = 1\n",
    "\n",
    "    # Background arc\n",
    "    ax5.plot(r * np.cos(theta), r * np.sin(theta), \"lightgray\", linewidth=10)\n",
    "\n",
    "    # Win rate arc\n",
    "    winrate_theta = np.linspace(0, winrate * np.pi, int(winrate * 100))\n",
    "    ax5.plot(\n",
    "        r * np.cos(winrate_theta), r * np.sin(winrate_theta), \"#4ECDC4\", linewidth=10\n",
    "    )\n",
    "\n",
    "    # Add needle\n",
    "    needle_angle = winrate * np.pi\n",
    "    ax5.arrow(\n",
    "        0,\n",
    "        0,\n",
    "        0.8 * np.cos(needle_angle),\n",
    "        0.8 * np.sin(needle_angle),\n",
    "        head_width=0.05,\n",
    "        head_length=0.05,\n",
    "        fc=\"red\",\n",
    "        ec=\"red\",\n",
    "    )\n",
    "\n",
    "    ax5.set_xlim(-1.2, 1.2)\n",
    "    ax5.set_ylim(-0.2, 1.2)\n",
    "    ax5.set_aspect(\"equal\")\n",
    "    ax5.axis(\"off\")\n",
    "    ax5.set_title(\"Win Rate Gauge\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "    # Add labels\n",
    "    ax5.text(-1, -0.1, \"0%\", ha=\"center\", fontweight=\"bold\")\n",
    "    ax5.text(0, -0.1, \"50%\", ha=\"center\", fontweight=\"bold\")\n",
    "    ax5.text(1, -0.1, \"100%\", ha=\"center\", fontweight=\"bold\")\n",
    "    ax5.text(\n",
    "        0,\n",
    "        0.5,\n",
    "        f\"{winrate:.1%}\",\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        fontsize=16,\n",
    "        fontweight=\"bold\",\n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8),\n",
    "    )\n",
    "\n",
    "    # 6. Summary Statistics Table\n",
    "    ax6 = plt.subplot(2, 3, 6)\n",
    "    ax6.axis(\"off\")\n",
    "\n",
    "    # Create summary statistics\n",
    "    total_evaluations = (\n",
    "        results[\"a_scores\"]\n",
    "        + results[\"b_scores\"]\n",
    "        + results[\"ties\"]\n",
    "        + results[\"inference_error\"]\n",
    "    )\n",
    "\n",
    "    summary_data = [\n",
    "        [\"Total Evaluations\", f\"{int(total_evaluations)}\"],\n",
    "        [\"A Scores\", f\"{int(results['a_scores'])}\"],\n",
    "        [\"B Scores\", f\"{int(results['b_scores'])}\"],\n",
    "        [\"Ties\", f\"{int(results['ties'])}\"],\n",
    "        [\"Inference Errors\", f\"{int(results['inference_error'])}\"],\n",
    "        [\"Win Rate (B vs A)\", f\"{results['winrate']:.3f}\"],\n",
    "        [\"95% CI Lower\", f\"{results['lower_rate']:.3f}\"],\n",
    "        [\"95% CI Upper\", f\"{results['upper_rate']:.3f}\"],\n",
    "        [\n",
    "            \"Error Rate\",\n",
    "            (\n",
    "                f\"{results['inference_error']/total_evaluations:.1%}\"\n",
    "                if total_evaluations > 0\n",
    "                else \"0%\"\n",
    "            ),\n",
    "        ],\n",
    "    ]\n",
    "\n",
    "    # Create table\n",
    "    table = ax6.table(\n",
    "        cellText=summary_data,\n",
    "        colLabels=[\"Metric\", \"Value\"],\n",
    "        cellLoc=\"left\",\n",
    "        loc=\"center\",\n",
    "        colWidths=[0.6, 0.4],\n",
    "    )\n",
    "\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1, 2)\n",
    "\n",
    "    # Style the table\n",
    "    for i in range(len(summary_data) + 1):\n",
    "        for j in range(2):\n",
    "            cell = table[(i, j)]\n",
    "            if i == 0:  # Header\n",
    "                cell.set_facecolor(\"#4ECDC4\")\n",
    "                cell.set_text_props(weight=\"bold\", color=\"white\")\n",
    "            else:\n",
    "                cell.set_facecolor(\"#f0f0f0\" if i % 2 == 0 else \"white\")\n",
    "                if j == 1:  # Value column\n",
    "                    cell.set_text_props(weight=\"bold\")\n",
    "\n",
    "    ax6.set_title(\"Summary Statistics\", fontsize=14, fontweight=\"bold\", pad=20)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results_path = \"./output/nova-micro-llm-judge-eval-job/eval_results/results_2025-06-26T22-02-09.817675.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(evaluation_results_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "fig = plot_llm_judge_results(data[\"results\"][\"all\"])\n",
    "\n",
    "output_file = os.path.join(\"./\", \"evaluation_metrics.png\")\n",
    "fig.savefig(output_file, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T14:49:23.956157Z",
     "iopub.status.busy": "2025-06-27T14:49:23.955800Z",
     "iopub.status.idle": "2025-06-27T14:49:23.962180Z",
     "shell.execute_reply": "2025-06-27T14:49:23.961415Z",
     "shell.execute_reply.started": "2025-06-27T14:49:23.956132Z"
    }
   },
   "source": [
    "**Based on the evaluation results:**\n",
    "\n",
    "**Performance Metrics** \n",
    "\n",
    "* **Total Evaluations:** 12 questions evaluated (6 Each)\n",
    "\n",
    "* **Model B (Claude 3.7) Performance:** 9 wins (75%)\n",
    "\n",
    "* **Model A (Qwen2.5) Performance:** 3 wins (25%)\n",
    "\n",
    "* **Ties:** 0\n",
    "\n",
    "* **Error Rate:** 0%\n",
    "\n",
    "**Statistical Confidence**  \n",
    "* **Win Rate:** 75% in favor of Model B\n",
    "\n",
    "* **95% Confidence Interval:** [9.1%, 91.7%]\n",
    "\n",
    "**Statistical Significance:** High confidence that Model B outperforms Model A\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T14:52:18.917247Z",
     "iopub.status.busy": "2025-06-27T14:52:18.916839Z",
     "iopub.status.idle": "2025-06-27T14:52:18.922880Z",
     "shell.execute_reply": "2025-06-27T14:52:18.922195Z",
     "shell.execute_reply.started": "2025-06-27T14:52:18.917223Z"
    }
   },
   "source": [
    "### Key Insights\n",
    "\n",
    "**Clear Winner:** Claude 3.7 Sonnet significantly outperformed Qwen2.5 1.5 B Model\n",
    "\n",
    "This was expected because Qwen 2.5 1.5B is a significantly smaller model than Claude 3.7\n",
    "\n",
    "**Consistent Performance:** No ties suggest clear quality differences\n",
    "\n",
    "**Reliable Evaluation:** Zero inference errors indicate robust setup\n",
    "\n",
    "**Statistical Validity:** Wide confidence interval due to small sample size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T14:53:46.318232Z",
     "iopub.status.busy": "2025-06-27T14:53:46.317515Z",
     "iopub.status.idle": "2025-06-27T14:53:46.324514Z",
     "shell.execute_reply": "2025-06-27T14:53:46.323623Z",
     "shell.execute_reply.started": "2025-06-27T14:53:46.318184Z"
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated a complete Amazon Nova LLM-as-a-Judge evaluation pipeline using Amazon SageMaker AI. The methodology provides:\n",
    "\n",
    "**Scalable Evaluation:** Automated comparison of multiple models\n",
    "\n",
    "**Statistical Rigor:** Confidence intervals and significance testing\n",
    "\n",
    "**Cost Efficiency:** Reduced need for human evaluation\n",
    "\n",
    "**Actionable Insights:** Clear metrics for model selection\n",
    "\n",
    "The results showed Claude 3.7 Sonnet outperforming Qwen2.5 with 75% win rate, providing valuable insights for model selection and deployment decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
