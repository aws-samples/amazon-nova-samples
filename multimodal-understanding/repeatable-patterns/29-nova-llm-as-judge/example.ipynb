{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17af4dc5",
   "metadata": {},
   "source": [
    "# Thematic Analysis with LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f8b432",
   "metadata": {},
   "source": [
    "## This notebook implements a pipeline for thematic analysis of text data using Amazon Bedrock and Anthropic's Claude model, followed by evaluation of the generated themes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949b2f07",
   "metadata": {},
   "source": [
    "### Step 1. Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7497cf8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-24T15:53:57.072898Z",
     "iopub.status.busy": "2025-07-24T15:53:57.072535Z",
     "iopub.status.idle": "2025-07-24T15:53:58.854414Z",
     "shell.execute_reply": "2025-07-24T15:53:58.853881Z",
     "shell.execute_reply.started": "2025-07-24T15:53:57.072874Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.12/site-packages (1.37.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (1.26.4)\n",
      "Collecting simpledorff\n",
      "  Downloading simpledorff-0.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.12/site-packages (1.15.2)\n",
      "Requirement already satisfied: botocore<1.38.0,>=1.37.1 in /opt/conda/lib/python3.12/site-packages (from boto3) (1.37.1)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.12/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.12.0,>=0.11.0 in /opt/conda/lib/python3.12/site-packages (from boto3) (0.11.3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.12/site-packages (from botocore<1.38.0,>=1.37.1->boto3) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/conda/lib/python3.12/site-packages (from botocore<1.38.0,>=1.37.1->boto3) (1.26.19)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.38.0,>=1.37.1->boto3) (1.17.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Downloading simpledorff-0.0.2-py3-none-any.whl (5.6 kB)\n",
      "Installing collected packages: simpledorff\n",
      "Successfully installed simpledorff-0.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install boto3 pandas numpy simpledorff scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2127d68a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-24T15:54:01.515204Z",
     "iopub.status.busy": "2025-07-24T15:54:01.514791Z",
     "iopub.status.idle": "2025-07-24T15:54:01.903281Z",
     "shell.execute_reply": "2025-07-24T15:54:01.902797Z",
     "shell.execute_reply.started": "2025-07-24T15:54:01.515182Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model ID: amazon.titan-tg1-large\n",
      "Model ID: amazon.titan-image-generator-v1:0\n",
      "Model ID: amazon.titan-image-generator-v1\n",
      "Model ID: amazon.titan-image-generator-v2:0\n",
      "Model ID: amazon.nova-premier-v1:0:8k\n",
      "Model ID: amazon.nova-premier-v1:0:20k\n",
      "Model ID: amazon.nova-premier-v1:0:1000k\n",
      "Model ID: amazon.nova-premier-v1:0:mm\n",
      "Model ID: amazon.nova-premier-v1:0\n",
      "Model ID: amazon.titan-text-premier-v1:0\n",
      "Model ID: amazon.nova-pro-v1:0:24k\n",
      "Model ID: amazon.nova-pro-v1:0:300k\n",
      "Model ID: amazon.nova-pro-v1:0\n",
      "Model ID: amazon.nova-lite-v1:0:24k\n",
      "Model ID: amazon.nova-lite-v1:0:300k\n",
      "Model ID: amazon.nova-lite-v1:0\n",
      "Model ID: amazon.nova-canvas-v1:0\n",
      "Model ID: amazon.nova-reel-v1:0\n",
      "Model ID: amazon.nova-reel-v1:1\n",
      "Model ID: amazon.nova-micro-v1:0:24k\n",
      "Model ID: amazon.nova-micro-v1:0:128k\n",
      "Model ID: amazon.nova-micro-v1:0\n",
      "Model ID: amazon.nova-sonic-v1:0\n",
      "Model ID: amazon.titan-embed-g1-text-02\n",
      "Model ID: amazon.titan-text-lite-v1:0:4k\n",
      "Model ID: amazon.titan-text-lite-v1\n",
      "Model ID: amazon.titan-text-express-v1:0:8k\n",
      "Model ID: amazon.titan-text-express-v1\n",
      "Model ID: amazon.titan-embed-text-v1:2:8k\n",
      "Model ID: amazon.titan-embed-text-v1\n",
      "Model ID: amazon.titan-embed-text-v2:0:8k\n",
      "Model ID: amazon.titan-embed-text-v2:0\n",
      "Model ID: amazon.titan-embed-image-v1:0\n",
      "Model ID: amazon.titan-embed-image-v1\n",
      "Model ID: stability.stable-diffusion-xl-v1:0\n",
      "Model ID: stability.stable-diffusion-xl-v1\n",
      "Model ID: ai21.jamba-instruct-v1:0\n",
      "Model ID: ai21.jamba-1-5-large-v1:0\n",
      "Model ID: ai21.jamba-1-5-mini-v1:0\n",
      "Model ID: anthropic.claude-instant-v1:2:100k\n",
      "Model ID: anthropic.claude-instant-v1\n",
      "Model ID: anthropic.claude-v2:0:18k\n",
      "Model ID: anthropic.claude-v2:0:100k\n",
      "Model ID: anthropic.claude-v2:1:18k\n",
      "Model ID: anthropic.claude-v2:1:200k\n",
      "Model ID: anthropic.claude-v2:1\n",
      "Model ID: anthropic.claude-v2\n",
      "Model ID: anthropic.claude-3-sonnet-20240229-v1:0:28k\n",
      "Model ID: anthropic.claude-3-sonnet-20240229-v1:0:200k\n",
      "Model ID: anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Model ID: anthropic.claude-3-haiku-20240307-v1:0:48k\n",
      "Model ID: anthropic.claude-3-haiku-20240307-v1:0:200k\n",
      "Model ID: anthropic.claude-3-haiku-20240307-v1:0\n",
      "Model ID: anthropic.claude-3-opus-20240229-v1:0:12k\n",
      "Model ID: anthropic.claude-3-opus-20240229-v1:0:28k\n",
      "Model ID: anthropic.claude-3-opus-20240229-v1:0:200k\n",
      "Model ID: anthropic.claude-3-opus-20240229-v1:0\n",
      "Model ID: anthropic.claude-3-5-sonnet-20240620-v1:0\n",
      "Model ID: anthropic.claude-3-5-sonnet-20241022-v2:0\n",
      "Model ID: anthropic.claude-3-7-sonnet-20250219-v1:0\n",
      "Model ID: anthropic.claude-3-5-haiku-20241022-v1:0\n",
      "Model ID: anthropic.claude-opus-4-20250514-v1:0\n",
      "Model ID: anthropic.claude-sonnet-4-20250514-v1:0\n",
      "Model ID: cohere.command-text-v14:7:4k\n",
      "Model ID: cohere.command-text-v14\n",
      "Model ID: cohere.command-r-v1:0\n",
      "Model ID: cohere.command-r-plus-v1:0\n",
      "Model ID: cohere.command-light-text-v14:7:4k\n",
      "Model ID: cohere.command-light-text-v14\n",
      "Model ID: cohere.embed-english-v3:0:512\n",
      "Model ID: cohere.embed-english-v3\n",
      "Model ID: cohere.embed-multilingual-v3:0:512\n",
      "Model ID: cohere.embed-multilingual-v3\n",
      "Model ID: deepseek.r1-v1:0\n",
      "Model ID: meta.llama3-8b-instruct-v1:0\n",
      "Model ID: meta.llama3-70b-instruct-v1:0\n",
      "Model ID: meta.llama3-1-8b-instruct-v1:0\n",
      "Model ID: meta.llama3-1-70b-instruct-v1:0\n",
      "Model ID: meta.llama3-2-11b-instruct-v1:0\n",
      "Model ID: meta.llama3-2-90b-instruct-v1:0\n",
      "Model ID: meta.llama3-2-1b-instruct-v1:0\n",
      "Model ID: meta.llama3-2-3b-instruct-v1:0\n",
      "Model ID: meta.llama3-3-70b-instruct-v1:0\n",
      "Model ID: meta.llama4-scout-17b-instruct-v1:0\n",
      "Model ID: meta.llama4-maverick-17b-instruct-v1:0\n",
      "Model ID: mistral.mistral-7b-instruct-v0:2\n",
      "Model ID: mistral.mixtral-8x7b-instruct-v0:1\n",
      "Model ID: mistral.mistral-large-2402-v1:0\n",
      "Model ID: mistral.mistral-small-2402-v1:0\n",
      "Model ID: mistral.pixtral-large-2502-v1:0\n",
      "Model ID: twelvelabs.marengo-embed-2-7-v1:0\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "from datetime import datetime\n",
    "from botocore.config import Config\n",
    "import re\n",
    "\n",
    "\n",
    "# Initialize S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "#Optional: Check which models you have access to \n",
    "bedrock = boto3.client('bedrock')\n",
    "response = bedrock.list_foundation_models()\n",
    "\n",
    "# Initialize AWS clients\n",
    "bedrock_runtime = boto3.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name='us-east-1',  # Important: Replace with your region if different\n",
    "    config=Config(\n",
    "        signature_version='v4',\n",
    "        retries={\n",
    "            'max_attempts': 3,\n",
    "            'mode': 'standard'\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "# Optional: Print available models\n",
    "for model in response['modelSummaries']:\n",
    "    print(f\"Model ID: {model['modelId']}\")\n",
    "    if model[\"inferenceTypesSupported\"] == \"ON_DEMAND\":\n",
    "        print(model)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6b48efb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-24T15:56:18.502669Z",
     "iopub.status.busy": "2025-07-24T15:56:18.502408Z",
     "iopub.status.idle": "2025-07-24T15:56:18.505423Z",
     "shell.execute_reply": "2025-07-24T15:56:18.504948Z",
     "shell.execute_reply.started": "2025-07-24T15:56:18.502653Z"
    }
   },
   "outputs": [],
   "source": [
    "# Configure the S3 for input text file as well as to upload the output\n",
    "bucket = 'my-example-name'\n",
    "raw_input = 'feedback_dummy_data.txt'\n",
    "output_themes = 'feedback_analyzed_claude.txt'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "57c19d7b",
   "metadata": {},
   "source": [
    "### Step 2: Generate Thematic Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aac83d02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-24T15:56:19.917422Z",
     "iopub.status.busy": "2025-07-24T15:56:19.917160Z",
     "iopub.status.idle": "2025-07-24T15:56:19.921853Z",
     "shell.execute_reply": "2025-07-24T15:56:19.921374Z",
     "shell.execute_reply.started": "2025-07-24T15:56:19.917407Z"
    }
   },
   "outputs": [],
   "source": [
    "def analyze_comment(comment, model_id=\"amazon.nova-pro-v1:0\"):\n",
    "    \"\"\"\n",
    "    Analyze customer comments using Nova Pro in a structured format.\n",
    "    Returns thematic analysis of customer feedback.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define system prompt\n",
    "    system_list = [\n",
    "        {\n",
    "            \"text\": \"You are a Customer Feedback Analyst responsible for identifying key themes and concerns in customer reviews. Your role is to extract meaningful insights that can drive product improvements within the team.\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Define the user message\n",
    "    message_list = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"text\": f\"\"\"Analyze this customer review and provide insights in JSON format:\n",
    "\n",
    "Review: \"{comment}\"\n",
    "\n",
    "Return only this JSON structure with your analysis:\n",
    "{{\n",
    "    \"main_theme\": \"primary theme identified\",\n",
    "    \"sub_theme\": \"secondary theme or specific aspect\",\n",
    "    \"rationale\": \"explanation of thematic analysis\"\n",
    "}}\n",
    "\n",
    "Provide only the JSON response without additional text.\"\"\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Configure inference parameters\n",
    "    inf_params = {\n",
    "        \"maxTokens\": 1000,\n",
    "        \"temperature\": 0.1,\n",
    "        \"topP\": 0.9,\n",
    "        \"topK\": 20\n",
    "    }\n",
    "\n",
    "    # Construct the request body\n",
    "    request_body = {\n",
    "        \"schemaVersion\": \"messages-v1\",\n",
    "        \"messages\": message_list,\n",
    "        \"system\": system_list,\n",
    "        \"inferenceConfig\": inf_params\n",
    "    }\n",
    "\n",
    "    # Make the request to Nova Pro\n",
    "    response = bedrock_runtime.invoke_model_with_response_stream(\n",
    "        modelId=model_id,\n",
    "        body=json.dumps(request_body)\n",
    "    )\n",
    "\n",
    "    # Process the response stream\n",
    "    full_response = \"\"\n",
    "    stream = response.get(\"body\")\n",
    "    if stream:\n",
    "        for event in stream:\n",
    "            chunk = event.get(\"chunk\")\n",
    "            if chunk:\n",
    "                chunk_json = json.loads(chunk.get(\"bytes\").decode())\n",
    "                content_block_delta = chunk_json.get(\"contentBlockDelta\")\n",
    "                if content_block_delta:\n",
    "                    full_response += content_block_delta.get(\"delta\").get(\"text\", \"\")\n",
    "\n",
    "    # Extract and validate JSON response\n",
    "    try:\n",
    "    \tjson_match = re.search(r'(\\{.*\\})', full_response, re.DOTALL)\n",
    "    \tif json_match:\n",
    "        \tjson_str = json_match.group(1)\n",
    "        \treturn json.loads(json_str)\n",
    "    \telse:\n",
    "        \traise json.JSONDecodeError(\"No JSON found\", full_response, 0)\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"main_theme\": \"Error in analysis\",\n",
    "            \"sub_theme\": \"Processing error\",\n",
    "            \"rationale\": str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16fbf4b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-24T15:58:11.222972Z",
     "iopub.status.busy": "2025-07-24T15:58:11.222497Z",
     "iopub.status.idle": "2025-07-24T15:58:11.979021Z",
     "shell.execute_reply": "2025-07-24T15:58:11.978453Z",
     "shell.execute_reply.started": "2025-07-24T15:58:11.222956Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing comment 1 of 1\n",
      "Analysis complete. Results saved to s3://genai-demos-bucket/feedback_analyzed_claude.txt\n"
     ]
    }
   ],
   "source": [
    "response = s3_client.get_object(Bucket=bucket, Key=raw_input)\n",
    "feedbacks = []\n",
    "\n",
    "for line in response['Body'].iter_lines():\n",
    "    decoded_line = line.decode('utf-8')\n",
    "    feedbacks.append(decoded_line)\n",
    "\n",
    "# Analyze each comment and store results\n",
    "analyzed_results = []\n",
    "output_content = \"\"\n",
    "\n",
    "for i, feedback in enumerate(feedbacks):\n",
    "    if feedback.strip():  # Skip empty lines\n",
    "        print(f\"Analyzing comment {i+1} of {len(feedbacks)}\")\n",
    "        analysis = analyze_comment(feedback)\n",
    "\n",
    "#Optional: For debugging\n",
    "#         print(\"Raw response for feedback:\", feedback)\n",
    "#         print(analysis)\n",
    "\n",
    "        result = {\n",
    "            \"original_comment\": feedback,\n",
    "            \"analysis\": analysis\n",
    "        }\n",
    "        analyzed_results.append(result)\n",
    "\n",
    "# Format output for each result\n",
    "        entry = {\n",
    "            \"Original Comment\": feedback,\n",
    "            \"main_theme\": analysis['main_theme'],\n",
    "            \"sub_theme\": analysis['sub_theme'],\n",
    "            \"rationale\": analysis['rationale']\n",
    "        }\n",
    "        output_content += \"\\n\".join(f\"{k}: {v}\" for k, v in entry.items()) + \"\\n\\n\"\n",
    "\n",
    "# Save results back to S3\n",
    "s3_client.put_object(\n",
    "    Bucket=bucket,\n",
    "    Key=output_themes,\n",
    "    Body=output_content.encode('utf-8')\n",
    ")\n",
    "\n",
    "print(f\"Analysis complete. Results saved to s3://{bucket}/{output_themes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae87ed3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-24T15:58:28.183143Z",
     "iopub.status.busy": "2025-07-24T15:58:28.182746Z",
     "iopub.status.idle": "2025-07-24T15:58:28.234095Z",
     "shell.execute_reply": "2025-07-24T15:58:28.233670Z",
     "shell.execute_reply.started": "2025-07-24T15:58:28.183126Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entry 1:\n",
      "Original Comment: Affordable and reliable.\n",
      "main_theme: positive feedback\n",
      "sub_theme: affordability and reliability\n",
      "rationale: The review highlights two key attributes of the product: it is 'affordable' and 'reliable', indicating customer satisfaction with both the price and the performance of the product.\n"
     ]
    }
   ],
   "source": [
    "#Optional: Read the analyzed results file\n",
    "response = s3_client.get_object(Bucket=bucket, Key=output_themes)\n",
    "content = response['Body'].read().decode('utf-8')\n",
    "\n",
    "# Split by double newlines since that's how entries are separated\n",
    "entries = content.split('\\n\\n')\n",
    "\n",
    "\n",
    "# Display first k entries \n",
    "k = 3\n",
    "for i, entry in enumerate(entries[:k]):\n",
    "    if entry.strip():  # Skip empty entries\n",
    "        print(f\"\\nEntry {i+1}:\")\n",
    "        print(entry.strip())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a629e04e",
   "metadata": {},
   "source": [
    "### Step 3. Deploy multiple pre-trained LLMs as judges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c639c1fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-24T16:04:00.407474Z",
     "iopub.status.busy": "2025-07-24T16:04:00.407238Z",
     "iopub.status.idle": "2025-07-24T16:04:00.415912Z",
     "shell.execute_reply": "2025-07-24T16:04:00.415393Z",
     "shell.execute_reply.started": "2025-07-24T16:04:00.407458Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_alignment_nova_product(comment, theme, subtheme, rationale, model_id):\n",
    "    \"\"\"Evaluate theme alignment using Nova model\"\"\"\n",
    "    \n",
    "    # Define system prompt\n",
    "    system_list = [\n",
    "        {\n",
    "            \"text\": \"You are a Product Researcher analyzing customer feedback. Your role is to evaluate how accurately our thematic analysis captures customer sentiments and experiences. Use your expertise in customer insights and product research to assess theme alignment.\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Define the user message\n",
    "    message_list = [\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"text\": f\"\"\"As a Product Researcher, evaluate the theme alignment using this scale:\n",
    "1: Poor - Our thematic analysis misses key customer product requirements\n",
    "2: Partial - Our analysis captures some but not all important product requirements feedback\n",
    "3: Strong - Our thematic analysis effectively captures the customer's requirements\n",
    "\n",
    "CUSTOMER FEEDBACK ANALYSIS:\n",
    "Voice of Customer: \"{comment}\"\n",
    "Primary Theme: {theme}\n",
    "Secondary Theme: {subtheme} \n",
    "Analysis Rationale: {rationale}\n",
    "\n",
    "Provide your research assessment in this JSON format:\n",
    "{{\n",
    "    \"alignment_score\": <1, 2, or 3>,\n",
    "    \"justification\": \"brief explanation of your evaluation from a research perspective\"\n",
    "}}\n",
    "\n",
    "Return only valid JSON with no additional commentary.\"\"\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Configure inference parameters\n",
    "    inf_params = {\n",
    "        \"maxTokens\": 500,\n",
    "        \"topP\": 0.9,\n",
    "        \"topK\": 20,\n",
    "        \"temperature\": 0.1\n",
    "    }\n",
    "\n",
    "    # Construct the request body\n",
    "    request_body = {\n",
    "        \"schemaVersion\": \"messages-v1\",\n",
    "        \"messages\": message_list,\n",
    "        \"system\": system_list,\n",
    "        \"inferenceConfig\": inf_params,\n",
    "    }\n",
    "\n",
    "    # Make the request\n",
    "    response = bedrock_runtime.invoke_model_with_response_stream(\n",
    "        modelId=model_id,\n",
    "        body=json.dumps(request_body)\n",
    "    )\n",
    "\n",
    "    # Process the response stream\n",
    "    full_response = \"\"\n",
    "    stream = response.get(\"body\")\n",
    "    if stream:\n",
    "        for event in stream:\n",
    "            chunk = event.get(\"chunk\")\n",
    "            if chunk:\n",
    "                chunk_json = json.loads(chunk.get(\"bytes\").decode())\n",
    "                content_block_delta = chunk_json.get(\"contentBlockDelta\")\n",
    "                if content_block_delta:\n",
    "                    full_response += content_block_delta.get(\"delta\").get(\"text\", \"\")\n",
    "\n",
    "\n",
    "    json_match = re.search(r'(\\{.*\\})', full_response, re.DOTALL)\n",
    "    if json_match:\n",
    "        json_str = json_match.group(1)\n",
    "        return json.loads(json_str)\n",
    "    else:\n",
    "        raise json.JSONDecodeError(\"No JSON found\", full_response, 0)\n",
    "\n",
    "## Deploying Nova Pro in a Sales persona\n",
    "\n",
    "def evaluate_alignment_nova_sales(comment, theme, subtheme, rationale, model_id):\n",
    "    \"\"\"Evaluate theme alignment using Nova model\"\"\"\n",
    "    \n",
    "    # Define system prompt\n",
    "    system_list = [\n",
    "        {\n",
    "            \"text\": \"You are a Customer Experience Advocate whose primary focus is understanding the emotional context and underlying customer needs. Your role is to analyze feedback from the customer's perspective, identifying both explicit and implicit emotional signals. Use your expertise in customer sales to assess theme alignment.\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Define the user message\n",
    "    message_list = [\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"text\": f\"\"\"As a Sales Representative, evaluate the theme alignment using this scale:\n",
    "1: Poor - The thematic analysis misses key customer sentiments\n",
    "2: Partial - The analysis captures some but not all important sentiments\n",
    "3: Strong - The thematic analysis effectively captures all the customer's core sentiments\n",
    "\n",
    "CUSTOMER FEEDBACK ANALYSIS:\n",
    "Voice of Customer: \"{comment}\"\n",
    "Primary Theme: {theme}\n",
    "Secondary Theme: {subtheme} \n",
    "Analysis Rationale: {rationale}\n",
    "\n",
    "Provide your research assessment in this JSON format:\n",
    "{{\n",
    "    \"alignment_score\": <1, 2, or 3>,\n",
    "    \"justification\": \"brief explanation of your evaluation from a research perspective\"\n",
    "}}\n",
    "\n",
    "Return only valid JSON with no additional commentary.\"\"\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Configure inference parameters\n",
    "    inf_params = {\n",
    "        \"maxTokens\": 500,\n",
    "        \"topP\": 0.9,\n",
    "        \"topK\": 20,\n",
    "        \"temperature\": 0.1\n",
    "    }\n",
    "\n",
    "    # Construct the request body\n",
    "    request_body = {\n",
    "        \"schemaVersion\": \"messages-v1\",\n",
    "        \"messages\": message_list,\n",
    "        \"system\": system_list,\n",
    "        \"inferenceConfig\": inf_params,\n",
    "    }\n",
    "\n",
    "    # Make the request\n",
    "    response = bedrock_runtime.invoke_model_with_response_stream(\n",
    "        modelId=model_id,\n",
    "        body=json.dumps(request_body)\n",
    "    )\n",
    "\n",
    "    # Process the response stream\n",
    "    full_response = \"\"\n",
    "    stream = response.get(\"body\")\n",
    "    if stream:\n",
    "        for event in stream:\n",
    "            chunk = event.get(\"chunk\")\n",
    "            if chunk:\n",
    "                chunk_json = json.loads(chunk.get(\"bytes\").decode())\n",
    "                content_block_delta = chunk_json.get(\"contentBlockDelta\")\n",
    "                if content_block_delta:\n",
    "                    full_response += content_block_delta.get(\"delta\").get(\"text\", \"\")\n",
    "\n",
    "    # Extract JSON from response\n",
    "    json_match = re.search(r'(\\{.*\\})', full_response, re.DOTALL)\n",
    "    if json_match:\n",
    "        json_str = json_match.group(1)\n",
    "        return json.loads(json_str)\n",
    "    else:\n",
    "        raise json.JSONDecodeError(\"No JSON found\", full_response, 0)\n",
    "\n",
    "## Deploying Claude 3.5 Sonnet \n",
    "\n",
    "def evaluate_alignment_claude(comment, theme, subtheme, rationale, model_id):\n",
    "    \"\"\"Evaluate theme alignment using Claude model\"\"\"\n",
    "    \n",
    "    # In Claude's API format, system message is a separate top-level parameter\n",
    "    system_message = \"You are a customer research executive evaluating how accurately themes match a customer comment. Provide your evaluation as JSON.\"\n",
    "    \n",
    "    # Define the messages (only user and assistant roles allowed)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": f\"\"\"Rate ONLY the theme alignment on this scale:\n",
    "1: Poor match - themes don't capture the main points\n",
    "2: Partial match - themes capture some but not all key points\n",
    "3: Strong match - themes accurately capture the main points\n",
    "\n",
    "REVIEW DETAILS:\n",
    "Comment: \"{comment}\"\n",
    "Main Theme: {theme}\n",
    "Sub-theme: {subtheme} \n",
    "Rationale: {rationale}\n",
    "\n",
    "Respond ONLY with this JSON structure:\n",
    "{{\n",
    "    \"alignment_score\": <1, 2, or 3>,\n",
    "    \"justification\": \"brief explanation of score\"\n",
    "}}\n",
    "\n",
    "Your response must be valid JSON with no other text.\"\"\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Construct the request body according to Claude's API format\n",
    "    request_body = {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"system\": system_message,\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": 500,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.9\n",
    "    }\n",
    "\n",
    "    # Make the request\n",
    "    response = bedrock_runtime.invoke_model_with_response_stream(\n",
    "        modelId=model_id,\n",
    "        body=json.dumps(request_body)\n",
    "    )\n",
    "\n",
    "    # Process the response stream\n",
    "    full_response = \"\"\n",
    "    stream = response.get(\"body\")\n",
    "    if stream:\n",
    "        for event in stream:\n",
    "            chunk = event.get(\"chunk\")\n",
    "            if chunk:\n",
    "                chunk_data = json.loads(chunk.get(\"bytes\").decode())\n",
    "                if chunk_data.get(\"type\") == \"content_block_delta\":\n",
    "                    delta = chunk_data.get(\"delta\", {})\n",
    "                    if \"text\" in delta:\n",
    "                        full_response += delta[\"text\"]\n",
    "\n",
    "    json_match = re.search(r'(\\{.*\\})', full_response, re.DOTALL)\n",
    "    if json_match:\n",
    "        json_str = json_match.group(1)\n",
    "        return json.loads(json_str)\n",
    "    else:\n",
    "        raise json.JSONDecodeError(\"No JSON found\", full_response, 0)\n",
    "\n",
    "\n",
    "model_name_to_id_map = {\"nova_product\": \"amazon.nova-lite-v1:0\",\n",
    "               \"nova_sales\" : \"amazon.nova-pro-v1:0\",\n",
    "               \"claude_v3_5\": \"anthropic.claude-3-5-sonnet-20240620-v1:0\"}\n",
    "\n",
    "model_name_to_id_map = {\"nova_product\": \"amazon.nova-lite-v1:0\",\n",
    "               \"nova_sales\" : \"amazon.nova-pro-v1:0\",\n",
    "               \"claude_v3_5\": \"us.anthropic.claude-3-5-haiku-20241022-v1:0\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "91c367cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-24T16:04:00.980068Z",
     "iopub.status.busy": "2025-07-24T16:04:00.979748Z",
     "iopub.status.idle": "2025-07-24T16:04:00.984341Z",
     "shell.execute_reply": "2025-07-24T16:04:00.983845Z",
     "shell.execute_reply.started": "2025-07-24T16:04:00.980053Z"
    }
   },
   "outputs": [],
   "source": [
    "bucket = 'my-example-name'\n",
    "raw_input = 'feedback_dummy_data.txt'\n",
    "output_themes = 'feedback_analyzed_claude.txt'\n",
    "\n",
    "def get_alignment_func(model_type):\n",
    "    if model_type == \"nova_product\":\n",
    "        return evaluate_alignment_nova_product\n",
    "    elif model_type == \"nova_sales\":\n",
    "        return evaluate_alignment_nova_sales\n",
    "    elif model_type == \"claude_v3_5\":\n",
    "        return evaluate_alignment_claude\n",
    "    else:\n",
    "        print(f\"Alignment function for model:{model_id} is not defined\")\n",
    "        raise NotImplementedError\n",
    "\n",
    "def evaluate_alignment_model(s3_client, model_name, bucket=bucket, key='feedback_analyzed.txt'):\n",
    "    # Rest of the code remains the same\n",
    "    # Read analyzed results\n",
    "    response = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "    content = response['Body'].read().decode('utf-8')\n",
    "\n",
    "    # Split into entries\n",
    "    entries = content.split('\\n\\n')\n",
    "\n",
    "    # Process each entry\n",
    "    output_content = \"\"\n",
    "    alignments = []\n",
    "    \n",
    "    alignment_func = get_alignment_func(model_name)\n",
    "\n",
    "    for i, entry in enumerate(entries):\n",
    "        if not entry.strip():\n",
    "            print(f\"{i+1} entry is empty. Skipping\")\n",
    "            continue\n",
    "        # Parse entry\n",
    "        lines = entry.strip().split('\\n')\n",
    "        entry_dict = {}\n",
    "        for line in lines:\n",
    "            if ':' in line:\n",
    "                key, value = line.split(':', 1)\n",
    "                entry_dict[key.strip()] = value.strip()\n",
    "\n",
    "        # # Get alignment score\n",
    "        \n",
    "        alignments.append(alignment_func(\n",
    "            entry_dict['Original Comment'],\n",
    "            entry_dict['main_theme'],\n",
    "            entry_dict['sub_theme'],\n",
    "            entry_dict['rationale'],\n",
    "            model_name_to_id_map[model_name]\n",
    "        ))\n",
    "        print(f\"Processed {i+1} / {len(entries)} entries with {model_name}\")\n",
    "    return alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8f56206d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-24T16:04:50.451084Z",
     "iopub.status.busy": "2025-07-24T16:04:50.450584Z",
     "iopub.status.idle": "2025-07-24T16:04:51.431408Z",
     "shell.execute_reply": "2025-07-24T16:04:51.430897Z",
     "shell.execute_reply.started": "2025-07-24T16:04:50.451062Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1 / 2 entries with nova_product\n",
      "Processed 2 / 2 entries with nova_product\n"
     ]
    }
   ],
   "source": [
    "alignment_dict = {}\n",
    "\n",
    "alignment_dict[\"nova_product\"] = evaluate_alignment_model(s3_client=s3_client, \n",
    "                                                  model_name='nova_product',\n",
    "                                                    key = output_themes)\n",
    "\n",
    "alignment_dict[\"nova_sales\"] = evaluate_alignment_model(s3_client=s3_client, \n",
    "                                                   model_name='nova_sales',\n",
    "                                                    key = output_themes)\n",
    "alignment_dict[\"claude\"] = evaluate_alignment_model(s3_client=s3_client, \n",
    "                                                    model_name='claude_v3_5',\n",
    "                                                    key = output_themes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d9642627",
   "metadata": {},
   "source": [
    "### Step 4. Implement Comparative Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e2244600",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-24T16:05:06.221617Z",
     "iopub.status.busy": "2025-07-24T16:05:06.221089Z",
     "iopub.status.idle": "2025-07-24T16:05:06.299775Z",
     "shell.execute_reply": "2025-07-24T16:05:06.299300Z",
     "shell.execute_reply.started": "2025-07-24T16:05:06.221596Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import simpledorff\n",
    "from scipy import stats\n",
    "import pprint\n",
    "\n",
    "\n",
    "def create_df(alignment_dict):\n",
    "    if not alignment_dict:\n",
    "        raise ValueError(\"Empty alignment dictionary provided\")\n",
    "\n",
    "    model_names = list(alignment_dict.keys())\n",
    "    res = pd.DataFrame(columns=model_names)\n",
    "    \n",
    "    # Get expected length from first model\n",
    "    N = len(alignment_dict[model_names[0]])\n",
    "    \n",
    "    # Validate all models have same length\n",
    "    for model_name, values in alignment_dict.items():\n",
    "        if len(values) != N:\n",
    "            raise ValueError(\n",
    "                f\"Inconsistent lengths detected: {model_name} has length {len(values)}, \"\n",
    "                f\"expected {N}\"\n",
    "            )\n",
    "    \n",
    "    # Create DataFrame\n",
    "    for i in range(N):\n",
    "        this_dict = {}\n",
    "        for model_name, v in alignment_dict.items():\n",
    "            this_dict[model_name] = v[i]['alignment_score']\n",
    "        res = pd.concat((res, pd.DataFrame.from_dict([this_dict])), ignore_index=True)\n",
    "                        \n",
    "    return res\n",
    "\n",
    "def calculate_percentage_agreement(df):\n",
    "    \"\"\"Calculate the percentage of perfect agreement between all raters\"\"\"\n",
    "    total_rows = len(df)\n",
    "    perfect_agreement_rows = sum(df.nunique(axis=1) == 1)\n",
    "    return (perfect_agreement_rows / total_rows) * 100\n",
    "\n",
    "def calculate_pairwise_cohens_kappa(df):\n",
    "    \"\"\"Calculate Cohen's Kappa for each pair of raters\"\"\"\n",
    "    models = df.columns\n",
    "    kappa_results = {}\n",
    "    \n",
    "    for i in range(len(models)):\n",
    "        for j in range(i+1, len(models)):\n",
    "            model1, model2 = models[i], models[j]\n",
    "            # Remove rows where either rater has missing values\n",
    "            valid_ratings = df[[model1, model2]].dropna()\n",
    "            \n",
    "            if len(valid_ratings) > 0:\n",
    "                kappa = cohen_kappa_score(\n",
    "                    valid_ratings[model1].astype(int),\n",
    "                    valid_ratings[model2].astype(int)\n",
    "                )\n",
    "                kappa_results[f\"{model1} vs {model2}\"] = kappa\n",
    "    \n",
    "    return kappa_results\n",
    "\n",
    "def calculate_krippendorffs_alpha(df): \n",
    "    \"\"\"Calculate Krippendorff's alpha for all raters\"\"\" \n",
    "    # Reshape the data into the format required by simpledorff \n",
    "    data = df.reset_index().melt(id_vars=['index'], var_name='annotator_id', value_name='annotation') \n",
    "    data = data.rename(columns={'index': 'document_id'}) \n",
    "    \n",
    "    return simpledorff.calculate_krippendorffs_alpha_for_df( \n",
    "        data, \n",
    "        experiment_col='document_id', \n",
    "        annotator_col='annotator_id', \n",
    "        class_col='annotation' \n",
    "    ) \n",
    "\n",
    "def calculate_spearmans_rho(df):\n",
    "    \"\"\"Calculate Spearman's rho for each pair of raters\"\"\"\n",
    "    models = df.columns\n",
    "    rho_results = {}\n",
    "    \n",
    "    for i in range(len(models)):\n",
    "        for j in range(i+1, len(models)):\n",
    "            model1, model2 = models[i], models[j]\n",
    "            # Remove rows where either rater has missing values\n",
    "            valid_ratings = df[[model1, model2]].dropna()\n",
    "            \n",
    "            if len(valid_ratings) > 0:\n",
    "                rho, p_value = stats.spearmanr(\n",
    "                    valid_ratings[model1],\n",
    "                    valid_ratings[model2]\n",
    "                )\n",
    "                rho_results[f\"{model1} vs {model2}\"] = {\n",
    "                    'rho': rho,\n",
    "                    'p_value': p_value\n",
    "                }\n",
    "    \n",
    "    return rho_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3deba3de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-24T16:14:35.809451Z",
     "iopub.status.busy": "2025-07-24T16:14:35.809178Z",
     "iopub.status.idle": "2025-07-24T16:14:35.830844Z",
     "shell.execute_reply": "2025-07-24T16:14:35.830402Z",
     "shell.execute_reply.started": "2025-07-24T16:14:35.809434Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1628/2301364946.py:87: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  rho, p_value = stats.spearmanr(\n",
      "/tmp/ipykernel_1628/2301364946.py:87: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  rho, p_value = stats.spearmanr(\n",
      "/tmp/ipykernel_1628/2301364946.py:87: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  rho, p_value = stats.spearmanr(\n"
     ]
    }
   ],
   "source": [
    "# Extract ratings into a DataFrame\n",
    "ratings_df = create_df(alignment_dict)\n",
    "\n",
    "# Calculate statistics\n",
    "stats_results = {\n",
    "    'Percentage Agreement': calculate_percentage_agreement(ratings_df),\n",
    "    'Cohens Kappa': calculate_pairwise_cohens_kappa(ratings_df),\n",
    "    'Krippendorffs Alpha': calculate_krippendorffs_alpha(ratings_df),\n",
    "    'Spearmans Rho': calculate_spearmans_rho(ratings_df)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0e42de5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-24T16:14:39.744799Z",
     "iopub.status.busy": "2025-07-24T16:14:39.744529Z",
     "iopub.status.idle": "2025-07-24T16:14:39.747981Z",
     "shell.execute_reply": "2025-07-24T16:14:39.747497Z",
     "shell.execute_reply.started": "2025-07-24T16:14:39.744783Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Cohens Kappa': {'nova_product vs claude': 0.0,\n",
      "                  'nova_product vs nova_sales': 0.0,\n",
      "                  'nova_sales vs claude': 0.0},\n",
      " 'Krippendorffs Alpha': -0.13636363636363624,\n",
      " 'Percentage Agreement': 0.0,\n",
      " 'Spearmans Rho': {'nova_product vs claude': {'p_value': nan, 'rho': nan},\n",
      "                   'nova_product vs nova_sales': {'p_value': nan, 'rho': nan},\n",
      "                   'nova_sales vs claude': {'p_value': nan, 'rho': nan}}}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(stats_results)"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
