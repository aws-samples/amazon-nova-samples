{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17af4dc5",
   "metadata": {},
   "source": [
    "# Thematic Analysis with LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f8b432",
   "metadata": {},
   "source": [
    "## This notebook implements a pipeline for thematic analysis of text data using Amazon Bedrock and Anthropic's Claude model, followed by evaluation of the generated themes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949b2f07",
   "metadata": {},
   "source": [
    "### Step 1. Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7497cf8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (1.37.31)\n",
      "Requirement already satisfied: anthropic in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (0.54.0)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (1.5.3)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (1.26.4)\n",
      "Requirement already satisfied: statsmodels in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (0.14.4)\n",
      "Requirement already satisfied: krippendorff in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (0.8.1)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (1.15.1)\n",
      "Requirement already satisfied: botocore<1.38.0,>=1.37.31 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3) (1.37.31)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.12.0,>=0.11.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3) (0.11.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anthropic) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anthropic) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.25.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anthropic) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anthropic) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anthropic) (2.11.3)\n",
      "Requirement already satisfied: sniffio in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anthropic) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anthropic) (4.12.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: patsy>=0.5.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from statsmodels) (1.0.1)\n",
      "Requirement already satisfied: packaging>=21.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from statsmodels) (21.3)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio<5,>=3.5.0->anthropic) (1.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio<5,>=3.5.0->anthropic) (3.10)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore<1.38.0,>=1.37.31->boto3) (2.3.0)\n",
      "Requirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx<1,>=0.25.0->anthropic) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx<1,>=0.25.0->anthropic) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic) (0.14.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from packaging>=21.3->statsmodels) (3.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->anthropic) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->anthropic) (0.4.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install boto3 anthropic pandas numpy statsmodels krippendorff scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2127d68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model ID: amazon.titan-tg1-large\n",
      "Model ID: amazon.titan-image-generator-v1:0\n",
      "Model ID: amazon.titan-image-generator-v1\n",
      "Model ID: amazon.titan-image-generator-v2:0\n",
      "Model ID: amazon.nova-premier-v1:0:8k\n",
      "Model ID: amazon.nova-premier-v1:0:20k\n",
      "Model ID: amazon.nova-premier-v1:0:1000k\n",
      "Model ID: amazon.nova-premier-v1:0:mm\n",
      "Model ID: amazon.nova-premier-v1:0\n",
      "Model ID: amazon.titan-text-premier-v1:0\n",
      "Model ID: amazon.nova-pro-v1:0:24k\n",
      "Model ID: amazon.nova-pro-v1:0:300k\n",
      "Model ID: amazon.nova-pro-v1:0\n",
      "Model ID: amazon.nova-lite-v1:0:24k\n",
      "Model ID: amazon.nova-lite-v1:0:300k\n",
      "Model ID: amazon.nova-lite-v1:0\n",
      "Model ID: amazon.nova-canvas-v1:0\n",
      "Model ID: amazon.nova-reel-v1:0\n",
      "Model ID: amazon.nova-reel-v1:1\n",
      "Model ID: amazon.nova-micro-v1:0:24k\n",
      "Model ID: amazon.nova-micro-v1:0:128k\n",
      "Model ID: amazon.nova-micro-v1:0\n",
      "Model ID: amazon.nova-sonic-v1:0\n",
      "Model ID: amazon.titan-embed-g1-text-02\n",
      "Model ID: amazon.titan-text-lite-v1:0:4k\n",
      "Model ID: amazon.titan-text-lite-v1\n",
      "Model ID: amazon.titan-text-express-v1:0:8k\n",
      "Model ID: amazon.titan-text-express-v1\n",
      "Model ID: amazon.titan-embed-text-v1:2:8k\n",
      "Model ID: amazon.titan-embed-text-v1\n",
      "Model ID: amazon.titan-embed-text-v2:0:8k\n",
      "Model ID: amazon.titan-embed-text-v2:0\n",
      "Model ID: amazon.titan-embed-image-v1:0\n",
      "Model ID: amazon.titan-embed-image-v1\n",
      "Model ID: stability.stable-diffusion-xl-v1:0\n",
      "Model ID: stability.stable-diffusion-xl-v1\n",
      "Model ID: ai21.jamba-instruct-v1:0\n",
      "Model ID: ai21.jamba-1-5-large-v1:0\n",
      "Model ID: ai21.jamba-1-5-mini-v1:0\n",
      "Model ID: anthropic.claude-instant-v1:2:100k\n",
      "Model ID: anthropic.claude-instant-v1\n",
      "Model ID: anthropic.claude-v2:0:18k\n",
      "Model ID: anthropic.claude-v2:0:100k\n",
      "Model ID: anthropic.claude-v2:1:18k\n",
      "Model ID: anthropic.claude-v2:1:200k\n",
      "Model ID: anthropic.claude-v2:1\n",
      "Model ID: anthropic.claude-v2\n",
      "Model ID: anthropic.claude-3-sonnet-20240229-v1:0:28k\n",
      "Model ID: anthropic.claude-3-sonnet-20240229-v1:0:200k\n",
      "Model ID: anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Model ID: anthropic.claude-3-haiku-20240307-v1:0:48k\n",
      "Model ID: anthropic.claude-3-haiku-20240307-v1:0:200k\n",
      "Model ID: anthropic.claude-3-haiku-20240307-v1:0\n",
      "Model ID: anthropic.claude-3-opus-20240229-v1:0:12k\n",
      "Model ID: anthropic.claude-3-opus-20240229-v1:0:28k\n",
      "Model ID: anthropic.claude-3-opus-20240229-v1:0:200k\n",
      "Model ID: anthropic.claude-3-opus-20240229-v1:0\n",
      "Model ID: anthropic.claude-3-5-sonnet-20240620-v1:0\n",
      "Model ID: anthropic.claude-3-5-sonnet-20241022-v2:0\n",
      "Model ID: anthropic.claude-3-7-sonnet-20250219-v1:0\n",
      "Model ID: anthropic.claude-3-5-haiku-20241022-v1:0\n",
      "Model ID: anthropic.claude-opus-4-20250514-v1:0\n",
      "Model ID: anthropic.claude-sonnet-4-20250514-v1:0\n",
      "Model ID: cohere.command-text-v14:7:4k\n",
      "Model ID: cohere.command-text-v14\n",
      "Model ID: cohere.command-r-v1:0\n",
      "Model ID: cohere.command-r-plus-v1:0\n",
      "Model ID: cohere.command-light-text-v14:7:4k\n",
      "Model ID: cohere.command-light-text-v14\n",
      "Model ID: cohere.embed-english-v3:0:512\n",
      "Model ID: cohere.embed-english-v3\n",
      "Model ID: cohere.embed-multilingual-v3:0:512\n",
      "Model ID: cohere.embed-multilingual-v3\n",
      "Model ID: deepseek.r1-v1:0\n",
      "Model ID: meta.llama3-8b-instruct-v1:0\n",
      "Model ID: meta.llama3-70b-instruct-v1:0\n",
      "Model ID: meta.llama3-1-8b-instruct-v1:0\n",
      "Model ID: meta.llama3-1-70b-instruct-v1:0\n",
      "Model ID: meta.llama3-2-11b-instruct-v1:0\n",
      "Model ID: meta.llama3-2-90b-instruct-v1:0\n",
      "Model ID: meta.llama3-2-1b-instruct-v1:0\n",
      "Model ID: meta.llama3-2-3b-instruct-v1:0\n",
      "Model ID: meta.llama3-3-70b-instruct-v1:0\n",
      "Model ID: meta.llama4-scout-17b-instruct-v1:0\n",
      "Model ID: meta.llama4-maverick-17b-instruct-v1:0\n",
      "Model ID: mistral.mistral-7b-instruct-v0:2\n",
      "Model ID: mistral.mixtral-8x7b-instruct-v0:1\n",
      "Model ID: mistral.mistral-large-2402-v1:0\n",
      "Model ID: mistral.mistral-small-2402-v1:0\n",
      "Model ID: mistral.pixtral-large-2502-v1:0\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "from datetime import datetime\n",
    "from botocore.config import Config\n",
    "\n",
    "\n",
    "# Initialize S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "#Optional: Check which models you have access to \n",
    "bedrock = boto3.client('bedrock')\n",
    "response = bedrock.list_foundation_models()\n",
    "\n",
    "# Initialize AWS clients\n",
    "bedrock_runtime = boto3.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name='us-east-1',  # Important: Replace with your region if different\n",
    "    config=Config(\n",
    "        signature_version='v4',\n",
    "        retries={\n",
    "            'max_attempts': 3,\n",
    "            'mode': 'standard'\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "# Optional: Print available models\n",
    "for model in response['modelSummaries']:\n",
    "    print(f\"Model ID: {model['modelId']}\")\n",
    "    if model[\"inferenceTypesSupported\"] == \"ON_DEMAND\":\n",
    "        print(model)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6b48efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the S3 for input text file as well as to upload the output\n",
    "bucket = 'drsre-bucket'\n",
    "raw_input = 'feedback_dummy_data.txt'\n",
    "output_themes = 'feedback_analyzed.txt'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "57c19d7b",
   "metadata": {},
   "source": [
    "### Step 2: Generate Thematic Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b212d10-c2aa-406e-b2cf-7c0c59e5c176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_comment(comment, model_id=\"amazon.nova-pro-v1:0\"):\n",
    "    \"\"\"\n",
    "    Analyze customer comments using Nova Pro in a structured format.\n",
    "    Returns thematic analysis of customer feedback.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define system prompt\n",
    "    system_list = [\n",
    "        {\n",
    "            \"text\": \"You are a Customer Feedback Analyst responsible for identifying key themes and concerns in customer reviews. Your role is to extract meaningful insights that can drive product improvements within the team.\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Define the user message\n",
    "    message_list = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"text\": f\"\"\"Analyze this customer review and provide insights in JSON format:\n",
    "\n",
    "Review: \"{comment}\"\n",
    "\n",
    "Return only this JSON structure with your analysis:\n",
    "{{\n",
    "    \"main_theme\": \"primary theme identified\",\n",
    "    \"sub_theme\": \"secondary theme or specific aspect\",\n",
    "    \"rationale\": \"explanation of thematic analysis\"\n",
    "}}\n",
    "\n",
    "Provide only the JSON response without additional text.\"\"\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Configure inference parameters\n",
    "    inf_params = {\n",
    "        \"maxTokens\": 1000,\n",
    "        \"temperature\": 0.1,\n",
    "        \"topP\": 0.9,\n",
    "        \"topK\": 20\n",
    "    }\n",
    "\n",
    "    # Construct the request body\n",
    "    request_body = {\n",
    "        \"schemaVersion\": \"messages-v1\",\n",
    "        \"messages\": message_list,\n",
    "        \"system\": system_list,\n",
    "        \"inferenceConfig\": inf_params\n",
    "    }\n",
    "\n",
    "    # Make the request to Nova Pro\n",
    "    response = bedrock_runtime.invoke_model_with_response_stream(\n",
    "        modelId=model_id,\n",
    "        body=json.dumps(request_body)\n",
    "    )\n",
    "\n",
    "    # Process the response stream\n",
    "    full_response = \"\"\n",
    "    stream = response.get(\"body\")\n",
    "    if stream:\n",
    "        for event in stream:\n",
    "            chunk = event.get(\"chunk\")\n",
    "            if chunk:\n",
    "                chunk_json = json.loads(chunk.get(\"bytes\").decode())\n",
    "                content_block_delta = chunk_json.get(\"contentBlockDelta\")\n",
    "                if content_block_delta:\n",
    "                    full_response += content_block_delta.get(\"delta\").get(\"text\", \"\")\n",
    "\n",
    "    # Extract and validate JSON response\n",
    "    try:\n",
    "        json_start = full_response.find('{')\n",
    "        json_end = full_response.rfind('}') + 1\n",
    "        if json_start >= 0 and json_end > json_start:\n",
    "            json_str = full_response[json_start:json_end]\n",
    "            return json.loads(json_str)\n",
    "        else:\n",
    "            raise json.JSONDecodeError(\"No JSON found\", full_response, 0)\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"main_theme\": \"Error in analysis\",\n",
    "            \"sub_theme\": \"Processing error\",\n",
    "            \"rationale\": str(e)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16fbf4b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing comment 1 of 50\n",
      "Analyzing comment 2 of 50\n",
      "Analyzing comment 3 of 50\n",
      "Analyzing comment 4 of 50\n",
      "Analyzing comment 5 of 50\n",
      "Analyzing comment 6 of 50\n",
      "Analyzing comment 7 of 50\n",
      "Analyzing comment 8 of 50\n",
      "Analyzing comment 9 of 50\n",
      "Analyzing comment 10 of 50\n",
      "Analyzing comment 11 of 50\n",
      "Analyzing comment 12 of 50\n",
      "Analyzing comment 13 of 50\n",
      "Analyzing comment 14 of 50\n",
      "Analyzing comment 15 of 50\n",
      "Analyzing comment 16 of 50\n",
      "Analyzing comment 17 of 50\n",
      "Analyzing comment 18 of 50\n",
      "Analyzing comment 19 of 50\n",
      "Analyzing comment 20 of 50\n",
      "Analyzing comment 21 of 50\n",
      "Analyzing comment 22 of 50\n",
      "Analyzing comment 23 of 50\n",
      "Analyzing comment 24 of 50\n",
      "Analyzing comment 25 of 50\n",
      "Analyzing comment 26 of 50\n",
      "Analyzing comment 27 of 50\n",
      "Analyzing comment 28 of 50\n",
      "Analyzing comment 29 of 50\n",
      "Analyzing comment 30 of 50\n",
      "Analyzing comment 31 of 50\n",
      "Analyzing comment 32 of 50\n",
      "Analyzing comment 33 of 50\n",
      "Analyzing comment 34 of 50\n",
      "Analyzing comment 35 of 50\n",
      "Analyzing comment 36 of 50\n",
      "Analyzing comment 37 of 50\n",
      "Analyzing comment 38 of 50\n",
      "Analyzing comment 39 of 50\n",
      "Analyzing comment 40 of 50\n",
      "Analyzing comment 41 of 50\n",
      "Analyzing comment 42 of 50\n",
      "Analyzing comment 43 of 50\n",
      "Analyzing comment 44 of 50\n",
      "Analyzing comment 45 of 50\n",
      "Analyzing comment 46 of 50\n",
      "Analyzing comment 47 of 50\n",
      "Analyzing comment 48 of 50\n",
      "Analyzing comment 49 of 50\n",
      "Analyzing comment 50 of 50\n",
      "Analysis complete. Results saved to s3://drsre-bucket/feedback_analyzed.txt\n"
     ]
    }
   ],
   "source": [
    "response = s3_client.get_object(Bucket=bucket, Key=raw_input)\n",
    "feedbacks = []\n",
    "\n",
    "for line in response['Body'].iter_lines():\n",
    "    decoded_line = line.decode('utf-8')\n",
    "    feedbacks.append(decoded_line)\n",
    "\n",
    "# Analyze each comment and store results\n",
    "analyzed_results = []\n",
    "output_content = \"\"\n",
    "\n",
    "for i, feedback in enumerate(feedbacks):\n",
    "    if feedback.strip():  # Skip empty lines\n",
    "        print(f\"Analyzing comment {i+1} of {len(feedbacks)}\")\n",
    "        analysis = analyze_comment(feedback)\n",
    "\n",
    "#Optional: For debugging\n",
    "#         print(\"Raw response for feedback:\", feedback)\n",
    "#         print(analysis)\n",
    "\n",
    "        result = {\n",
    "            \"original_comment\": feedback,\n",
    "            \"analysis\": analysis\n",
    "        }\n",
    "        analyzed_results.append(result)\n",
    "\n",
    "# Format output for each result\n",
    "        entry = {\n",
    "            \"Original Comment\": feedback,\n",
    "            \"main_theme\": analysis['main_theme'],\n",
    "            \"sub_theme\": analysis['sub_theme'],\n",
    "            \"rationale\": analysis['rationale']\n",
    "        }\n",
    "        output_content += \"\\n\".join(f\"{k}: {v}\" for k, v in entry.items()) + \"\\n\\n\"\n",
    "\n",
    "# Save results back to S3\n",
    "s3_client.put_object(\n",
    "    Bucket=bucket,\n",
    "    Key=output_themes,\n",
    "    Body=output_content.encode('utf-8')\n",
    ")\n",
    "\n",
    "print(f\"Analysis complete. Results saved to s3://{bucket}/{output_themes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae87ed3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entry 1:\n",
      "Original Comment: Affordable and reliable.\n",
      "main_theme: positive feedback\n",
      "sub_theme: affordability and reliability\n",
      "rationale: The review highlights two key attributes of the product: it is both affordable and reliable, indicating customer satisfaction with both the price and performance.\n",
      "\n",
      "Entry 2:\n",
      "Original Comment: Packaging was damaged upon arrival.\n",
      "main_theme: packaging\n",
      "sub_theme: damage\n",
      "rationale: The review specifically mentions that the packaging was damaged upon arrival, indicating a problem with how the product was packaged and handled during transit.\n",
      "\n",
      "Entry 3:\n",
      "Original Comment: Average experience, could be better.\n",
      "main_theme: customer satisfaction\n",
      "sub_theme: overall experience\n",
      "rationale: the review indicates a neutral to slightly negative sentiment towards the product or service, suggesting room for improvement in the overall customer experience\n"
     ]
    }
   ],
   "source": [
    "#Optional: Read the analyzed results file\n",
    "response = s3_client.get_object(Bucket=bucket, Key=output_themes)\n",
    "content = response['Body'].read().decode('utf-8')\n",
    "\n",
    "# Split by double newlines since that's how entries are separated\n",
    "entries = content.split('\\n\\n')\n",
    "\n",
    "\n",
    "# Display first k entries \n",
    "k = 3\n",
    "for i, entry in enumerate(entries[:k]):\n",
    "    if entry.strip():  # Skip empty entries\n",
    "        print(f\"\\nEntry {i+1}:\")\n",
    "        print(entry.strip())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a629e04e",
   "metadata": {},
   "source": [
    "### Step 3. Deploy multiple pre-trained LLMs as judges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c639c1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Deploying Nova Pro in a Product Researcher persona\n",
    "\n",
    "def evaluate_alignment_nova_product(comment, theme, subtheme, rationale, model_id):\n",
    "    \"\"\"Evaluate theme alignment using Nova model\"\"\"\n",
    "    \n",
    "    # Define system prompt\n",
    "    system_list = [\n",
    "        {\n",
    "            \"text\": \"You are a Product Researcher analyzing customer feedback. Your role is to evaluate how accurately our thematic analysis captures customer sentiments and experiences. Use your expertise in customer insights and product research to assess theme alignment.\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Define the user message\n",
    "    message_list = [\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"text\": f\"\"\"As a Product Researcher, evaluate the theme alignment using this scale:\n",
    "1: Poor - Our thematic analysis misses key customer product requirements\n",
    "2: Partial - Our analysis captures some but not all important product requirements feedback\n",
    "3: Strong - Our thematic analysis effectively captures the customer's requirements\n",
    "\n",
    "CUSTOMER FEEDBACK ANALYSIS:\n",
    "Voice of Customer: \"{comment}\"\n",
    "Primary Theme: {theme}\n",
    "Secondary Theme: {subtheme} \n",
    "Analysis Rationale: {rationale}\n",
    "\n",
    "Provide your research assessment in this JSON format:\n",
    "{{\n",
    "    \"alignment_score\": <1, 2, or 3>,\n",
    "    \"justification\": \"brief explanation of your evaluation from a research perspective\"\n",
    "}}\n",
    "\n",
    "Return only valid JSON with no additional commentary.\"\"\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Configure inference parameters\n",
    "    inf_params = {\n",
    "        \"maxTokens\": 500,\n",
    "        \"topP\": 0.9,\n",
    "        \"topK\": 20,\n",
    "        \"temperature\": 0.1\n",
    "    }\n",
    "\n",
    "    # Construct the request body\n",
    "    request_body = {\n",
    "        \"schemaVersion\": \"messages-v1\",\n",
    "        \"messages\": message_list,\n",
    "        \"system\": system_list,\n",
    "        \"inferenceConfig\": inf_params,\n",
    "    }\n",
    "\n",
    "    # Make the request\n",
    "    response = bedrock_runtime.invoke_model_with_response_stream(\n",
    "        modelId=model_id,\n",
    "        body=json.dumps(request_body)\n",
    "    )\n",
    "\n",
    "    # Process the response stream\n",
    "    full_response = \"\"\n",
    "    stream = response.get(\"body\")\n",
    "    if stream:\n",
    "        for event in stream:\n",
    "            chunk = event.get(\"chunk\")\n",
    "            if chunk:\n",
    "                chunk_json = json.loads(chunk.get(\"bytes\").decode())\n",
    "                content_block_delta = chunk_json.get(\"contentBlockDelta\")\n",
    "                if content_block_delta:\n",
    "                    full_response += content_block_delta.get(\"delta\").get(\"text\", \"\")\n",
    "\n",
    "    # Extract JSON from response\n",
    "    json_start = full_response.find('{')\n",
    "    json_end = full_response.rfind('}') + 1\n",
    "    if json_start >= 0 and json_end > json_start:\n",
    "        json_str = full_response[json_start:json_end]\n",
    "        return json.loads(json_str)\n",
    "    else:\n",
    "        raise json.JSONDecodeError(\"No JSON found\", full_response, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b3304dd-05d9-4189-acba-fe9af1e9758e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Deploying Nova Pro in a Sales persona\n",
    "\n",
    "def evaluate_alignment_nova_sales(comment, theme, subtheme, rationale, model_id):\n",
    "    \"\"\"Evaluate theme alignment using Nova model\"\"\"\n",
    "    \n",
    "    # Define system prompt\n",
    "    system_list = [\n",
    "        {\n",
    "            \"text\": \"You are a Customer Experience Advocate whose primary focus is understanding the emotional context and underlying customer needs. Your role is to analyze feedback from the customer's perspective, identifying both explicit and implicit emotional signals. Use your expertise in customer sales to assess theme alignment.\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Define the user message\n",
    "    message_list = [\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"text\": f\"\"\"As a Sales Representative, evaluate the theme alignment using this scale:\n",
    "1: Poor - The thematic analysis misses key customer sentiments\n",
    "2: Partial - The analysis captures some but not all important sentiments\n",
    "3: Strong - The thematic analysis effectively captures all the customer's core sentiments\n",
    "\n",
    "CUSTOMER FEEDBACK ANALYSIS:\n",
    "Voice of Customer: \"{comment}\"\n",
    "Primary Theme: {theme}\n",
    "Secondary Theme: {subtheme} \n",
    "Analysis Rationale: {rationale}\n",
    "\n",
    "Provide your research assessment in this JSON format:\n",
    "{{\n",
    "    \"alignment_score\": <1, 2, or 3>,\n",
    "    \"justification\": \"brief explanation of your evaluation from a research perspective\"\n",
    "}}\n",
    "\n",
    "Return only valid JSON with no additional commentary.\"\"\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Configure inference parameters\n",
    "    inf_params = {\n",
    "        \"maxTokens\": 500,\n",
    "        \"topP\": 0.9,\n",
    "        \"topK\": 20,\n",
    "        \"temperature\": 0.1\n",
    "    }\n",
    "\n",
    "    # Construct the request body\n",
    "    request_body = {\n",
    "        \"schemaVersion\": \"messages-v1\",\n",
    "        \"messages\": message_list,\n",
    "        \"system\": system_list,\n",
    "        \"inferenceConfig\": inf_params,\n",
    "    }\n",
    "\n",
    "    # Make the request\n",
    "    response = bedrock_runtime.invoke_model_with_response_stream(\n",
    "        modelId=model_id,\n",
    "        body=json.dumps(request_body)\n",
    "    )\n",
    "\n",
    "    # Process the response stream\n",
    "    full_response = \"\"\n",
    "    stream = response.get(\"body\")\n",
    "    if stream:\n",
    "        for event in stream:\n",
    "            chunk = event.get(\"chunk\")\n",
    "            if chunk:\n",
    "                chunk_json = json.loads(chunk.get(\"bytes\").decode())\n",
    "                content_block_delta = chunk_json.get(\"contentBlockDelta\")\n",
    "                if content_block_delta:\n",
    "                    full_response += content_block_delta.get(\"delta\").get(\"text\", \"\")\n",
    "\n",
    "    # Extract JSON from response\n",
    "    json_start = full_response.find('{')\n",
    "    json_end = full_response.rfind('}') + 1\n",
    "    if json_start >= 0 and json_end > json_start:\n",
    "        json_str = full_response[json_start:json_end]\n",
    "        return json.loads(json_str)\n",
    "    else:\n",
    "        raise json.JSONDecodeError(\"No JSON found\", full_response, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7f12fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Deploying Claude 3.5 Sonnet \n",
    "\n",
    "def evaluate_alignment_claude(comment, theme, subtheme, rationale, model_id):\n",
    "    \"\"\"Evaluate theme alignment using Claude model\"\"\"\n",
    "    \n",
    "    # In Claude's API format, system message is a separate top-level parameter\n",
    "    system_message = \"You are a customer research executive evaluating how accurately themes match a customer comment. Provide your evaluation as JSON.\"\n",
    "    \n",
    "    # Define the messages (only user and assistant roles allowed)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": f\"\"\"Rate ONLY the theme alignment on this scale:\n",
    "1: Poor match - themes don't capture the main points\n",
    "2: Partial match - themes capture some but not all key points\n",
    "3: Strong match - themes accurately capture the main points\n",
    "\n",
    "REVIEW DETAILS:\n",
    "Comment: \"{comment}\"\n",
    "Main Theme: {theme}\n",
    "Sub-theme: {subtheme} \n",
    "Rationale: {rationale}\n",
    "\n",
    "Respond ONLY with this JSON structure:\n",
    "{{\n",
    "    \"alignment_score\": <1, 2, or 3>,\n",
    "    \"justification\": \"brief explanation of score\"\n",
    "}}\n",
    "\n",
    "Your response must be valid JSON with no other text.\"\"\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Construct the request body according to Claude's API format\n",
    "    request_body = {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"system\": system_message,\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": 500,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.9\n",
    "    }\n",
    "\n",
    "    # Make the request\n",
    "    response = bedrock_runtime.invoke_model_with_response_stream(\n",
    "        modelId=model_id,\n",
    "        body=json.dumps(request_body)\n",
    "    )\n",
    "\n",
    "    # Process the response stream\n",
    "    full_response = \"\"\n",
    "    stream = response.get(\"body\")\n",
    "    if stream:\n",
    "        for event in stream:\n",
    "            chunk = event.get(\"chunk\")\n",
    "            if chunk:\n",
    "                chunk_data = json.loads(chunk.get(\"bytes\").decode())\n",
    "                if chunk_data.get(\"type\") == \"content_block_delta\":\n",
    "                    delta = chunk_data.get(\"delta\", {})\n",
    "                    if \"text\" in delta:\n",
    "                        full_response += delta[\"text\"]\n",
    "\n",
    "    # Extract JSON from response\n",
    "    json_start = full_response.find('{')\n",
    "    json_end = full_response.rfind('}') + 1\n",
    "    if json_start >= 0 and json_end > json_start:\n",
    "        json_str = full_response[json_start:json_end]\n",
    "        return json.loads(json_str)\n",
    "    else:\n",
    "        raise json.JSONDecodeError(\"No JSON found\", full_response, 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "91c367cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name_to_id_map = {\"nova_product\": \"amazon.nova-pro-v1:0\",\n",
    "#                \"nova_sales\" : \"amazon.nova-pro-v1:0\",\n",
    "#                \"claude_v3_5\": \"anthropic.claude-3-5-sonnet-20240620-v1:0\"}\n",
    "\n",
    "model_name_to_id_map = {\"nova_product\": \"amazon.nova-lite-v1:0\",\n",
    "               \"nova_sales\" : \"amazon.nova-pro-v1:0\",\n",
    "               \"claude_v3_5\": \"anthropic.claude-3-5-sonnet-20240620-v1:0\"}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_alignment_func(model_type):\n",
    "    if model_type == \"nova_product\":\n",
    "        return evaluate_alignment_nova_product\n",
    "    elif model_type == \"nova_sales\":\n",
    "        return evaluate_alignment_nova_sales\n",
    "    elif model_type == \"claude_v3_5\":\n",
    "        return evaluate_alignment_claude\n",
    "    else:\n",
    "        print(f\"Alignment function for model:{model_id} is not defined\")\n",
    "        raise NotImplementedError\n",
    "\n",
    "def evaluate_alignment_model(s3_client, model_name, bucket=bucket, key='feedback_analyzed.txt'):\n",
    "    # Rest of the code remains the same\n",
    "    # Read analyzed results\n",
    "    response = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "    content = response['Body'].read().decode('utf-8')\n",
    "\n",
    "    # Split into entries\n",
    "    entries = content.split('\\n\\n')\n",
    "\n",
    "    # Process each entry\n",
    "    output_content = \"\"\n",
    "    alignments = []\n",
    "    \n",
    "    alignment_func = get_alignment_func(model_name)\n",
    "\n",
    "    for i, entry in enumerate(entries):\n",
    "        if not entry.strip():\n",
    "            print(f\"{i+1} entry is empty. Skipping\")\n",
    "            continue\n",
    "        # Parse entry\n",
    "        lines = entry.strip().split('\\n')\n",
    "        entry_dict = {}\n",
    "        for line in lines:\n",
    "            if ':' in line:\n",
    "                key, value = line.split(':', 1)\n",
    "                entry_dict[key.strip()] = value.strip()\n",
    "\n",
    "        # # Get alignment score\n",
    "        \n",
    "        alignments.append(alignment_func(\n",
    "            entry_dict['Original Comment'],\n",
    "            entry_dict['main_theme'],\n",
    "            entry_dict['sub_theme'],\n",
    "            entry_dict['rationale'],\n",
    "            model_name_to_id_map[model_name]\n",
    "        ))\n",
    "        print(f\"Processed {i+1} / {len(entries)} entries with {model_name}\")\n",
    "    return alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8f56206d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1 / 51 entries with nova_product\n",
      "Processed 2 / 51 entries with nova_product\n",
      "Processed 3 / 51 entries with nova_product\n",
      "Processed 4 / 51 entries with nova_product\n",
      "Processed 5 / 51 entries with nova_product\n",
      "Processed 6 / 51 entries with nova_product\n",
      "Processed 7 / 51 entries with nova_product\n",
      "Processed 8 / 51 entries with nova_product\n",
      "Processed 9 / 51 entries with nova_product\n",
      "Processed 10 / 51 entries with nova_product\n",
      "Processed 11 / 51 entries with nova_product\n",
      "Processed 12 / 51 entries with nova_product\n",
      "Processed 13 / 51 entries with nova_product\n",
      "Processed 14 / 51 entries with nova_product\n",
      "Processed 15 / 51 entries with nova_product\n",
      "Processed 16 / 51 entries with nova_product\n",
      "Processed 17 / 51 entries with nova_product\n",
      "Processed 18 / 51 entries with nova_product\n",
      "Processed 19 / 51 entries with nova_product\n",
      "Processed 20 / 51 entries with nova_product\n",
      "Processed 21 / 51 entries with nova_product\n",
      "Processed 22 / 51 entries with nova_product\n",
      "Processed 23 / 51 entries with nova_product\n",
      "Processed 24 / 51 entries with nova_product\n",
      "Processed 25 / 51 entries with nova_product\n",
      "Processed 26 / 51 entries with nova_product\n",
      "Processed 27 / 51 entries with nova_product\n",
      "Processed 28 / 51 entries with nova_product\n",
      "Processed 29 / 51 entries with nova_product\n",
      "Processed 30 / 51 entries with nova_product\n",
      "Processed 31 / 51 entries with nova_product\n",
      "Processed 32 / 51 entries with nova_product\n",
      "Processed 33 / 51 entries with nova_product\n",
      "Processed 34 / 51 entries with nova_product\n",
      "Processed 35 / 51 entries with nova_product\n",
      "Processed 36 / 51 entries with nova_product\n",
      "Processed 37 / 51 entries with nova_product\n",
      "Processed 38 / 51 entries with nova_product\n",
      "Processed 39 / 51 entries with nova_product\n",
      "Processed 40 / 51 entries with nova_product\n",
      "Processed 41 / 51 entries with nova_product\n",
      "Processed 42 / 51 entries with nova_product\n",
      "Processed 43 / 51 entries with nova_product\n",
      "Processed 44 / 51 entries with nova_product\n",
      "Processed 45 / 51 entries with nova_product\n",
      "Processed 46 / 51 entries with nova_product\n",
      "Processed 47 / 51 entries with nova_product\n",
      "Processed 48 / 51 entries with nova_product\n",
      "Processed 49 / 51 entries with nova_product\n",
      "Processed 50 / 51 entries with nova_product\n",
      "51 entry is empty. Skipping\n",
      "Processed 1 / 51 entries with nova_sales\n",
      "Processed 2 / 51 entries with nova_sales\n",
      "Processed 3 / 51 entries with nova_sales\n",
      "Processed 4 / 51 entries with nova_sales\n",
      "Processed 5 / 51 entries with nova_sales\n",
      "Processed 6 / 51 entries with nova_sales\n",
      "Processed 7 / 51 entries with nova_sales\n",
      "Processed 8 / 51 entries with nova_sales\n",
      "Processed 9 / 51 entries with nova_sales\n",
      "Processed 10 / 51 entries with nova_sales\n",
      "Processed 11 / 51 entries with nova_sales\n",
      "Processed 12 / 51 entries with nova_sales\n",
      "Processed 13 / 51 entries with nova_sales\n",
      "Processed 14 / 51 entries with nova_sales\n",
      "Processed 15 / 51 entries with nova_sales\n",
      "Processed 16 / 51 entries with nova_sales\n",
      "Processed 17 / 51 entries with nova_sales\n",
      "Processed 18 / 51 entries with nova_sales\n",
      "Processed 19 / 51 entries with nova_sales\n",
      "Processed 20 / 51 entries with nova_sales\n",
      "Processed 21 / 51 entries with nova_sales\n",
      "Processed 22 / 51 entries with nova_sales\n",
      "Processed 23 / 51 entries with nova_sales\n",
      "Processed 24 / 51 entries with nova_sales\n",
      "Processed 25 / 51 entries with nova_sales\n",
      "Processed 26 / 51 entries with nova_sales\n",
      "Processed 27 / 51 entries with nova_sales\n",
      "Processed 28 / 51 entries with nova_sales\n",
      "Processed 29 / 51 entries with nova_sales\n",
      "Processed 30 / 51 entries with nova_sales\n",
      "Processed 31 / 51 entries with nova_sales\n",
      "Processed 32 / 51 entries with nova_sales\n",
      "Processed 33 / 51 entries with nova_sales\n",
      "Processed 34 / 51 entries with nova_sales\n",
      "Processed 35 / 51 entries with nova_sales\n",
      "Processed 36 / 51 entries with nova_sales\n",
      "Processed 37 / 51 entries with nova_sales\n",
      "Processed 38 / 51 entries with nova_sales\n",
      "Processed 39 / 51 entries with nova_sales\n",
      "Processed 40 / 51 entries with nova_sales\n",
      "Processed 41 / 51 entries with nova_sales\n",
      "Processed 42 / 51 entries with nova_sales\n",
      "Processed 43 / 51 entries with nova_sales\n",
      "Processed 44 / 51 entries with nova_sales\n",
      "Processed 45 / 51 entries with nova_sales\n",
      "Processed 46 / 51 entries with nova_sales\n",
      "Processed 47 / 51 entries with nova_sales\n",
      "Processed 48 / 51 entries with nova_sales\n",
      "Processed 49 / 51 entries with nova_sales\n",
      "Processed 50 / 51 entries with nova_sales\n",
      "51 entry is empty. Skipping\n",
      "Processed 1 / 51 entries with claude_v3_5\n",
      "Processed 2 / 51 entries with claude_v3_5\n",
      "Processed 3 / 51 entries with claude_v3_5\n",
      "Processed 4 / 51 entries with claude_v3_5\n",
      "Processed 5 / 51 entries with claude_v3_5\n",
      "Processed 6 / 51 entries with claude_v3_5\n",
      "Processed 7 / 51 entries with claude_v3_5\n",
      "Processed 8 / 51 entries with claude_v3_5\n",
      "Processed 9 / 51 entries with claude_v3_5\n",
      "Processed 10 / 51 entries with claude_v3_5\n",
      "Processed 11 / 51 entries with claude_v3_5\n",
      "Processed 12 / 51 entries with claude_v3_5\n",
      "Processed 13 / 51 entries with claude_v3_5\n",
      "Processed 14 / 51 entries with claude_v3_5\n",
      "Processed 15 / 51 entries with claude_v3_5\n",
      "Processed 16 / 51 entries with claude_v3_5\n",
      "Processed 17 / 51 entries with claude_v3_5\n",
      "Processed 18 / 51 entries with claude_v3_5\n",
      "Processed 19 / 51 entries with claude_v3_5\n",
      "Processed 20 / 51 entries with claude_v3_5\n",
      "Processed 21 / 51 entries with claude_v3_5\n",
      "Processed 22 / 51 entries with claude_v3_5\n",
      "Processed 23 / 51 entries with claude_v3_5\n",
      "Processed 24 / 51 entries with claude_v3_5\n",
      "Processed 25 / 51 entries with claude_v3_5\n",
      "Processed 26 / 51 entries with claude_v3_5\n",
      "Processed 27 / 51 entries with claude_v3_5\n",
      "Processed 28 / 51 entries with claude_v3_5\n",
      "Processed 29 / 51 entries with claude_v3_5\n",
      "Processed 30 / 51 entries with claude_v3_5\n",
      "Processed 31 / 51 entries with claude_v3_5\n",
      "Processed 32 / 51 entries with claude_v3_5\n",
      "Processed 33 / 51 entries with claude_v3_5\n",
      "Processed 34 / 51 entries with claude_v3_5\n",
      "Processed 35 / 51 entries with claude_v3_5\n",
      "Processed 36 / 51 entries with claude_v3_5\n",
      "Processed 37 / 51 entries with claude_v3_5\n",
      "Processed 38 / 51 entries with claude_v3_5\n",
      "Processed 39 / 51 entries with claude_v3_5\n",
      "Processed 40 / 51 entries with claude_v3_5\n",
      "Processed 41 / 51 entries with claude_v3_5\n",
      "Processed 42 / 51 entries with claude_v3_5\n",
      "Processed 43 / 51 entries with claude_v3_5\n",
      "Processed 44 / 51 entries with claude_v3_5\n",
      "Processed 45 / 51 entries with claude_v3_5\n",
      "Processed 46 / 51 entries with claude_v3_5\n",
      "Processed 47 / 51 entries with claude_v3_5\n",
      "Processed 48 / 51 entries with claude_v3_5\n",
      "Processed 49 / 51 entries with claude_v3_5\n",
      "Processed 50 / 51 entries with claude_v3_5\n",
      "51 entry is empty. Skipping\n"
     ]
    }
   ],
   "source": [
    "alignment_dict = {}\n",
    "\n",
    "alignment_dict[\"nova_product\"] = evaluate_alignment_model(s3_client=s3_client, \n",
    "                                                  model_name='nova_product')\n",
    "\n",
    "alignment_dict[\"nova_sales\"] = evaluate_alignment_model(s3_client=s3_client, \n",
    "                                                   model_name='nova_sales')\n",
    "alignment_dict[\"claude\"] = evaluate_alignment_model(s3_client=s3_client, \n",
    "                                                    model_name='claude_v3_5')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d9642627",
   "metadata": {},
   "source": [
    "### Step 4. Implement Comparative Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e2244600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from krippendorff import alpha\n",
    "from scipy import stats\n",
    "import pprint\n",
    "\n",
    "\n",
    "def create_df(alignment_dict):\n",
    "    model_names = list(alignment_dict.keys())\n",
    "    res = pd.DataFrame(columns=model_names)\n",
    "    \n",
    "    N = len(alignment_dict[model_names[0]])\n",
    "    \n",
    "    for k, v in alignment_dict.items():\n",
    "        assert len(v) == N\n",
    "    \n",
    "    for i in range(N):\n",
    "        this_dict = {}\n",
    "        for model_name, v in alignment_dict.items():\n",
    "            this_dict[model_name] = v[i]['alignment_score']\n",
    "        res = pd.concat((res, pd.DataFrame.from_dict([this_dict])), ignore_index=True)\n",
    "                        \n",
    "    return res\n",
    "        \n",
    "\n",
    "def calculate_percentage_agreement(df):\n",
    "    \"\"\"Calculate the percentage of perfect agreement between all raters\"\"\"\n",
    "    total_rows = len(df)\n",
    "    perfect_agreement_rows = sum(df.nunique(axis=1) == 1)\n",
    "    return (perfect_agreement_rows / total_rows) * 100\n",
    "\n",
    "def calculate_pairwise_cohens_kappa(df):\n",
    "    \"\"\"Calculate Cohen's Kappa for each pair of raters\"\"\"\n",
    "    models = df.columns\n",
    "    kappa_results = {}\n",
    "    \n",
    "    for i in range(len(models)):\n",
    "        for j in range(i+1, len(models)):\n",
    "            model1, model2 = models[i], models[j]\n",
    "            # Remove rows where either rater has missing values\n",
    "            valid_ratings = df[[model1, model2]].dropna()\n",
    "            \n",
    "            if len(valid_ratings) > 0:\n",
    "                kappa = cohen_kappa_score(\n",
    "                    valid_ratings[model1].astype(int),\n",
    "                    valid_ratings[model2].astype(int)\n",
    "                )\n",
    "                kappa_results[f\"{model1} vs {model2}\"] = kappa\n",
    "    \n",
    "    return kappa_results\n",
    "\n",
    "def calculate_krippendorffs_alpha(df):\n",
    "    \"\"\"Calculate Krippendorff's alpha for all raters\"\"\"\n",
    "    # Convert DataFrame to array format required by krippendorff alpha\n",
    "    data = df.values.T.astype(\"int\")\n",
    "    return alpha(reliability_data=data, value_domain=np.arange(1, 4), level_of_measurement=\"ordinal\")\n",
    "\n",
    "def calculate_spearmans_rho(df):\n",
    "    \"\"\"Calculate Spearman's rho for each pair of raters\"\"\"\n",
    "    models = df.columns\n",
    "    rho_results = {}\n",
    "    \n",
    "    for i in range(len(models)):\n",
    "        for j in range(i+1, len(models)):\n",
    "            model1, model2 = models[i], models[j]\n",
    "            # Remove rows where either rater has missing values\n",
    "            valid_ratings = df[[model1, model2]].dropna()\n",
    "            \n",
    "            if len(valid_ratings) > 0:\n",
    "                rho, p_value = stats.spearmanr(\n",
    "                    valid_ratings[model1],\n",
    "                    valid_ratings[model2]\n",
    "                )\n",
    "                rho_results[f\"{model1} vs {model2}\"] = {\n",
    "                    'rho': rho,\n",
    "                    'p_value': p_value\n",
    "                }\n",
    "    \n",
    "    return rho_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3deba3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30266/3136113337.py:71: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  rho, p_value = stats.spearmanr(\n",
      "/tmp/ipykernel_30266/3136113337.py:71: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  rho, p_value = stats.spearmanr(\n"
     ]
    }
   ],
   "source": [
    "# Extract ratings into a DataFrame\n",
    "ratings_df = create_df(alignment_dict)\n",
    "\n",
    "# Calculate statistics\n",
    "stats_results = {\n",
    "    'Percentage Agreement': calculate_percentage_agreement(ratings_df),\n",
    "    'Cohens Kappa': calculate_pairwise_cohens_kappa(ratings_df),\n",
    "    'Krippendorffs Alpha': calculate_krippendorffs_alpha(ratings_df),\n",
    "    'Spearmans Rho': calculate_spearmans_rho(ratings_df)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5f9ece6a-d196-4f36-8fc3-0b1012795007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nova_product</th>\n",
       "      <th>nova_sales</th>\n",
       "      <th>claude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   nova_product nova_sales claude\n",
       "0             3          3      3\n",
       "1             3          2      3\n",
       "2             2          2      3\n",
       "3             2          3      3\n",
       "4             3          3      3\n",
       "5             3          3      3\n",
       "6             3          3      3\n",
       "7             3          3      3\n",
       "8             3          3      3\n",
       "9             3          2      3\n",
       "10            3          3      3\n",
       "11            3          3      3\n",
       "12            3          3      3\n",
       "13            3          3      3\n",
       "14            3          3      3\n",
       "15            2          3      3\n",
       "16            3          3      3\n",
       "17            3          3      3\n",
       "18            2          2      3\n",
       "19            3          3      3\n",
       "20            2          2      3\n",
       "21            2          3      3\n",
       "22            3          3      3\n",
       "23            2          3      3\n",
       "24            2          2      3\n",
       "25            2          2      3\n",
       "26            3          2      3\n",
       "27            3          3      3\n",
       "28            2          2      3\n",
       "29            3          3      3\n",
       "30            3          3      3\n",
       "31            3          3      3\n",
       "32            3          3      3\n",
       "33            2          2      3\n",
       "34            3          2      3\n",
       "35            3          3      3\n",
       "36            3          3      3\n",
       "37            3          3      3\n",
       "38            3          3      3\n",
       "39            3          3      3\n",
       "40            3          3      3\n",
       "41            3          3      3\n",
       "42            3          3      3\n",
       "43            3          3      3\n",
       "44            3          3      3\n",
       "45            3          3      3\n",
       "46            3          3      3\n",
       "47            3          3      3\n",
       "48            3          3      3\n",
       "49            3          3      3"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0e42de5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Cohens Kappa': {'nova_product vs claude': 0.0,\n",
      "                  'nova_product vs nova_sales': 0.5337995337995338,\n",
      "                  'nova_sales vs claude': 0.0},\n",
      " 'Krippendorffs Alpha': 0.2063210227272727,\n",
      " 'Percentage Agreement': 70.0,\n",
      " 'Spearmans Rho': {'nova_product vs claude': {'p_value': nan, 'rho': nan},\n",
      "                   'nova_product vs nova_sales': {'p_value': 6.549773578646997e-05,\n",
      "                                                  'rho': 0.5337995337995338},\n",
      "                   'nova_sales vs claude': {'p_value': nan, 'rho': nan}}}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(stats_results)"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
