{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent Content Creation Pipeline with CrewAI and Amazon Bedrock\n",
    "\n",
    "This notebook implements a functional multi-agent content creation pipeline using CrewAI and Amazon Bedrock. The system performs actual web searches and uses real data to generate high-quality blog content.\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "```\n",
    "                ┌────────────────┐\n",
    "                │                │\n",
    "                │  Amazon        │\n",
    "                │  Bedrock       │\n",
    "                │                │\n",
    "                └────────────────┘\n",
    "                        ▲\n",
    "                        │\n",
    "                        ▼\n",
    "┌───────────────────────────────────────────────┐\n",
    "│                  CrewAI                       │\n",
    "│  ┌──────────┐    ┌──────────┐    ┌──────────┐ │\n",
    "│  │          │    │          │    │          │ │\n",
    "│  │Researcher│───►│  Writer  │───►│  Editor  │ │\n",
    "│  │  Agent   │    │  Agent   │    │  Agent   │ │\n",
    "│  │          │    │          │    │          │ │\n",
    "│  └──────────┘    └──────────┘    └──────────┘ │\n",
    "│        │               │               │      │\n",
    "│        ▼               ▼               ▼      │\n",
    "│  ┌──────────┐    ┌──────────┐    ┌──────────┐ │\n",
    "│  │ Internet │    │ Content  │    │ SEO      │ │\n",
    "│  │ Search   │    │ Creation │    │ Analysis │ │\n",
    "│  │ Tool     │    │ Tool     │    │ Tool     │ │\n",
    "│  └──────────┘    └──────────┘    └──────────┘ │\n",
    "└───────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Notes:**\n",
    "- Make sure you are running this code using your AWS Credentials. This notebook assumes you are loading the credentials using an IAM role, however, you may use your access_key if you are not using IAM Roles. For more details about how to set temporaty AWS credentials please check [this link](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp_use-resources.html).\n",
    "- Before using Amazon Bedrock models in this notebook you need to enable them in your account in us-east-1, for more details about the steps required to enabled the model please check [this link](https://docs.aws.amazon.com/bedrock/latest/userguide/model-access-modify.html).\n",
    "- This notebook uses Amazon Nova Lite 2.0 as default, all [charges on-demand on request basis](https://aws.amazon.com/bedrock/pricing/).\n",
    "- Amazon Nova Lite 2.0 will be used with the [Cross-Region Inference mode](https://docs.aws.amazon.com/bedrock/latest/userguide/cross-region-inference.html).\n",
    "- While this notebook uses us-east-1, Amazon Nova Lite 2.0 is available in a variety of AWS Regions. Check our [document pages](https://docs.aws.amazon.com/bedrock/latest/userguide/models-regions.html) for more details about the regions available.\n",
    "\n",
    "## Installation and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install -r requirements.txt -Uq "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# CrewAI imports\n",
    "from crewai import Agent, Task, Crew, Process\n",
    "from crewai.tools import BaseTool, tool\n",
    "\n",
    "# Search tools imports\n",
    "from langchain_community.tools import DuckDuckGoSearchRun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Amazon Bedrock\n",
    "\n",
    "Let's set up Amazon Bedrock as our LLM provider. You need to have your AWS credentials properly configured. Here we use a [custom implementation](https://docs.crewai.com/en/learn/custom-llm) because liteLLM does not yet support the newly released Nova Lite 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from crewai import BaseLLM\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "class NovaLLM(BaseLLM):\n",
    "    \"\"\"Custom LLM implementation for Amazon Nova models with CrewAI.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str = \"us.amazon.nova-2-lite-v1:0\",\n",
    "        temperature: Optional[float] = 0.7,\n",
    "        region_name: str = \"us-east-1\",\n",
    "        max_tokens: int = 16384,\n",
    "        reasoning_effort: Optional[str] = None,  # \"low\", \"medium\", \"high\"\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize Nova LLM for CrewAI.\n",
    "        \n",
    "        Args:\n",
    "            model: Nova model ID (e.g., \"us.amazon.nova-2-lite-v1:0\")\n",
    "            temperature: Sampling temperature (0-1)\n",
    "            region_name: AWS region\n",
    "            max_tokens: Maximum tokens to generate\n",
    "            reasoning_effort: Enable reasoning (\"low\", \"medium\", \"high\")\n",
    "        \"\"\"\n",
    "        super().__init__(model=model, temperature=temperature)\n",
    "        \n",
    "        self.bedrock = boto3.client('bedrock-runtime', region_name=region_name)\n",
    "        self.max_tokens = max_tokens\n",
    "        self.reasoning_effort = reasoning_effort\n",
    "        \n",
    "    def call(\n",
    "        self,\n",
    "        messages: Union[str, List[Dict[str, str]]],\n",
    "        tools: Optional[List[dict]] = None,\n",
    "        callbacks: Optional[List[Any]] = None,\n",
    "        available_functions: Optional[Dict[str, Any]] = None,\n",
    "        **kwargs\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Call the Nova LLM with the given messages.\n",
    "        \"\"\"\n",
    "        # Convert string to message format if needed\n",
    "        if isinstance(messages, str):\n",
    "            messages = [{\"role\": \"user\", \"content\": messages}]\n",
    "        \n",
    "        # Convert to Nova format and call API\n",
    "        try:\n",
    "            nova_request = self._convert_to_nova_format(messages, tools)\n",
    "            response = self.bedrock.converse(**nova_request)\n",
    "            return self._extract_response(response, messages, tools, available_functions)\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            return f\"Error calling Nova API: {str(e)}\\n{traceback.format_exc()}\"\n",
    "    \n",
    "\n",
    "    def _convert_to_nova_format(\n",
    "        self,\n",
    "        messages: List[Dict[str, str]],\n",
    "        tools: Optional[List[dict]] = None\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Convert OpenAI/CrewAI format to Nova format.\"\"\"\n",
    "        nova_request = {\n",
    "            \"modelId\": self.model,\n",
    "            \"inferenceConfig\": {\n",
    "                \"temperature\": self.temperature,\n",
    "                \"maxTokens\": self.max_tokens,\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Separate system and conversation messages\n",
    "        system_messages = []\n",
    "        conversation_messages = []\n",
    "        has_assistant_prefill = False\n",
    "    \n",
    "        for msg in messages:\n",
    "            role = msg.get(\"role\", \"\")\n",
    "            content = msg.get(\"content\", \"\")\n",
    "            \n",
    "            if role == \"system\":\n",
    "                system_messages.append({\"text\": str(content)})\n",
    "            elif role in [\"user\", \"assistant\"]:\n",
    "                # Check if there's assistant prefill\n",
    "                if role == \"assistant\":\n",
    "                    has_assistant_prefill = True\n",
    "                    \n",
    "                # Ensure content is in Nova format\n",
    "                if isinstance(content, str):\n",
    "                    conversation_messages.append({\n",
    "                        \"role\": role,\n",
    "                        \"content\": [{\"text\": content}]\n",
    "                    })\n",
    "                elif isinstance(content, list):\n",
    "                    conversation_messages.append({\n",
    "                        \"role\": role,\n",
    "                        \"content\": content\n",
    "                    })\n",
    "            elif role == \"tool\":\n",
    "                has_assistant_prefill = True  # Tool messages imply assistant prefill\n",
    "                # Convert tool result to Nova format\n",
    "                conversation_messages.append({\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [{\n",
    "                        \"toolResult\": {\n",
    "                            \"toolUseId\": msg.get(\"tool_call_id\", \"unknown\"),\n",
    "                            \"content\": [{\"text\": str(content)}]\n",
    "                        }\n",
    "                    }]\n",
    "                })\n",
    "    \n",
    "        # Add system messages\n",
    "        if system_messages:\n",
    "            nova_request[\"system\"] = system_messages\n",
    "        \n",
    "        # Add conversation messages\n",
    "        if conversation_messages:\n",
    "            nova_request[\"messages\"] = conversation_messages\n",
    "        else:\n",
    "            # Fallback for empty messages\n",
    "            nova_request[\"messages\"] = [\n",
    "                {\"role\": \"user\", \"content\": [{\"text\": \"Hello\"}]}\n",
    "            ]\n",
    "        \n",
    "        # Convert tools to Nova format\n",
    "        if tools:\n",
    "            nova_tools = self._convert_tools_to_nova_format(tools)\n",
    "            if nova_tools:\n",
    "                nova_request[\"toolConfig\"] = {\"tools\": nova_tools}\n",
    "        \n",
    "        # Add reasoning config ONLY if no assistant prefill exists since reasoning is incompatible with assistant prefill\n",
    "        if self.reasoning_effort and not has_assistant_prefill:\n",
    "            nova_request[\"additionalModelRequestFields\"] = {\n",
    "                \"reasoningConfig\": {\n",
    "                    \"type\": \"enabled\",\n",
    "                    \"maxReasoningEffort\": self.reasoning_effort\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        return nova_request    \n",
    "    \n",
    "    def _convert_tools_to_nova_format(self, tools: List[dict]) -> List[dict]:\n",
    "        \"\"\"Convert tool definitions to Nova toolSpec format.\"\"\"\n",
    "        nova_tools = []\n",
    "        \n",
    "        for tool in tools:\n",
    "            try:\n",
    "                # Handle OpenAI function format\n",
    "                if isinstance(tool, dict) and tool.get(\"type\") == \"function\":\n",
    "                    function = tool.get(\"function\", {})\n",
    "                    nova_tools.append({\n",
    "                        \"toolSpec\": {\n",
    "                            \"name\": function.get(\"name\", \"unknown\"),\n",
    "                            \"description\": function.get(\"description\", \"\"),\n",
    "                            \"inputSchema\": {\n",
    "                                \"json\": function.get(\"parameters\", {\n",
    "                                    \"type\": \"object\",\n",
    "                                    \"properties\": {},\n",
    "                                    \"required\": []\n",
    "                                })\n",
    "                            }\n",
    "                        }\n",
    "                    })\n",
    "                # Handle direct tool spec format\n",
    "                elif isinstance(tool, dict) and \"name\" in tool:\n",
    "                    nova_tools.append({\n",
    "                        \"toolSpec\": {\n",
    "                            \"name\": tool.get(\"name\", \"unknown\"),\n",
    "                            \"description\": tool.get(\"description\", \"\"),\n",
    "                            \"inputSchema\": {\n",
    "                                \"json\": tool.get(\"parameters\", {\n",
    "                                    \"type\": \"object\",\n",
    "                                    \"properties\": {},\n",
    "                                    \"required\": []\n",
    "                                })\n",
    "                            }\n",
    "                        }\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Failed to convert tool: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return nova_tools\n",
    "    \n",
    "    def _extract_response(\n",
    "        self,\n",
    "        response: Dict[str, Any],\n",
    "        messages: List[Dict[str, str]],\n",
    "        tools: Optional[List[dict]],\n",
    "        available_functions: Optional[Dict[str, Any]]\n",
    "    ) -> str:\n",
    "        \"\"\"Extract text response or handle tool calls.\"\"\"\n",
    "        output = response.get('output', {})\n",
    "        message = output.get('message', {})\n",
    "        content_blocks = message.get('content', [])\n",
    "        \n",
    "        # Collect all text parts and tool uses\n",
    "        text_parts = []\n",
    "        tool_use_blocks = []\n",
    "        \n",
    "        for block in content_blocks:\n",
    "            # Skip reasoning content blocks (Nova 2.0 feature)\n",
    "            # These contain the model's thinking process but not the final answer\n",
    "            if 'reasoningContent' in block:\n",
    "                continue\n",
    "            \n",
    "            # Collect tool use blocks\n",
    "            if 'toolUse' in block:\n",
    "                tool_use_blocks.append(block['toolUse'])\n",
    "                continue\n",
    "            \n",
    "            # Collect text content\n",
    "            if 'text' in block:\n",
    "                text_parts.append(block['text'])\n",
    "        \n",
    "        # If we have tool uses and available_functions, execute them\n",
    "        if tool_use_blocks and available_functions:\n",
    "            for tool_use in tool_use_blocks:\n",
    "                tool_result = self._execute_tool(tool_use, available_functions)\n",
    "                \n",
    "                # Add tool use and result to message history\n",
    "                messages.append({\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": [{\"toolUse\": tool_use}]\n",
    "                })\n",
    "                messages.append({\n",
    "                    \"role\": \"tool\",\n",
    "                    \"tool_call_id\": tool_use.get('toolUseId', ''),\n",
    "                    \"content\": tool_result\n",
    "                })\n",
    "            \n",
    "            # Recursively call to get final answer\n",
    "            return self.call(messages, tools, None, available_functions)\n",
    "        \n",
    "        # If we have tool uses but no available_functions, \n",
    "        # CrewAI will handle tool execution - return text if available\n",
    "        if tool_use_blocks and not available_functions:\n",
    "            # If there's also text, return it (model might have provided both)\n",
    "            if text_parts:\n",
    "                return '\\n'.join(text_parts)\n",
    "            \n",
    "            # Otherwise, format tool call for CrewAI to parse\n",
    "            # CrewAI expects a specific format for tool calls\n",
    "            tool_use = tool_use_blocks[0]\n",
    "            tool_name = tool_use.get('name', 'unknown')\n",
    "            tool_input = tool_use.get('input', {})\n",
    "            \n",
    "            # Format as CrewAI expects\n",
    "            return f\"Action: {tool_name}\\nAction Input: {json.dumps(tool_input)}\"\n",
    "        \n",
    "        # Return combined text response\n",
    "        if text_parts:\n",
    "            return '\\n'.join(text_parts)\n",
    "        \n",
    "        return \"No response generated\"\n",
    "    \n",
    "    def _execute_tool(\n",
    "        self,\n",
    "        tool_use: Dict[str, Any],\n",
    "        available_functions: Dict[str, Any]\n",
    "    ) -> str:\n",
    "        \"\"\"Execute a tool and return the result.\"\"\"\n",
    "        tool_name = tool_use.get('name', '')\n",
    "        tool_input = tool_use.get('input', {})\n",
    "        \n",
    "        # Parse JSON string input\n",
    "        if isinstance(tool_input, str):\n",
    "            try:\n",
    "                tool_input = json.loads(tool_input)\n",
    "            except json.JSONDecodeError:\n",
    "                return f\"Error: Invalid tool input format\"\n",
    "        \n",
    "        # Execute function\n",
    "        if tool_name in available_functions:\n",
    "            try:\n",
    "                result = available_functions[tool_name](**tool_input)\n",
    "                return str(result)\n",
    "            except Exception as e:\n",
    "                return f\"Error executing {tool_name}: {str(e)}\"\n",
    "        \n",
    "        return f\"Error: Tool '{tool_name}' not found\"\n",
    "    \n",
    "    def supports_function_calling(self) -> bool:\n",
    "        \"\"\"Nova models support function calling.\"\"\"\n",
    "        return True\n",
    "    \n",
    "    def get_context_window_size(self) -> int:\n",
    "        \"\"\"Return context window size based on model.\"\"\"\n",
    "        if \"lite\" in self.model.lower():\n",
    "            return 200000\n",
    "        elif \"pro\" in self.model.lower():\n",
    "            return 300000\n",
    "        return 128000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import LLM\n",
    "\n",
    "# Setting model_id and AWS region\n",
    "region = \"us-east-1\"\n",
    "model_id = \"us.amazon.nova-2-lite-v1:0\"\n",
    "\n",
    "# Create Nova LLM with custom class imnplementation\n",
    "llm = NovaLLM(\n",
    "    model=model_id,\n",
    "    temperature=0.7,\n",
    "    region_name = region,\n",
    "    reasoning_effort=\"medium\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Tools for Agents\n",
    "\n",
    "Now, let's define the tools our agents will use. We'll implement actual search functionality with DuckDuckGo for comprehensive research and an SEO analysis tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DuckDuckGo search tool\n",
    "@tool(\"Internet Search\")\n",
    "def search_internet(query: str) -> str:\n",
    "    \"\"\"Search the internet for information on a specific query or topic.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query string.\n",
    "    \n",
    "    Returns:\n",
    "        str: Search results as text.\n",
    "    \"\"\"\n",
    "    search_tool = DuckDuckGoSearchRun()\n",
    "    results = search_tool.run(query)\n",
    "    return f\"Search results for '{query}':\\n\\n{results}\"\n",
    "\n",
    "# SEO Analysis Tool\n",
    "class SEOAnalysisInput(BaseModel):\n",
    "    \"\"\"Input schema for SEO analysis tool.\"\"\"\n",
    "    content: str = Field(..., description=\"The content to analyze for SEO optimization\")\n",
    "    target_keywords: List[str] = Field(..., description=\"List of target keywords to check for in the content\")\n",
    "\n",
    "class SEOAnalysisTool(BaseTool):\n",
    "    \"\"\"Tool for analyzing content for SEO optimization.\"\"\"\n",
    "    name: str = \"SEO Content Analyzer\"\n",
    "    description: str = \"Analyzes content for SEO optimization and provides recommendations\"\n",
    "    args_schema: type[BaseModel] = SEOAnalysisInput\n",
    "    \n",
    "    def _run(self, content: str, target_keywords: List[str]) -> str:\n",
    "        # Calculate basic metrics\n",
    "        word_count = len(content.split())\n",
    "        \n",
    "        # Calculate keyword density\n",
    "        keyword_counts = {}\n",
    "        content_lower = content.lower()\n",
    "        \n",
    "        for keyword in target_keywords:\n",
    "            count = content_lower.count(keyword.lower())\n",
    "            density = round(count / word_count * 100, 2) if word_count > 0 else 0\n",
    "            keyword_counts[keyword] = {\n",
    "                \"count\": count,\n",
    "                \"density\": density\n",
    "            }\n",
    "        \n",
    "        # Calculate readability metrics\n",
    "        sentences = max(1, content.count('.') + content.count('!') + content.count('?'))\n",
    "        avg_words_per_sentence = word_count / sentences\n",
    "        \n",
    "        # Generate recommendations\n",
    "        recommendations = []\n",
    "        \n",
    "        if word_count < 800:\n",
    "            recommendations.append(\"Increase content length to at least 800 words for better SEO performance\")\n",
    "        \n",
    "        for keyword, data in keyword_counts.items():\n",
    "            if data[\"count\"] == 0:\n",
    "                recommendations.append(f\"Add the keyword '{keyword}' to your content\")\n",
    "            elif data[\"density\"] < 0.5:\n",
    "                recommendations.append(f\"Increase the usage of '{keyword}' (current density: {data['density']}%)\")\n",
    "            elif data[\"density\"] > 2.5:\n",
    "                recommendations.append(f\"Reduce the usage of '{keyword}' to avoid keyword stuffing (current density: {data['density']}%)\")\n",
    "        \n",
    "        # Format response\n",
    "        response = f\"# SEO Content Analysis\\n\\n\"\n",
    "        response += f\"## Overview\\n\"\n",
    "        response += f\"- Word Count: {word_count} words ({word_count < 800 and 'too short' or word_count > 2000 and 'lengthy' or 'good length'})\\n\"\n",
    "        response += f\"- Average Sentence Length: {round(avg_words_per_sentence, 1)} words\\n\\n\"\n",
    "        \n",
    "        response += f\"## Keyword Analysis\\n\"\n",
    "        for keyword, data in keyword_counts.items():\n",
    "            response += f\"- '{keyword}': {data['count']} occurrences ({data['density']}% density)\\n\"\n",
    "        \n",
    "        response += f\"\\n## Recommendations\\n\"\n",
    "        if not recommendations:\n",
    "            response += \"- Content is well-optimized for the target keywords.\\n\"\n",
    "        else:\n",
    "            for rec in recommendations:\n",
    "                response += f\"- {rec}\\n\"\n",
    "        \n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Specialized Agents\n",
    "\n",
    "Now let's create our three specialized agents: Researcher, Writer, and Editor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Researcher Agent\n",
    "researcher = Agent(\n",
    "    role=\"Research Specialist\",\n",
    "    goal=\"Conduct thorough research on topics and provide comprehensive, accurate information\",\n",
    "    backstory=\"\"\"You are an expert researcher with a talent for finding the most relevant and up-to-date \n",
    "    information on any topic. You're known for your ability to gather comprehensive data, verify sources, \n",
    "    and organize information logically. Your research always includes key statistics, expert opinions, \n",
    "    and real-world examples to provide a complete picture of the topic.\"\"\",\n",
    "    verbose=True,\n",
    "    allow_delegation=False,\n",
    "    tools=[search_internet],\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "# Create the Writer Agent\n",
    "writer = Agent(\n",
    "    role=\"Content Writer\",\n",
    "    goal=\"Create engaging, informative content based on research findings\",\n",
    "    backstory=\"\"\"You are a talented content writer with years of experience creating engaging blog posts. \n",
    "    You excel at turning research into compelling narratives that educate and entertain. \n",
    "    Your writing is always clear, structured, and tailored to the target audience. \n",
    "    You know how to naturally incorporate keywords while maintaining a conversational tone.\"\"\",\n",
    "    verbose=True,\n",
    "    allow_delegation=False,  # Disable delegation to avoid errors\n",
    "    tools=[],  # The writer uses the LLM's capabilities for content creation\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "# Create the Editor Agent\n",
    "editor = Agent(\n",
    "    role=\"Content Editor & SEO Specialist\",\n",
    "    goal=\"Refine and optimize content for readability and SEO performance\",\n",
    "    backstory=\"\"\"You are a meticulous editor and SEO specialist. You have a keen eye for detail \n",
    "    and can transform good content into excellent content. You understand SEO best practices \n",
    "    and know how to optimize content for search engines without sacrificing readability or user experience. \n",
    "    You ensure content is error-free, well-structured, and strategically optimized for its target keywords.\"\"\",\n",
    "    verbose=True,\n",
    "    allow_delegation=False,\n",
    "    tools=[SEOAnalysisTool()],\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Tasks for Each Agent\n",
    "\n",
    "Now let's define the specific tasks for our content creation pipeline. We'll choose a topic and target keywords for our blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define blog post topic and target keywords\n",
    "blog_topic = \"Sustainable Gardening Practices for Urban Homes\"\n",
    "target_keywords = [\"sustainable gardening\", \"urban gardening\", \"eco-friendly gardening\", \"composting\", \"water conservation\"]\n",
    "\n",
    "# Task 1: Research the topic thoroughly\n",
    "research_task = Task(\n",
    "    description=f\"\"\"\n",
    "    Research the topic of \"{blog_topic}\" comprehensively. Your research should include:\n",
    "    \n",
    "    1. Latest trends and innovations in {blog_topic}\n",
    "    2. Best practices and techniques that are accessible for urban dwellers\n",
    "    3. Expert opinions and data-backed information\n",
    "    4. Common challenges and practical solutions\n",
    "    5. Real-world examples and success stories\n",
    "    6. Resources or tools that would be helpful to readers\n",
    "    \n",
    "    Use the Internet Search tool to gather diverse and current information.\n",
    "    Focus on content that includes the target keywords: {', '.join(target_keywords)}\n",
    "    \n",
    "    Organize your research findings in a structured format that will be helpful for the content writer.\n",
    "    Include specific data points, statistics, and quotable insights when available.\n",
    "    \n",
    "    Return a comprehensive, organized research document with clear sections.\n",
    "    \"\"\",\n",
    "    agent=researcher,\n",
    "    expected_output=\"A comprehensive research brief with organized, factual information about sustainable gardening practices for urban homes, including trends, best practices, expert insights, and real-world examples.\"\n",
    ")\n",
    "\n",
    "# Task 2: Write engaging blog content based on research\n",
    "writing_task = Task(\n",
    "    description=f\"\"\"\n",
    "    Using the research provided, create an engaging and informative blog post about \"{blog_topic}\".\n",
    "    \n",
    "    The research findings are as follows:\n",
    "    {{research_task.output}}\n",
    "    \n",
    "    Your blog post should:\n",
    "    1. Start with an engaging introduction that hooks the reader and establishes the importance of the topic\n",
    "    2. Include a logical structure with clear headings and subheadings\n",
    "    3. Provide practical, actionable advice that urban dwellers can implement\n",
    "    4. Incorporate real examples, success stories, or case studies\n",
    "    5. Naturally integrate the target keywords: {', '.join(target_keywords)}\n",
    "    6. End with a compelling conclusion and call-to-action\n",
    "    \n",
    "    The tone should be conversational yet informative, accessible to beginners while still valuable to those with some gardening experience.\n",
    "    \n",
    "    Aim for approximately 1,200-1,500 words in total.\n",
    "    \n",
    "    Return a complete, well-structured blog post in markdown format.\n",
    "    \"\"\",\n",
    "    agent=writer,\n",
    "    context=[research_task],\n",
    "    expected_output=\"A well-structured, engaging blog post about sustainable gardening practices for urban homes that effectively utilizes the research findings and naturally incorporates the target keywords.\"\n",
    ")\n",
    "\n",
    "# Task 3: Edit and optimize the content for SEO\n",
    "editing_task = Task(\n",
    "    description=f\"\"\"\n",
    "    Review, edit, and optimize the following blog post about \"{blog_topic}\".\n",
    "    \n",
    "    Content to edit:\n",
    "    {{writing_task.output}}\n",
    "    \n",
    "    Target keywords for SEO: {target_keywords}\n",
    "    \n",
    "    Your editing tasks:\n",
    "    \n",
    "    1. Improve the overall structure and flow\n",
    "    2. Enhance readability with appropriate formatting (headings, bullet points, short paragraphs)\n",
    "    3. Optimize for SEO using the target keywords: {', '.join(target_keywords)}\n",
    "    4. Check factual accuracy and ensure proper attribution of sources when needed\n",
    "    5. Refine the writing style for clarity and engagement\n",
    "    6. Create a compelling meta title and meta description\n",
    "    \n",
    "    First, use the SEO Content Analyzer tool to evaluate the content. \n",
    "    When calling the tool, you MUST provide:\n",
    "    - content: the blog post text to analyze\n",
    "    - target_keywords: {target_keywords}\n",
    "    \n",
    "    Then, apply your improvements based on the SEO analysis to create the final version.\n",
    "    \n",
    "    IMPORTANT: Return ONLY the final, optimized blog post in markdown format with the meta title \n",
    "    and description at the top. Do NOT include any explanatory text, comments, or descriptions \n",
    "    about what you did. Just return the pure markdown content.\n",
    "    \"\"\",\n",
    "    agent=editor,\n",
    "    context=[writing_task],\n",
    "    expected_output=\"Pure markdown blog post with meta title and description at the top, with no additional explanatory text or comments.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Run the Content Creation Crew\n",
    "\n",
    "Now we'll create and run our content creation crew with the defined agents and tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the content creation crew\n",
    "content_crew = Crew(\n",
    "    agents=[researcher, writer, editor],\n",
    "    tasks=[research_task, writing_task, editing_task],\n",
    "    verbose=True,  # Set to True for detailed logs\n",
    "    process=Process.sequential  # Execute tasks in sequence\n",
    ")\n",
    "\n",
    "# Run the crew\n",
    "result = content_crew.kickoff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the Final Blog Post and Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the final edited blog post\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL BLOG POST CONTENT\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Individual Agent Outputs (Optional)\n",
    "\n",
    "Let's also look at the output from each stage of the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display research findings\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESEARCH FINDINGS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "print(research_task.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display initial blog draft\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INITIAL BLOG DRAFT\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "print(writing_task.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This CrewAI content pipeline demonstrates how multiple specialized AI agents can collaborate to create high-quality content. The process follows a logical workflow:\n",
    "\n",
    "1. The Researcher agent gathers comprehensive information from the web\n",
    "2. The Writer agent transforms that research into engaging content\n",
    "3. The Editor agent optimizes the content for readability and SEO\n",
    "\n",
    "The result is a polished, SEO-optimized blog post ready for publication, created entirely through the orchestrated collaboration of AI agents."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "repeatable-patterns-H6m-84_c",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
