{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NovaStreamParser Tutorial\n",
    "\n",
    "This notebook provides a step-by-step guide to using the NovaStreamParser package for parsing and processing streaming responses from AWS Bedrock Nova models.\n",
    "\n",
    "## Overview\n",
    "\n",
    "NovaStreamParser provides decorators that enable extraction of content between specified XML tags from both `invoke_model_with_response_stream` and `converse_stream` API responses. This is particularly useful for extracting reasoning content from `<thinking>` tags or other structured XML content in model responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries\n",
    "\n",
    "First, let's import all the necessary libraries and modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "from NovaStreamParser.nova_parsed_event_stream import (\n",
    "    parse_invoke_model_with_response_stream,\n",
    "    parse_converse_stream\n",
    ")\n",
    "\n",
    "print(\"✅ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define Decorated Stream Processing Functions\n",
    "\n",
    "The NovaStreamParser uses decorators to wrap your stream processing functions. Let's create two decorated functions - one for each API type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decorator for converse_stream API\n",
    "@parse_converse_stream(\"thinking\")\n",
    "def process_converse_stream(response_stream):\n",
    "    \"\"\"Process converse_stream responses and extract content from <thinking> tags\"\"\"\n",
    "    return response_stream\n",
    "\n",
    "# Decorator for invoke_model_with_response_stream API\n",
    "@parse_invoke_model_with_response_stream(target_tag_name=\"thinking\")\n",
    "def process_invoke_model_with_response_stream(response_stream):\n",
    "    \"\"\"Process invoke_model_with_response_stream responses and extract content from <thinking> tags\"\"\"\n",
    "    return response_stream\n",
    "\n",
    "print(\"✅ Decorated functions defined\")\n",
    "print(\"   - process_converse_stream: Extracts content from <thinking> tags in converse_stream\")\n",
    "print(\"   - process_invoke_model_with_response_stream: Extracts content from <thinking> tags in invoke_model_with_response_stream\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Configure the System Prompt and Instructions\n",
    "\n",
    "Let's set up the system prompt that instructs the model to use `<thinking>` tags for reasoning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base system instructions\n",
    "system_text = \"\"\"\n",
    "You are a friend and helpful assistant that answers questions about the weather. \n",
    "The user and you will engage in a dialog exchanging the transcripts of a natural real-time conversation. Keep your responses short, generally two or three sentences for chatty scenarios.\n",
    "You can use the tool to get the weather for a city.\n",
    "You can use the tool multiple times to get the weather for multiple cities.\n",
    "You can use the tool to get the weather for multiple cities at once.\n",
    "\"\"\"\n",
    "\n",
    "# Important: Instruct the model to use <thinking> tags\n",
    "postamble = \"\\nWhen reasoning on your replies, place the reasoning in <thinking></thinking> tags.\"\n",
    "\n",
    "# Combine into system prompt format\n",
    "system_prompt = [{\"text\": system_text + postamble}]\n",
    "\n",
    "print(\"✅ System prompt configured\")\n",
    "print(f\"   - Base instructions: {len(system_text)} characters\")\n",
    "print(f\"   - Includes instruction to use <thinking> tags for reasoning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Set Up Inference Configuration\n",
    "\n",
    "Configure the model's inference parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference configuration\n",
    "inference_config = {\n",
    "    \"maxTokens\": 1024,    # Maximum tokens to generate\n",
    "    \"topP\": 0.9,          # Nucleus sampling parameter\n",
    "    \"temperature\": 0.7    # Controls randomness (0.0 = deterministic, 1.0 = very random)\n",
    "}\n",
    "\n",
    "print(\"✅ Inference configuration set:\")\n",
    "for key, value in inference_config.items():\n",
    "    print(f\"   - {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Define Tool Configuration\n",
    "\n",
    "Set up the weather tool that the model can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool configuration for weather queries\n",
    "tool_config = {\n",
    "    \"tools\": [\n",
    "        {\n",
    "            \"toolSpec\": {\n",
    "                \"name\": \"getWeather\",\n",
    "                \"description\": \"A tool to get the weather\",\n",
    "                \"inputSchema\": {\n",
    "                    \"json\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"city\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"A tool to get the weather for a particular city.\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"city\"]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"toolChoice\": {\n",
    "        \"auto\": {}  # Let the model decide when to use tools\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"✅ Tool configuration defined:\")\n",
    "print(f\"   - Tool name: {tool_config['tools'][0]['toolSpec']['name']}\")\n",
    "print(f\"   - Tool choice: Auto (model decides when to use)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Create Conversation Messages\n",
    "\n",
    "Set up a realistic conversation flow that demonstrates tool usage and thinking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversation messages demonstrating a natural flow\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"text\": \"Hi what's the weather?\"\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"text\": \"Hi there! Could you please tell me which city you're interested in?\"\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"text\": \"Yes, I'm in Seattle, Washington\"\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"text\": \"<thinking>I need to get the weather for Seattle, Washington.</thinking>\"\n",
    "            },\n",
    "            {\n",
    "                \"toolUse\": {\n",
    "                    'name': 'getWeather',\n",
    "                    'toolUseId': '4356828f-a39c-4e4e-b9d5-dcf6027a4c7a',\n",
    "                    'input': {\n",
    "                        'city': 'Seattle, Washington'\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"text\": \"Actually, I'm in Tacoma, Washington.\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"✅ Conversation messages created:\")\n",
    "print(f\"   - Total messages: {len(messages)}\")\n",
    "print(f\"   - Includes example of <thinking> tags in assistant response\")\n",
    "print(f\"   - Demonstrates tool usage scenario\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Prepare Request Body and Model Configuration\n",
    "\n",
    "Combine all configurations into the request body:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all configurations\n",
    "body = {\n",
    "    \"system\": system_prompt,\n",
    "    \"inferenceConfig\": inference_config,\n",
    "    \"toolConfig\": tool_config,\n",
    "    \"messages\": messages\n",
    "}\n",
    "\n",
    "# Model configuration\n",
    "LITE_MODEL_ID = \"us.amazon.nova-lite-v1:0\"\n",
    "modelId = LITE_MODEL_ID\n",
    "\n",
    "print(\"✅ Request body prepared:\")\n",
    "print(f\"   - Model ID: {modelId}\")\n",
    "print(f\"   - Body contains: {list(body.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Initialize AWS Bedrock Client\n",
    "\n",
    "Create the Bedrock Runtime client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Bedrock Runtime client in the AWS Region of your choice\n",
    "client = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "\n",
    "print(\"✅ AWS Bedrock Runtime client created\")\n",
    "print(f\"   - Region: us-east-1\")\n",
    "print(f\"   - Service: bedrock-runtime\")\n",
    "\n",
    "# Verify client configuration\n",
    "try:\n",
    "    # This will help verify AWS credentials are configured\n",
    "    print(f\"   - Client region: {client.meta.region_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"   - Warning: Could not verify client configuration: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a little helper function to just print out the events of the stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_helper(event, mode = \"\"):\n",
    "    block = None\n",
    "    \n",
    "    if (mode == \"CONVERSE\"):\n",
    "        block = event\n",
    "    else:\n",
    "        chunk = event.get(\"chunk\")\n",
    "        block = json.loads(chunk.get(\"bytes\").decode())\n",
    "\n",
    "    if (block is not None\n",
    "        and block.get(\"contentBlockDelta\") is not None\n",
    "        and block[\"contentBlockDelta\"].get(\"delta\") is not None\n",
    "        and block[\"contentBlockDelta\"][\"delta\"].get(\"text\") is not None):\n",
    "        print(f\"{block[\"contentBlockDelta\"][\"delta\"][\"text\"]}\", end=\"\")\n",
    "    elif (block is not None\n",
    "        and block.get(\"contentBlockDelta\") is not None\n",
    "        and block[\"contentBlockDelta\"].get(\"delta\") is not None\n",
    "        and block[\"contentBlockDelta\"][\"delta\"].get(\"toolUse\") is not None):\n",
    "        print(f\"{block[\"contentBlockDelta\"][\"delta\"][\"toolUse\"]}\", end=\"\")\n",
    "    elif (block is not None\n",
    "        and block.get(\"contentBlockStart\") is not None\n",
    "        and block[\"contentBlockStart\"].get(\"start\") is not None):\n",
    "            print(f\"{ block[\"contentBlockStart\"].get(\"start\") }\", end=\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Demonstrate invoke_model_with_response_stream\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use invoke_model_with_streaming API, but without parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🚀 Starting invoke_model_with_response_stream, not parsed demonstration...\")\n",
    "\n",
    "try:\n",
    "    # Make the API call\n",
    "    response = client.invoke_model_with_response_stream(\n",
    "        modelId=modelId,\n",
    "        body=json.dumps(body)\n",
    "    )\n",
    "    \n",
    "    # Get the response stream\n",
    "    response_stream = response.get('body')\n",
    "    \n",
    "    # Process the stream using our decorated function\n",
    "    print(\"\\n\\n----------------  un-parsed stream ---------------\\n\")\n",
    "    for event in response_stream:\n",
    "        print_helper(event)\n",
    "    \n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during invoke_model_with_response_stream: {e}\")\n",
    "    print(\"   Make sure your AWS credentials are configured and you have access to Bedrock Nova models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use the first API method with our decorated function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🚀 Starting invoke_model_with_response_stream parsed demonstration...\")\n",
    "\n",
    "try:\n",
    "    # Make the API call\n",
    "    response = client.invoke_model_with_response_stream(\n",
    "        modelId=modelId,\n",
    "        body=json.dumps(body)\n",
    "    )\n",
    "    \n",
    "    # Get the response stream\n",
    "    response_stream = response.get('body')\n",
    "    \n",
    "    # print(\"📡 Response stream received, processing events...\")\n",
    "    # print(\"-\" * 40)\n",
    "    \n",
    "    # Process the stream using our decorated function\n",
    "    # This will extract content from <thinking> tags\n",
    "    print(\"\\n\\n----------------  parsed stream ---------------\\n\") \n",
    "    for event in process_invoke_model_with_response_stream(response_stream):\n",
    "        print_helper(event)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during invoke_model_with_response_stream: {e}\")\n",
    "    print(\"   Make sure your AWS credentials are configured and you have access to Bedrock Nova models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Demonstrate converse_stream\n",
    "\n",
    "Now let's use the second API method with our other decorated function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the converse streaming API, but without parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🚀 Starting converse_stream un-parsed demonstration...\")\n",
    "\n",
    "try:\n",
    "    # Make the API call using converse_stream\n",
    "    response = client.converse_stream(\n",
    "        modelId=LITE_MODEL_ID,\n",
    "        messages=body[\"messages\"],\n",
    "        system=body[\"system\"],\n",
    "        inferenceConfig=body[\"inferenceConfig\"],\n",
    "        toolConfig=body[\"toolConfig\"]\n",
    "    )\n",
    "    \n",
    "    # Get the response stream\n",
    "    response_stream = response.get('stream')\n",
    "    \n",
    "    \n",
    "    # Process the stream using our decorated function\n",
    "    # This will extract content from <thinking> tags\n",
    "    print(\"\\n\\n----------------  un-parsed stream ---------------\\n\") \n",
    "    for event in response_stream:\n",
    "        print_helper(event, \"CONVERSE\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during converse_stream: {e}\")\n",
    "    print(\"   Make sure your AWS credentials are configured and you have access to Bedrock Nova models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use the second API method with our other decorated function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🚀 Starting converse_stream parsed demonstration...\")\n",
    "\n",
    "try:\n",
    "    # Make the API call using converse_stream\n",
    "    response = client.converse_stream(\n",
    "        modelId=LITE_MODEL_ID,\n",
    "        messages=body[\"messages\"],\n",
    "        system=body[\"system\"],\n",
    "        inferenceConfig=body[\"inferenceConfig\"],\n",
    "        toolConfig=body[\"toolConfig\"]\n",
    "    )\n",
    "    \n",
    "    # Get the response stream\n",
    "    response_stream = response.get('stream')\n",
    "    \n",
    "    \n",
    "    # Process the stream using our decorated function\n",
    "    # This will extract content from <thinking> tags\n",
    "    print(\"\\n\\n----------------  parsed stream ---------------\\n\") \n",
    "    for event in process_converse_stream(response_stream):\n",
    "        print_helper(event, \"CONVERSE\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during converse_stream: {e}\")\n",
    "    print(\"   Make sure your AWS credentials are configured and you have access to Bedrock Nova models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Understanding the Output\n",
    "\n",
    "Let's break down what the NovaStreamParser decorators do:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How NovaStreamParser Works\n",
    "\n",
    "1. **Decoration**: The decorators wrap your stream processing functions\n",
    "2. **Stream Interception**: The original response stream is intercepted and processed\n",
    "3. **Event Processing**: Different event types are handled appropriately:\n",
    "   - `messageStart`: Initialization\n",
    "   - `contentBlockDelta`: Content processing and tag extraction\n",
    "   - `messageStop`: Finalization\n",
    "   - `metadata`: Stream completion\n",
    "4. **Tag Extraction**: Content within the specified XML tags (`<thinking>` in our case) is extracted\n",
    "5. **Stream Generation**: A new processed stream is generated with the extracted content\n",
    "\n",
    "### Key Benefits\n",
    "\n",
    "- **Real-time Processing**: Processes streaming responses as they arrive\n",
    "- **XML Tag Extraction**: Cleanly separates reasoning content from regular responses\n",
    "- **Dual API Support**: Works with both streaming APIs\n",
    "- **Event-based**: Handles different types of streaming events appropriately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Customization Examples\n",
    "\n",
    "Here are some ways you can customize the NovaStreamParser for different use cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Extract different XML tags\n",
    "@parse_converse_stream(\"analysis\")\n",
    "def process_analysis_tags(response_stream):\n",
    "    \"\"\"Extract content from <analysis> tags\"\"\"\n",
    "    return response_stream\n",
    "\n",
    "# Example 2: Extract code blocks\n",
    "@parse_invoke_model_with_response_stream(target_tag_name=\"code\")\n",
    "def process_code_blocks(response_stream):\n",
    "    \"\"\"Extract content from <code> tags\"\"\"\n",
    "    return response_stream\n",
    "\n",
    "# Example 3: Extract reasoning steps\n",
    "@parse_converse_stream(\"reasoning\")\n",
    "def process_reasoning_steps(response_stream):\n",
    "    \"\"\"Extract content from <reasoning> tags\"\"\"\n",
    "    return response_stream\n",
    "\n",
    "print(\"✅ Custom extraction functions defined:\")\n",
    "print(\"   - process_analysis_tags: Extracts <analysis> content\")\n",
    "print(\"   - process_code_blocks: Extracts <code> content\")\n",
    "print(\"   - process_reasoning_steps: Extracts <reasoning> content\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Error Handling and Best Practices\n",
    "\n",
    "Here are some best practices when using NovaStreamParser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Practice 1: Always handle exceptions\n",
    "def safe_stream_processing(client, model_id, body):\n",
    "    \"\"\"Safely process streams with proper error handling\"\"\"\n",
    "    try:\n",
    "        response = client.converse_stream(\n",
    "            modelId=model_id,\n",
    "            messages=body[\"messages\"],\n",
    "            system=body[\"system\"],\n",
    "            inferenceConfig=body[\"inferenceConfig\"],\n",
    "            toolConfig=body.get(\"toolConfig\")  # Use .get() for optional parameters\n",
    "        )\n",
    "        \n",
    "        response_stream = response.get('stream')\n",
    "        \n",
    "        for event in process_converse_stream(response_stream):\n",
    "            yield event\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing stream: {e}\")\n",
    "        return None\n",
    "\n",
    "# Best Practice 2: Validate inputs\n",
    "def validate_configuration(body, model_id):\n",
    "    \"\"\"Validate configuration before making API calls\"\"\"\n",
    "    required_keys = [\"messages\", \"system\", \"inferenceConfig\"]\n",
    "    \n",
    "    for key in required_keys:\n",
    "        if key not in body:\n",
    "            raise ValueError(f\"Missing required key: {key}\")\n",
    "    \n",
    "    if not model_id:\n",
    "        raise ValueError(\"Model ID is required\")\n",
    "    \n",
    "    if not body[\"messages\"]:\n",
    "        raise ValueError(\"At least one message is required\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "print(\"✅ Best practices functions defined:\")\n",
    "print(\"   - safe_stream_processing: Handles exceptions gracefully\")\n",
    "print(\"   - validate_configuration: Validates inputs before API calls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've successfully learned how to use NovaStreamParser! Here's what we covered:\n",
    "\n",
    "### Key Concepts:\n",
    "1. **Decorator Pattern**: Using `@parse_converse_stream` and `@parse_invoke_model_with_response_stream`\n",
    "2. **XML Tag Extraction**: Extracting content from specified tags (like `<thinking>`)\n",
    "3. **Dual API Support**: Working with both Bedrock streaming APIs\n",
    "4. **Real-time Processing**: Handling streaming responses as they arrive\n",
    "\n",
    "### Implementation Steps:\n",
    "1. Import the NovaStreamParser decorators\n",
    "2. Define decorated functions for stream processing\n",
    "3. Configure system prompts to use XML tags\n",
    "4. Set up inference and tool configurations\n",
    "5. Create conversation messages\n",
    "6. Initialize AWS Bedrock client\n",
    "7. Process streams using decorated functions\n",
    "\n",
    "### Next Steps:\n",
    "- Experiment with different XML tag names\n",
    "- Try different model configurations\n",
    "- Implement custom processing logic within your decorated functions\n",
    "- Add error handling and logging for production use\n",
    "\n",
    "The NovaStreamParser makes it easy to extract structured content from Nova model responses while maintaining the benefits of streaming APIs!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Nova Stream Parser (Python 3.12)",
   "language": "python",
   "name": "nova-stream-parser"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
