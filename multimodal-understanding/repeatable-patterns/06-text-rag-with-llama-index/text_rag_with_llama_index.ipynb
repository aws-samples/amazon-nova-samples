{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a (Very Simple) Vector Store from Scratch with Amazon Bedrock and Amazon Nova Lite 2.0\n",
    "\n",
    "In this tutorial, we show you how to build a simple in-memory vector store that can store documents along with metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisite\n",
    "Installing required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"llama-index>=0.12.3\" llama-index-llms-bedrock-converse llama-index-embeddings-bedrock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl --user-agent \"Mozilla\" -L \"https://arxiv.org/pdf/2307.09288.pdf\" -o \"data/llama2.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Simple in memory RAG\n",
    "Here we use BedockEmbedding class to create document embeddings using default bedrock embedding model and we use the Amazon Nova Lite 2.0 model for text generation. We use a custom class because the Nova lite 2.0 is not yet supported in llama index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Optional, Sequence\n",
    "import boto3\n",
    "from llama_index.core.llms import (\n",
    "    CustomLLM,\n",
    "    CompletionResponse,\n",
    "    CompletionResponseGen,\n",
    "    LLMMetadata,\n",
    ")\n",
    "from llama_index.core.llms.callbacks import llm_completion_callback\n",
    "from llama_index.core.base.llms.types import ChatMessage\n",
    "\n",
    "\n",
    "class BedrockNovaLLM(CustomLLM):\n",
    "    context_window: int = 300000  # Adjust based on model specs\n",
    "    num_output: int = 5000  # Adjust based on your needs\n",
    "    model_name: str = \"us.amazon.nova-2-lite-v1:0\"\n",
    "    temperature: float = 0.7\n",
    "    max_reasoning_effort: str = \"medium\"  # low, medium, high\n",
    "    enable_reasoning: bool = True\n",
    "    tools: Optional[list] = None\n",
    "    system_prompt: Optional[str] = None\n",
    "    \n",
    "    # Boto3 client\n",
    "    _client: Any = None\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # Initialize Bedrock client\n",
    "        self._client = boto3.client(\n",
    "            service_name='bedrock-runtime',\n",
    "            region_name=kwargs.get('region_name', 'us-east-1')\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def metadata(self) -> LLMMetadata:\n",
    "        \"\"\"Get LLM metadata.\"\"\"\n",
    "        return LLMMetadata(\n",
    "            context_window=self.context_window,\n",
    "            num_output=self.num_output,\n",
    "            model_name=self.model_name,\n",
    "        )\n",
    "\n",
    "    def _prepare_request(self, prompt: str, **kwargs: Any) -> dict:\n",
    "        \"\"\"Prepare the request payload for Bedrock.\"\"\"\n",
    "        request = {\n",
    "            \"modelId\": self.model_name,\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [{\"text\": prompt}]\n",
    "                }\n",
    "            ],\n",
    "            \"inferenceConfig\": {\n",
    "                \"temperature\": kwargs.get(\"temperature\", self.temperature),\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Add system prompt if provided\n",
    "        if self.system_prompt:\n",
    "            request[\"system\"] = [{\"text\": self.system_prompt}]\n",
    "        \n",
    "        # Add tools if provided\n",
    "        if self.tools:\n",
    "            request[\"toolConfig\"] = {\"tools\": self.tools}\n",
    "        \n",
    "        # Add reasoning config if enabled\n",
    "        if self.enable_reasoning:\n",
    "            request[\"additionalModelRequestFields\"] = {\n",
    "                \"reasoningConfig\": {\n",
    "                    \"type\": \"enabled\",\n",
    "                    \"maxReasoningEffort\": kwargs.get(\n",
    "                        \"max_reasoning_effort\", self.max_reasoning_effort\n",
    "                    )\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        return request\n",
    "\n",
    "    def _process_stream(self, response_stream: dict) -> str:\n",
    "        \"\"\"Process the streaming response from Bedrock.\"\"\"\n",
    "        full_text = \"\"\n",
    "        reasoning_text = \"\"\n",
    "        tool_use = None\n",
    "        \n",
    "        try:\n",
    "            stream = response_stream.get('stream')\n",
    "            for event in stream:\n",
    "                if 'contentBlockStart' in event:\n",
    "                    block_start = event['contentBlockStart']\n",
    "                    if 'start' in block_start:\n",
    "                        start = block_start['start']\n",
    "                        if 'reasoning' in start:\n",
    "                            reasoning_text = \"\"\n",
    "                \n",
    "                elif 'contentBlockDelta' in event:\n",
    "                    delta = event['contentBlockDelta']['delta']\n",
    "                    \n",
    "                    if 'text' in delta:\n",
    "                        text = delta['text']\n",
    "                        full_text += text\n",
    "                    \n",
    "                    elif 'reasoning' in delta:\n",
    "                        reasoning_text += delta['reasoning']['text']\n",
    "                    \n",
    "                    elif 'toolUse' in delta:\n",
    "                        tool_use = delta['toolUse']\n",
    "                \n",
    "                elif 'messageStop' in event:\n",
    "                    break\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing stream: {e}\")\n",
    "            raise\n",
    "        \n",
    "        # If tool use was detected, format it in the response\n",
    "        if tool_use:\n",
    "            full_text += f\"\\n[Tool Use: {tool_use}]\"\n",
    "        \n",
    "        return full_text\n",
    "\n",
    "    @llm_completion_callback()\n",
    "    def complete(self, prompt: str, formatted: bool = False, **kwargs: Any) -> CompletionResponse:\n",
    "        \"\"\"Complete the prompt (non-streaming).\"\"\"\n",
    "        request = self._prepare_request(prompt, **kwargs)\n",
    "        \n",
    "        # Use streaming but collect all results\n",
    "        response_stream = self._client.converse_stream(**request)\n",
    "        text = self._process_stream(response_stream)\n",
    "        \n",
    "        return CompletionResponse(text=text)\n",
    "\n",
    "    @llm_completion_callback()\n",
    "    def stream_complete(\n",
    "        self, prompt: str, formatted: bool = False, **kwargs: Any\n",
    "    ) -> CompletionResponseGen:\n",
    "        \"\"\"Stream the completion response.\"\"\"\n",
    "        request = self._prepare_request(prompt, **kwargs)\n",
    "        response_stream = self._client.converse_stream(**request)\n",
    "        \n",
    "        accumulated_text = \"\"\n",
    "        \n",
    "        try:\n",
    "            stream = response_stream.get('stream')\n",
    "            for event in stream:\n",
    "                if 'contentBlockDelta' in event:\n",
    "                    delta = event['contentBlockDelta']['delta']\n",
    "                    \n",
    "                    if 'text' in delta:\n",
    "                        token = delta['text']\n",
    "                        accumulated_text += token\n",
    "                        yield CompletionResponse(\n",
    "                            text=accumulated_text,\n",
    "                            delta=token\n",
    "                        )\n",
    "                    \n",
    "                    elif 'reasoning' in delta:\n",
    "                        # Optionally yield reasoning tokens\n",
    "                        token = delta['reasoning']['text']\n",
    "                        # You can choose to include or exclude reasoning\n",
    "                        # accumulated_text += f\"[Reasoning: {token}]\"\n",
    "                        pass\n",
    "                    \n",
    "                    elif 'toolUse' in delta:\n",
    "                        tool_use = delta['toolUse']\n",
    "                        token = f\"\\n[Tool Use: {tool_use}]\"\n",
    "                        accumulated_text += token\n",
    "                        yield CompletionResponse(\n",
    "                            text=accumulated_text,\n",
    "                            delta=token\n",
    "                        )\n",
    "                \n",
    "                elif 'messageStop' in event:\n",
    "                    break\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error in stream_complete: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings, SimpleDirectoryReader, SummaryIndex\n",
    "\n",
    "# Initialize your custom LLM\n",
    "llm = BedrockNovaLLM(\n",
    "    region_name='us-east-1',  # Change to your region\n",
    "    temperature=0.7,\n",
    "    enable_reasoning=True,\n",
    "    max_reasoning_effort=\"medium\",\n",
    ")\n",
    "\n",
    "# Set as default LLM\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.llms.bedrock_converse import BedrockConverse\n",
    "from llama_index.embeddings.bedrock import BedrockEmbedding\n",
    "# Load documents\n",
    "documents = SimpleDirectoryReader(\"data/\").load_data()\n",
    "\n",
    "# Create a vector store index\n",
    "index = VectorStoreIndex.from_documents(documents, embed_model=BedrockEmbedding())\n",
    "\n",
    "# Query the index\n",
    "response = index.as_query_engine(llm=llm).query(\"Can you tell me about the key concepts for safety finetuning\")\n",
    "\n",
    "# Print the response\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "repeatable-patterns-H6m-84_c",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
