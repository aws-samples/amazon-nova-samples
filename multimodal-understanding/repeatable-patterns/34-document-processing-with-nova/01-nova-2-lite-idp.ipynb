{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "md-0",
   "metadata": {},
   "source": [
    "# Intelligent Document Processing with Amazon Nova 2 Lite\n",
    "\n",
    "This notebook demonstrates how to build a comprehensive **Intelligent Document Processing (IDP)** system using **Amazon Nova 2 Lite** on Amazon Bedrock.\n",
    "\n",
    "We showcase how a single foundation model can handle the full IDP lifecycle — from document classification and summarization, through structured data extraction with schema enforcement, to validation with extended thinking and computational analysis with the built-in code interpreter.\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "| Capability | Description |\n",
    "|---|---|\n",
    "| **Document Classification** | Automatically identify document types (bank statements, claims, medical forms) |\n",
    "| **Summarization** | Generate structured summaries from PDFs and scanned images |\n",
    "| **Structured Extraction** | Extract schema-compliant JSON using tool configurations (blueprints) |\n",
    "| **Bounding Box Visualization** | Localize extracted fields spatially on document images |\n",
    "| **Extended Thinking** | Compare reasoning depth levels (low/medium/high) for complex extraction |\n",
    "| **Multi-Turn Document Q&A** | Interactive conversation over documents with context retention |\n",
    "| **Code Interpreter** | Built-in Python sandbox for computation, validation, and analysis |\n",
    "| **End-to-End Pipeline** | Classify → Extract → Validate → Analyze → Report |\n",
    "\n",
    "### Sample Documents\n",
    "\n",
    "This notebook processes the following documents from the `samples/` folder:\n",
    "\n",
    "| File | Type | Format |\n",
    "|---|---|---|\n",
    "| `BankStatement.pdf` | Bank statement (native PDF) | Single-page PDF |\n",
    "| `BankStatement.jpg` | Bank statement (scanned) | JPEG image |\n",
    "| `claim-form.png` | CMS-1500 medical claim form | PNG image |\n",
    "| `claims-pack.pdf` | Multi-page claims package | 10-page PDF (scanned images) |\n",
    "| `sample1_cms-1500-P.pdf` | CMS-1500 medical claim form | Single-page PDF |\n",
    "\n",
    "### Known PDF Limitations\n",
    "\n",
    "Some PDFs may not be processed directly by Nova due to unsupported internal formats:\n",
    "- **CMYK color profiles** — PDFs using CMYK instead of RGB color space\n",
    "- **ICC color profiles** — PDFs with embedded ICC color management profiles\n",
    "- **Transparency masks** — PDFs with alpha channels or transparency layers\n",
    "- **SVG images** — PDFs containing embedded SVG graphics\n",
    "\n",
    "When this happens, our utility functions automatically **fall back to converting PDF pages to JPEG images** using PyMuPDF, then send the images instead. This preserves all visual content while avoiding the format restrictions.\n",
    "\n",
    "> **Note**: The image fallback uses **PyMuPDF** \n",
    "### Prerequisites\n",
    "\n",
    "- An AWS account with Amazon Bedrock access enabled\n",
    "- Amazon Nova 2 Lite model access enabled\n",
    "- Python 3.10+ with `boto3`, `Pillow`, and `PyMuPDF`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-1",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eff9ba5-fa3e-4953-8561-f1a18ffaef76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, sys, os\n",
    "\n",
    "def pip_install(*packages):\n",
    "    subprocess.check_call(\n",
    "        [sys.executable, '-m', 'pip', 'install', '-q'] + list(packages),\n",
    "        env={k: v for k, v in os.environ.items() if 'BASH_FUNC' not in k}\n",
    "    )\n",
    "\n",
    "def conda_install(*packages):\n",
    "    conda_exe = os.environ.get('CONDA_EXE', '')\n",
    "    if conda_exe and os.path.exists(conda_exe):\n",
    "        result = subprocess.run(\n",
    "            [conda_exe, 'install', '-y', '-q', '-c', 'conda-forge'] + list(packages),\n",
    "            capture_output=True, text=True,\n",
    "            env={k: v for k, v in os.environ.items() if 'BASH_FUNC' not in k}\n",
    "        )\n",
    "        return result.returncode == 0\n",
    "    return False\n",
    "\n",
    "# Install PyMuPDF (PDF-to-image conversion)\n",
    "try:\n",
    "    import fitz\n",
    "    print(f'✓ PyMuPDF already installed (version {fitz.VersionBind})')\n",
    "except ImportError:\n",
    "    print('Installing PyMuPDF...')\n",
    "    if not conda_install('pymupdf'):\n",
    "        pip_install('PyMuPDF')\n",
    "    import fitz\n",
    "    print(f'✓ PyMuPDF installed (version {fitz.VersionBind})')\n",
    "\n",
    "# Install remaining dependencies\n",
    "pip_install('boto3>=1.40.26', 'botocore>=1.40.26', 'pillow')\n",
    "print('✓ All dependencies installed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import base64\n",
    "import sagemaker\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "from botocore.config import Config\n",
    "from botocore.exceptions import ClientError\n",
    "from IPython.display import display, Image, IFrame, Markdown, JSON\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sts_client = boto3.client('sts')\n",
    "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "session = sagemaker.Session()\n",
    "\n",
    "REGION = boto3.session.Session().region_name\n",
    "NOVA_LITE_MODEL_ID = \"us.amazon.nova-2-lite-v1:0\"\n",
    "SAMPLES_DIR = \"samples\"\n",
    "\n",
    "bedrock_client = boto3.client(\n",
    "    \"bedrock-runtime\",\n",
    "    region_name=REGION,\n",
    "    config=Config(connect_timeout=300, read_timeout=300, retries={\"max_attempts\": 3})\n",
    ")\n",
    "\n",
    "print(f\"Bedrock client initialized in region: {REGION}\")\n",
    "print(f\"Nova 2 Lite model: {NOVA_LITE_MODEL_ID}\")\n",
    "\n",
    "print(f\"\\nSample documents:\")\n",
    "SUPPORTED_EXTENSIONS = {'.pdf', '.png', '.jpg', '.jpeg', '.gif', '.webp', '.csv', '.html', '.txt', '.md', '.doc', '.docx', '.xls', '.xlsx'}\n",
    "sample_files = sorted(f for f in Path(SAMPLES_DIR).iterdir() if f.is_file() and f.suffix.lower() in SUPPORTED_EXTENSIONS)\n",
    "\n",
    "for f in sample_files:\n",
    "    size_kb = f.stat().st_size / 1024\n",
    "    print(f\"  - {f.name} ({size_kb:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-10",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Import Utility Functions\n",
    "\n",
    "All reusable helpers are defined in `utils.py` — this includes document format detection, Bedrock Converse API wrappers, response parsing, and bounding box visualization functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import (\n",
    "    get_document_format, is_image_format, build_content_block,\n",
    "    build_content_blocks_from_pdf_images,\n",
    "    invoke_nova, extract_text, extract_tool_input, show_usage,\n",
    "    get_document_image, get_color_for_field, draw_bounding_boxes,\n",
    "    invoke_nova_with_reasoning, extract_reasoning_and_text,\n",
    "    invoke_nova_with_code_interpreter, extract_code_interpreter_results,\n",
    "    CODE_INTERPRETER_TOOL\n",
    ")\n",
    "\n",
    "print(\"Utility functions loaded from utils.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-12",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Document Classification\n",
    "\n",
    "Document classification is the critical first step in any IDP pipeline — routing documents to the appropriate extraction logic.\n",
    "\n",
    "Here we use Amazon Nova's multimodal understanding to classify documents by type, detecting whether a document is a bank statement, insurance claim form, medical claim form, etc. We use **Nova 2 Lite** for fast, cost-effective classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSIFY_SYSTEM = \"\"\"You are an expert document classifier for an Intelligent Document Processing system.\n",
    "Analyze documents and classify them into predefined categories with high accuracy.\n",
    "Always respond with valid JSON only - no markdown formatting, no explanation outside the JSON.\"\"\"\n",
    "\n",
    "CLASSIFY_PROMPT = \"\"\"Analyze the provided document and classify it into one of these categories:\n",
    "\n",
    "- BANK_STATEMENT: Bank account statements showing transactions, balances, account details\n",
    "- MEDICAL_CLAIM_FORM: Medical/healthcare claim forms (CMS-1500, UB-04, HCFA)\n",
    "- INVOICE: Bills or invoices for goods/services\n",
    "- OTHER: Documents that don't fit the above categories\n",
    "\n",
    "Return a JSON object with these fields:\n",
    "{\"document_type\": \"<CATEGORY>\", \"confidence\": \"HIGH|MEDIUM|LOW\", \"reasoning\": \"<brief explanation>\", \"detected_fields\": [\"<key field types found>\"]}\"\"\"\n",
    "\n",
    "classification_results = {}\n",
    "\n",
    "for fp in sample_files:\n",
    "    fmt = get_document_format(str(fp))\n",
    "    \n",
    "    # For multi-page PDFs, classify each page individually\n",
    "    if fmt == \"pdf\":\n",
    "        try:\n",
    "            import fitz\n",
    "            doc = fitz.open(str(fp))\n",
    "            num_pages = len(doc)\n",
    "            doc.close()\n",
    "        except Exception:\n",
    "            num_pages = 1\n",
    "        \n",
    "        if num_pages > 1:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Classifying: {fp.name} ({num_pages} pages — classifying each page)\")\n",
    "            print(\"=\" * 60)\n",
    "            \n",
    "            page_images = build_content_blocks_from_pdf_images(str(fp), max_pages=num_pages)\n",
    "            page_results = []\n",
    "            \n",
    "            for page_num, img_block in enumerate(page_images, 1):\n",
    "                print(f\"\\n  --- Page {page_num}/{num_pages} ---\")\n",
    "                # Send single page image for classification\n",
    "                messages = [{\"role\": \"user\", \"content\": [\n",
    "                    {\"text\": CLASSIFY_PROMPT},\n",
    "                    img_block\n",
    "                ]}]\n",
    "                resp = bedrock_client.converse(\n",
    "                    modelId=NOVA_LITE_MODEL_ID,\n",
    "                    messages=messages,\n",
    "                    system=[{\"text\": CLASSIFY_SYSTEM}],\n",
    "                    inferenceConfig={\"maxTokens\": 1024, \"temperature\": 0, \"topP\": 0.9}\n",
    "                )\n",
    "                txt = extract_text(resp)\n",
    "                print(f\"  {txt[:150]}...\")\n",
    "                show_usage(resp)\n",
    "                \n",
    "                try:\n",
    "                    clean = txt.strip()\n",
    "                    if clean.startswith(\"```\"):\n",
    "                        clean = clean.split(\"\\n\", 1)[1].rsplit(\"```\", 1)[0]\n",
    "                    result = json.loads(clean)\n",
    "                    result[\"page\"] = page_num\n",
    "                    page_results.append(result)\n",
    "                except json.JSONDecodeError:\n",
    "                    page_results.append({\"page\": page_num, \"raw_response\": txt})\n",
    "            \n",
    "            classification_results[fp.name] = {\"pages\": page_results, \"total_pages\": num_pages}\n",
    "            continue\n",
    "    \n",
    "    # Single-page documents (images and single-page PDFs)\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Classifying: {fp.name}\")\n",
    "    print(\"=\" * 60)\n",
    "    resp = invoke_nova(bedrock_client,\n",
    "        CLASSIFY_PROMPT, file_paths=str(fp), model_id=NOVA_LITE_MODEL_ID,\n",
    "        system_prompt=CLASSIFY_SYSTEM, max_tokens=1024\n",
    "    )\n",
    "    txt = extract_text(resp)\n",
    "    print(txt)\n",
    "    show_usage(resp)\n",
    "    try:\n",
    "        clean = txt.strip()\n",
    "        if clean.startswith(\"```\"):\n",
    "            clean = clean.split(\"\\n\", 1)[1].rsplit(\"```\", 1)[0]\n",
    "        classification_results[fp.name] = json.loads(clean)\n",
    "    except json.JSONDecodeError:\n",
    "        classification_results[fp.name] = {\"raw_response\": txt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CLASSIFICATION SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "dash = \"-\"\n",
    "print(f\"{'File':<30} {'Page':<6} {'Type':<25} {'Confidence'}\")\n",
    "print(\"-\" * 70)\n",
    "for fn, r in classification_results.items():\n",
    "    if \"pages\" in r:\n",
    "        for pr in r[\"pages\"]:\n",
    "            page = pr.get(\"page\", \"?\")\n",
    "            print(f\"{fn:<30} {str(page):<6} {pr.get('document_type', '?'):<25} {pr.get('confidence', '?')}\")\n",
    "    else:\n",
    "        print(f\"{fn:<30} {dash:<6} {r.get('document_type', '?'):<25} {r.get('confidence', '?')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-15",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Document Summarization\n",
    "\n",
    "Amazon Nova can generate comprehensive summaries from documents, extracting key information and presenting it in a structured format.\n",
    "\n",
    "Here we extract summaries and key insights from each document type — processing both native PDF and scanned image documents through Nova 2 Lite's multimodal Converse API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUMMARY_SYSTEM = \"\"\"You are an expert document analyst. Extract and present information in a clear, structured format.\n",
    "Be thorough and accurate - do not fabricate information not present in the document.\"\"\"\n",
    "\n",
    "BANK_PROMPT = \"\"\"Analyze this bank statement and provide a comprehensive summary:\n",
    "\n",
    "1. **Account Information**: Account holder, account number, statement period, bank name\n",
    "2. **Financial Summary**: Opening balance, closing balance, total credits, total debits\n",
    "3. **Transaction Overview**: Number of transactions, largest transaction, recurring patterns\n",
    "4. **Key Insights**: Notable observations about spending patterns or account activity\n",
    "\n",
    "Present your analysis in well-structured markdown.\"\"\"\n",
    "\n",
    "print(\"Processing: BankStatement.pdf\")\n",
    "resp = invoke_nova(bedrock_client, BANK_PROMPT, f\"{SAMPLES_DIR}/BankStatement.pdf\",\n",
    "                   model_id=NOVA_LITE_MODEL_ID, system_prompt=SUMMARY_SYSTEM, max_tokens=4096)\n",
    "show_usage(resp)\n",
    "Markdown(extract_text(resp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Processing: BankStatement.jpg (scanned image)\")\n",
    "resp = invoke_nova(bedrock_client, BANK_PROMPT, f\"{SAMPLES_DIR}/BankStatement.jpg\",\n",
    "                   model_id=NOVA_LITE_MODEL_ID, system_prompt=SUMMARY_SYSTEM, max_tokens=4096)\n",
    "show_usage(resp)\n",
    "Markdown(extract_text(resp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLAIM_PROMPT = \"\"\"Analyze this medical claim form and provide a comprehensive summary:\n",
    "\n",
    "1. **Claimant Information**: Name, policy number, contact details\n",
    "2. **Claim Details**: Type of claim, date of incident/loss, description\n",
    "3. **Financial Details**: Claimed amount, deductibles\n",
    "4. **Form Status**: Whether form appears complete, any missing fields\n",
    "5. **Key Observations**: Notable details or flags\n",
    "\n",
    "Present in well-structured markdown.\"\"\"\n",
    "\n",
    "print(\"Processing: claim-form.png\")\n",
    "resp = invoke_nova(bedrock_client, CLAIM_PROMPT, f\"{SAMPLES_DIR}/claim-form.png\",\n",
    "                   model_id=NOVA_LITE_MODEL_ID, system_prompt=SUMMARY_SYSTEM, max_tokens=4096)\n",
    "show_usage(resp)\n",
    "Markdown(extract_text(resp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-20",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Structured Data Extraction with Blueprints\n",
    "\n",
    "One of the most powerful IDP capabilities is extracting structured JSON data that conforms to a predefined schema. We achieve this using the Converse API's **tool configuration** — the tool schema acts as a \"blueprint\" that constrains Nova to return data matching the exact structure you need.\n",
    "\n",
    "This approach gives you:\n",
    "- **Schema enforcement** — output always matches your expected structure\n",
    "- **Type safety** — numbers, booleans, arrays are properly typed\n",
    "- **Completeness** — required fields are always attempted\n",
    "- **Consistency** — same schema produces comparable outputs across documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-21",
   "metadata": {},
   "source": [
    "### 5.1 Bank Statement Blueprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_statement_tool = {\n",
    "    \"tools\": [\n",
    "        {\n",
    "            \"toolSpec\": {\n",
    "                \"name\": \"extract_bank_statement\",\n",
    "                \"description\": \"Extract structured data from a bank statement document.\",\n",
    "                \"inputSchema\": {\n",
    "                    \"json\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"bank_name\": {\n",
    "                                \"type\": \"string\"\n",
    "                            },\n",
    "                            \"account_holder_name\": {\n",
    "                                \"type\": \"string\"\n",
    "                            },\n",
    "                            \"account_number\": {\n",
    "                                \"type\": \"string\"\n",
    "                            },\n",
    "                            \"statement_period\": {\n",
    "                                \"type\": \"object\",\n",
    "                                \"properties\": {\n",
    "                                    \"start_date\": {\n",
    "                                        \"type\": \"string\",\n",
    "                                        \"format\": \"date\"\n",
    "                                    },\n",
    "                                    \"end_date\": {\n",
    "                                        \"type\": \"string\",\n",
    "                                        \"format\": \"date\"\n",
    "                                    }\n",
    "                                },\n",
    "                                \"required\": [\n",
    "                                    \"start_date\",\n",
    "                                    \"end_date\"\n",
    "                                ]\n",
    "                            },\n",
    "                            \"opening_balance\": {\n",
    "                                \"type\": \"number\"\n",
    "                            },\n",
    "                            \"closing_balance\": {\n",
    "                                \"type\": \"number\"\n",
    "                            },\n",
    "                            \"total_credits\": {\n",
    "                                \"type\": \"number\"\n",
    "                            },\n",
    "                            \"total_debits\": {\n",
    "                                \"type\": \"number\"\n",
    "                            },\n",
    "                            \"currency\": {\n",
    "                                \"type\": \"string\"\n",
    "                            },\n",
    "                            \"transactions\": {\n",
    "                                \"type\": \"array\",\n",
    "                                \"items\": {\n",
    "                                    \"type\": \"object\",\n",
    "                                    \"properties\": {\n",
    "                                        \"date\": {\n",
    "                                            \"type\": \"string\",\n",
    "                                            \"format\": \"date\"\n",
    "                                        },\n",
    "                                        \"description\": {\n",
    "                                            \"type\": \"string\"\n",
    "                                        },\n",
    "                                        \"amount\": {\n",
    "                                            \"type\": \"number\"\n",
    "                                        },\n",
    "                                        \"type\": {\n",
    "                                            \"type\": \"string\",\n",
    "                                            \"enum\": [\n",
    "                                                \"credit\",\n",
    "                                                \"debit\"\n",
    "                                            ]\n",
    "                                        },\n",
    "                                        \"balance\": {\n",
    "                                            \"type\": \"number\"\n",
    "                                        }\n",
    "                                    }\n",
    "                                }\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\n",
    "                            \"bank_name\",\n",
    "                            \"account_holder_name\",\n",
    "                            \"account_number\",\n",
    "                            \"statement_period\",\n",
    "                            \"opening_balance\",\n",
    "                            \"closing_balance\"\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"toolChoice\": {\n",
    "        \"tool\": {\n",
    "            \"name\": \"extract_bank_statement\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Bank Statement Blueprint defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extracting structured data: BankStatement.pdf\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "resp = invoke_nova(bedrock_client,\n",
    "    model_id=NOVA_LITE_MODEL_ID,\n",
    "    prompt=\"Extract all structured data from this bank statement document.\",\n",
    "    file_paths=f\"{SAMPLES_DIR}/BankStatement.pdf\",\n",
    "    system_prompt=\"You are a document data extraction expert. Extract all data accurately.\",\n",
    "    tool_config=bank_statement_tool, max_tokens=8000\n",
    ")\n",
    "\n",
    "bank_data = extract_tool_input(resp)\n",
    "show_usage(resp)\n",
    "\n",
    "if bank_data:\n",
    "    print(\"\\nExtracted Bank Statement Data:\")\n",
    "    print(json.dumps(bank_data, indent=2))\n",
    "else:\n",
    "    print(\"Fallback to text response:\")\n",
    "    print(extract_text(resp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extracting structured data: BankStatement.jpg (scanned)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "resp = invoke_nova(bedrock_client,\n",
    "    model_id=NOVA_LITE_MODEL_ID,\n",
    "    prompt=\"Extract all structured data from this scanned bank statement image.\",\n",
    "    file_paths=f\"{SAMPLES_DIR}/BankStatement.jpg\",\n",
    "    system_prompt=\"You are a document data extraction expert. Extract data accurately from scanned images.\",\n",
    "    tool_config=bank_statement_tool, max_tokens=8000\n",
    ")\n",
    "\n",
    "bank_img_data = extract_tool_input(resp)\n",
    "show_usage(resp)\n",
    "\n",
    "if bank_img_data:\n",
    "    print(\"\\nExtracted Bank Statement Data (from image):\")\n",
    "    print(json.dumps(bank_img_data, indent=2))\n",
    "else:\n",
    "    print(\"Fallback to text response:\")\n",
    "    print(extract_text(resp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-25",
   "metadata": {},
   "source": [
    "### 5.2 CMS-1500 Medical Claim Form Blueprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "cms1500_tool = {\n",
    "    \"tools\": [\n",
    "        {\n",
    "            \"toolSpec\": {\n",
    "                \"name\": \"extract_cms1500\",\n",
    "                \"description\": \"Extract structured data from a CMS-1500 Health Insurance Claim Form.\",\n",
    "                \"inputSchema\": {\n",
    "                    \"json\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"insurance_type\": {\n",
    "                                \"type\": \"string\"\n",
    "                            },\n",
    "                            \"insured_id\": {\n",
    "                                \"type\": \"string\"\n",
    "                            },\n",
    "                            \"patient\": {\n",
    "                                \"type\": \"object\",\n",
    "                                \"properties\": {\n",
    "                                    \"name\": {\n",
    "                                        \"type\": \"string\"\n",
    "                                    },\n",
    "                                    \"date_of_birth\": {\n",
    "                                        \"type\": \"string\",\n",
    "                                        \"format\": \"date\"\n",
    "                                    },\n",
    "                                    \"gender\": {\n",
    "                                        \"type\": \"string\",\n",
    "                                        \"enum\": [\n",
    "                                            \"M\",\n",
    "                                            \"F\"\n",
    "                                        ]\n",
    "                                    },\n",
    "                                    \"address\": {\n",
    "                                        \"type\": \"string\"\n",
    "                                    },\n",
    "                                    \"city\": {\n",
    "                                        \"type\": \"string\"\n",
    "                                    },\n",
    "                                    \"state\": {\n",
    "                                        \"type\": \"string\"\n",
    "                                    },\n",
    "                                    \"zip\": {\n",
    "                                        \"type\": \"string\"\n",
    "                                    },\n",
    "                                    \"phone\": {\n",
    "                                        \"type\": \"string\"\n",
    "                                    }\n",
    "                                },\n",
    "                                \"required\": [\n",
    "                                    \"name\"\n",
    "                                ]\n",
    "                            },\n",
    "                            \"insured\": {\n",
    "                                \"type\": \"object\",\n",
    "                                \"properties\": {\n",
    "                                    \"name\": {\n",
    "                                        \"type\": \"string\"\n",
    "                                    },\n",
    "                                    \"policy_group_number\": {\n",
    "                                        \"type\": \"string\"\n",
    "                                    },\n",
    "                                    \"plan_name\": {\n",
    "                                        \"type\": \"string\"\n",
    "                                    }\n",
    "                                }\n",
    "                            },\n",
    "                            \"diagnosis_codes\": {\n",
    "                                \"type\": \"array\",\n",
    "                                \"items\": {\n",
    "                                    \"type\": \"string\"\n",
    "                                }\n",
    "                            },\n",
    "                            \"service_lines\": {\n",
    "                                \"type\": \"array\",\n",
    "                                \"items\": {\n",
    "                                    \"type\": \"object\",\n",
    "                                    \"properties\": {\n",
    "                                        \"date_from\": {\n",
    "                                            \"type\": \"string\",\n",
    "                                            \"format\": \"date\"\n",
    "                                        },\n",
    "                                        \"date_to\": {\n",
    "                                            \"type\": \"string\",\n",
    "                                            \"format\": \"date\"\n",
    "                                        },\n",
    "                                        \"place_of_service\": {\n",
    "                                            \"type\": \"string\"\n",
    "                                        },\n",
    "                                        \"procedure_code\": {\n",
    "                                            \"type\": \"string\"\n",
    "                                        },\n",
    "                                        \"modifier\": {\n",
    "                                            \"type\": \"string\"\n",
    "                                        },\n",
    "                                        \"diagnosis_pointer\": {\n",
    "                                            \"type\": \"string\"\n",
    "                                        },\n",
    "                                        \"charges\": {\n",
    "                                            \"type\": \"number\"\n",
    "                                        },\n",
    "                                        \"units\": {\n",
    "                                            \"type\": \"integer\"\n",
    "                                        }\n",
    "                                    }\n",
    "                                }\n",
    "                            },\n",
    "                            \"total_charges\": {\n",
    "                                \"type\": \"number\"\n",
    "                            },\n",
    "                            \"amount_paid\": {\n",
    "                                \"type\": \"number\"\n",
    "                            },\n",
    "                            \"balance_due\": {\n",
    "                                \"type\": \"number\"\n",
    "                            },\n",
    "                            \"referring_provider\": {\n",
    "                                \"type\": \"string\"\n",
    "                            },\n",
    "                            \"rendering_provider\": {\n",
    "                                \"type\": \"object\",\n",
    "                                \"properties\": {\n",
    "                                    \"name\": {\n",
    "                                        \"type\": \"string\"\n",
    "                                    },\n",
    "                                    \"npi\": {\n",
    "                                        \"type\": \"string\"\n",
    "                                    }\n",
    "                                }\n",
    "                            },\n",
    "                            \"facility\": {\n",
    "                                \"type\": \"object\",\n",
    "                                \"properties\": {\n",
    "                                    \"name\": {\n",
    "                                        \"type\": \"string\"\n",
    "                                    },\n",
    "                                    \"address\": {\n",
    "                                        \"type\": \"string\"\n",
    "                                    },\n",
    "                                    \"npi\": {\n",
    "                                        \"type\": \"string\"\n",
    "                                    }\n",
    "                                }\n",
    "                            },\n",
    "                            \"patient_signature_on_file\": {\n",
    "                                \"type\": \"boolean\"\n",
    "                            },\n",
    "                            \"accept_assignment\": {\n",
    "                                \"type\": \"boolean\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\n",
    "                            \"patient\",\n",
    "                            \"diagnosis_codes\",\n",
    "                            \"service_lines\",\n",
    "                            \"total_charges\"\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"toolChoice\": {\n",
    "        \"tool\": {\n",
    "            \"name\": \"extract_cms1500\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"CMS-1500 Blueprint defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extracting structured data: sample1_cms-1500-P.pdf\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "resp = invoke_nova(bedrock_client,\n",
    "    model_id=NOVA_LITE_MODEL_ID,\n",
    "    prompt=\"Extract all structured data from this CMS-1500 health insurance claim form.\",\n",
    "    file_paths=f\"{SAMPLES_DIR}/sample1_cms-1500-P.pdf\",\n",
    "    system_prompt=\"You are a medical billing data extraction expert. Extract all fields accurately.\",\n",
    "    tool_config=cms1500_tool, max_tokens=8000\n",
    ")\n",
    "\n",
    "cms_data = extract_tool_input(resp)\n",
    "show_usage(resp)\n",
    "\n",
    "if cms_data:\n",
    "    print(\"\\nExtracted CMS-1500 Data:\")\n",
    "    print(json.dumps(cms_data, indent=2))\n",
    "else:\n",
    "    print(\"Fallback to text response:\")\n",
    "    print(extract_text(resp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb-md-0",
   "metadata": {},
   "source": [
    "---\n",
    "## 5.3 Document Visualization with Bounding Boxes\n",
    "\n",
    "After extracting structured data, we can ask Amazon Nova to also return the **spatial location** (bounding boxes) of each extracted field within the document. We then overlay these bounding boxes on the rendered document image to visualize what was extracted and where.\n",
    "\n",
    "This is particularly useful for:\n",
    "- **Verification**: Confirming the model extracted data from the correct location\n",
    "- **Debugging**: Identifying extraction errors or misaligned fields\n",
    "- **UI Integration**: Highlighting extracted fields in a document viewer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb-md-2",
   "metadata": {},
   "source": [
    "### 5.3.1 Extract Data with Bounding Boxes from Bank Statement (Image)\n",
    "\n",
    "We ask Nova to return both the extracted field values AND their spatial locations as normalized bounding box coordinates (0-1 range relative to document dimensions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb-code-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BB_EXTRACT_PROMPT = \"\"\"Analyze this document image and extract key fields with their spatial locations.\n",
    "\n",
    "For each field you find, return:\n",
    "- field_name: the name/type of the field\n",
    "- value: the extracted text value\n",
    "- bbox: bounding box as [x1, y1, x2, y2] — the top-left corner (x1,y1) and bottom-right corner (x2,y2).\n",
    "  ALL coordinates are NORMALIZED fractions from 0.0 to 1.0 relative to the full document dimensions.\n",
    "  (0,0) is the top-left of the document, (1,1) is the bottom-right.\n",
    "  IMPORTANT: x2 must be GREATER than x1, and y2 must be GREATER than y1.\n",
    "  These are absolute positions, NOT widths/heights.\n",
    "\n",
    "Return a JSON object with a \"fields\" array containing objects with field_name, value, and bbox.\n",
    "Example format:\n",
    "{\"fields\": [{\"field_name\": \"account_number\", \"value\": \"1234567890\", \"bbox\": [0.15, 0.12, 0.45, 0.16]}]}\n",
    "\n",
    "Extract ALL visible fields including headers, names, numbers, dates, amounts, and any other text content.\n",
    "Return ONLY valid JSON.\"\"\"\n",
    "\n",
    "BB_SYSTEM = \"You are a document analysis expert that extracts text with precise spatial locations. Return only valid JSON.\"\n",
    "\n",
    "# Process bank statement image\n",
    "print(\"Extracting fields with bounding boxes: BankStatement.jpg\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "resp = invoke_nova(bedrock_client,\n",
    "    prompt=BB_EXTRACT_PROMPT,\n",
    "    file_paths=f\"{SAMPLES_DIR}/BankStatement.jpg\",\n",
    "    model_id=NOVA_LITE_MODEL_ID,\n",
    "    system_prompt=BB_SYSTEM,\n",
    "    max_tokens=8000\n",
    ")\n",
    "\n",
    "show_usage(resp)\n",
    "txt = extract_text(resp)\n",
    "\n",
    "# Parse the response\n",
    "try:\n",
    "    clean = txt.strip()\n",
    "    if clean.startswith(\"```\"):\n",
    "        clean = clean.split(\"\\n\", 1)[1].rsplit(\"```\", 1)[0]\n",
    "    bb_data = json.loads(clean)\n",
    "    fields = bb_data.get(\"fields\", [])\n",
    "    print(f\"Extracted {len(fields)} fields with bounding boxes\")\n",
    "    for f in fields[:5]:\n",
    "        print(f\"  {f['field_name']}: {str(f['value'])[:50]} | bbox: {f.get('bbox')}\")\n",
    "    if len(fields) > 5:\n",
    "        print(f\"  ... and {len(fields)-5} more fields\")\n",
    "except json.JSONDecodeError:\n",
    "    print(\"Could not parse JSON response, showing raw text:\")\n",
    "    print(txt[:1000])\n",
    "    fields = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb-code-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize bounding boxes on the bank statement image\n",
    "if fields:\n",
    "    doc_img = get_document_image(f\"{SAMPLES_DIR}/BankStatement.jpg\")\n",
    "    if doc_img:\n",
    "        # Print raw bboxes for debugging coordinate issues\n",
    "        print(\"Raw bounding boxes from model:\")\n",
    "        for _f in fields[:5]:\n",
    "            fn = _f.get(\"field_name\", \"\"); bb = _f.get(\"bbox\"); print(f\"  {fn}: bbox={bb}\")\n",
    "        print()\n",
    "        annotated_img, legend = draw_bounding_boxes(\n",
    "            doc_img, fields, title=\"Bank Statement - Extracted Fields\"\n",
    "        )\n",
    "        \n",
    "        # Display the annotated image\n",
    "        print(f\"Document size: {doc_img.size[0]}x{doc_img.size[1]} pixels\")\n",
    "        print(f\"Fields visualized: {len([f for f in fields if f.get('bbox')])}\")\n",
    "        print()\n",
    "        \n",
    "        # Show legend\n",
    "        print(\"Legend:\")\n",
    "        for name, value, color in legend[:15]:\n",
    "            print(f\"  [{color}] {name}: {value}\")\n",
    "        \n",
    "        # Display in notebook\n",
    "        display(annotated_img)\n",
    "    else:\n",
    "        print(\"Could not load document image\")\n",
    "else:\n",
    "    print(\"No fields with bounding boxes to visualize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb-md-5",
   "metadata": {},
   "source": [
    "### 5.3.2 Extract Data with Bounding Boxes from Insurance Claim Form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb-code-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process claim form image\n",
    "print(\"Extracting fields with bounding boxes: claim-form.png\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "resp = invoke_nova(bedrock_client,\n",
    "    prompt=BB_EXTRACT_PROMPT,\n",
    "    file_paths=f\"{SAMPLES_DIR}/claim-form.png\",\n",
    "    model_id=NOVA_LITE_MODEL_ID,\n",
    "    system_prompt=BB_SYSTEM,\n",
    "    max_tokens=8000\n",
    ")\n",
    "\n",
    "show_usage(resp)\n",
    "txt = extract_text(resp)\n",
    "\n",
    "try:\n",
    "    clean = txt.strip()\n",
    "    if clean.startswith(\"```\"):\n",
    "        clean = clean.split(\"\\n\", 1)[1].rsplit(\"```\", 1)[0]\n",
    "    bb_claim = json.loads(clean)\n",
    "    claim_fields = bb_claim.get(\"fields\", [])\n",
    "    print(f\"Extracted {len(claim_fields)} fields with bounding boxes\")\n",
    "except json.JSONDecodeError:\n",
    "    print(\"Could not parse JSON response\")\n",
    "    claim_fields = []\n",
    "\n",
    "if claim_fields:\n",
    "    doc_img = get_document_image(f\"{SAMPLES_DIR}/claim-form.png\")\n",
    "    if doc_img:\n",
    "        annotated_img, legend = draw_bounding_boxes(\n",
    "            doc_img, claim_fields, title=\"Insurance Claim Form - Extracted Fields\"\n",
    "        )\n",
    "        print(f\"\\nDocument size: {doc_img.size[0]}x{doc_img.size[1]} pixels\")\n",
    "        print(f\"Fields visualized: {len([f for f in claim_fields if f.get('bbox')])}\")\n",
    "        display(annotated_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-31",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Extraction Mode Comparison: Standard vs Extended Thinking\n",
    "\n",
    "Nova 2 Lite can operate in two modes:\n",
    "- **Standard mode** (default): Fast, efficient extraction ideal for high-volume processing\n",
    "- **Extended thinking mode**: Deeper reasoning for complex documents requiring validation and cross-referencing\n",
    "\n",
    "Let's compare both modes on the same document to see the quality/speed tradeoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPARE_PROMPT = \"\"\"Extract all key-value pairs from this document as a JSON object.\n",
    "Be thorough — include every field, amount, date, name, and identifier.\"\"\"\n",
    "\n",
    "doc_path = f\"{SAMPLES_DIR}/sample1_cms-1500-P.pdf\"\n",
    "import time\n",
    "\n",
    "comparison_results = {}\n",
    "\n",
    "# Mode 1: Standard extraction (no reasoning)\n",
    "print(\"=\" * 60)\n",
    "print(\"MODE 1: Standard Extraction (no extended thinking)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "start = time.time()\n",
    "resp_standard = invoke_nova(bedrock_client,\n",
    "    prompt=COMPARE_PROMPT, file_paths=doc_path,\n",
    "    model_id=NOVA_LITE_MODEL_ID,\n",
    "    system_prompt=\"Extract all data as JSON.\", max_tokens=4096\n",
    ")\n",
    "elapsed_standard = time.time() - start\n",
    "txt_standard = extract_text(resp_standard)\n",
    "show_usage(resp_standard)\n",
    "print(f\"Time: {elapsed_standard:.2f}s\")\n",
    "print(f\"Output length: {len(txt_standard)} chars\")\n",
    "print(txt_standard[:800])\n",
    "\n",
    "comparison_results[\"standard\"] = {\n",
    "    \"time\": elapsed_standard,\n",
    "    \"length\": len(txt_standard),\n",
    "    \"tokens\": resp_standard.get(\"usage\", {}) if resp_standard else {}\n",
    "}\n",
    "\n",
    "# Mode 2: Extended thinking extraction (medium effort)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODE 2: Extended Thinking Extraction (medium effort)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "start = time.time()\n",
    "resp_thinking = invoke_nova_with_reasoning(bedrock_client,\n",
    "    prompt=COMPARE_PROMPT,\n",
    "    file_paths=doc_path,\n",
    "    reasoning_effort=\"medium\",\n",
    "    system_prompt=\"Extract all data as JSON. Reason carefully about each field.\"\n",
    ")\n",
    "elapsed_thinking = time.time() - start\n",
    "reasoning, txt_thinking = extract_reasoning_and_text(resp_thinking)\n",
    "show_usage(resp_thinking)\n",
    "print(f\"Time: {elapsed_thinking:.2f}s\")\n",
    "print(f\"Reasoning: {reasoning[:100]}...\")\n",
    "print(f\"Output length: {len(txt_thinking)} chars\")\n",
    "print(txt_thinking[:800])\n",
    "\n",
    "comparison_results[\"extended_thinking\"] = {\n",
    "    \"time\": elapsed_thinking,\n",
    "    \"length\": len(txt_thinking),\n",
    "    \"tokens\": resp_thinking.get(\"usage\", {}) if resp_thinking else {}\n",
    "}\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COMPARISON SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Mode':<25} {'Time (s)':<12} {'Output Len':<12} {'Input Tok':<12} {'Output Tok'}\")\n",
    "print(\"-\" * 75)\n",
    "for mode, data in comparison_results.items():\n",
    "    t = data[\"tokens\"]\n",
    "    print(f\"{mode:<25} {data['time']:<12.2f} {data['length']:<12} \"\n",
    "          f\"{t.get('inputTokens','?'):<12} {t.get('outputTokens','?')}\")\n",
    "\n",
    "print(\"\\nExtended thinking typically produces more thorough extraction,\")\n",
    "print(\"especially for complex documents with many interrelated fields.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-33",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Multi-Document Processing and Business Insights\n",
    "\n",
    "In production IDP systems, you often need to process multiple documents and generate aggregate insights.\n",
    "\n",
    "Here we process all sample documents, extract structured data from each, and then use Nova to generate cross-document business intelligence — identifying relationships, patterns, and actionable insights across the document portfolio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-34",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_extractions = {}\n",
    "\n",
    "UNIVERSAL_EXTRACT = \"\"\"Analyze this document and extract ALL information as a structured JSON object.\n",
    "Include:\n",
    "- document_type: what kind of document this is\n",
    "- all_fields: a dictionary of every field/value pair found\n",
    "- entities: list of people, organizations, addresses found\n",
    "- financial_data: any monetary amounts, totals, balances\n",
    "- dates: all dates found with their context\n",
    "- reference_numbers: any IDs, account numbers, policy numbers, claim numbers\n",
    "\n",
    "Return valid JSON only.\"\"\"\n",
    "\n",
    "EXTRACT_SYS = \"\"\"You are an expert document processing system. Extract all information accurately.\n",
    "Return only valid JSON - no markdown, no explanation.\"\"\"\n",
    "\n",
    "for fp in sample_files:\n",
    "    print(f\"\\nProcessing: {fp.name}\")\n",
    "    print(\"-\" * 40)\n",
    "    resp = invoke_nova(bedrock_client,\n",
    "        UNIVERSAL_EXTRACT, file_paths=str(fp), model_id=NOVA_LITE_MODEL_ID,\n",
    "        system_prompt=EXTRACT_SYS, max_tokens=6000\n",
    "    )\n",
    "    txt = extract_text(resp)\n",
    "    show_usage(resp)\n",
    "    try:\n",
    "        clean = txt.strip()\n",
    "        if clean.startswith(\"```\"):\n",
    "            clean = clean.split(\"\\n\", 1)[1].rsplit(\"```\", 1)[0]\n",
    "        all_extractions[fp.name] = json.loads(clean)\n",
    "        print(f\"  Extracted {len(json.dumps(all_extractions[fp.name]))} chars of structured data\")\n",
    "    except json.JSONDecodeError:\n",
    "        all_extractions[fp.name] = {\"raw_text\": txt[:500]}\n",
    "        print(f\"  Stored as raw text ({len(txt)} chars)\")\n",
    "\n",
    "print(f\"\\nTotal documents processed: {len(all_extractions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-35",
   "metadata": {},
   "source": [
    "### 7.1 Aggregate Business Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-36",
   "metadata": {},
   "outputs": [],
   "source": [
    "INSIGHTS_PROMPT = f\"\"\"You are a business analyst. Based on the following extracted data from multiple documents,\n",
    "provide comprehensive business insights:\n",
    "\n",
    "1. **Document Portfolio Summary**: Types and counts of documents processed\n",
    "2. **Financial Overview**: All monetary amounts found across documents, totals\n",
    "3. **Entity Analysis**: Key people, organizations mentioned across documents\n",
    "4. **Cross-Document Relationships**: Any connections between documents (shared entities, references)\n",
    "5. **Risk Flags**: Any anomalies, missing data, or concerns identified\n",
    "6. **Recommendations**: Actionable next steps based on the document analysis\n",
    "\n",
    "Extracted data from all documents:\n",
    "{json.dumps(all_extractions, indent=2, default=str)[:15000]}\n",
    "\n",
    "Provide your analysis in well-structured markdown.\"\"\"\n",
    "\n",
    "print(\"Generating aggregate business insights...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "resp = invoke_nova(bedrock_client,\n",
    "    prompt=INSIGHTS_PROMPT,\n",
    "    model_id=NOVA_LITE_MODEL_ID,\n",
    "    system_prompt=\"You are an expert business analyst specializing in document intelligence and cross-document analysis.\",\n",
    "    max_tokens=6000\n",
    ")\n",
    "\n",
    "show_usage(resp)\n",
    "Markdown(extract_text(resp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qa-md-0",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Interactive Document Q&A (Multi-Turn Conversation)\n",
    "\n",
    "A powerful capability of Amazon Nova is **multi-turn conversation over documents**. You can send a document once, then ask a series of follow-up questions — the model retains the full conversation context across turns.\n",
    "\n",
    "This is valuable for:\n",
    "- **Human-in-the-loop workflows**: An operator reviews extracted data and asks clarifying questions\n",
    "- **Deep document investigation**: Drilling into specific sections, cross-referencing data points\n",
    "- **Conversational data extraction**: Iteratively pulling different pieces of information\n",
    "\n",
    "The Converse API supports this natively through its `messages` list — each turn is a user/assistant message pair."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qa-md-1",
   "metadata": {},
   "source": [
    "### 8.1 Multi-Turn Q&A on a Medical Claim Form\n",
    "\n",
    "Let's demonstrate a multi-turn conversation where we:\n",
    "1. Send the CMS-1500 form and ask for a high-level overview\n",
    "2. Follow up with specific questions about diagnosis codes\n",
    "3. Ask about financial details\n",
    "4. Request a risk assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qa-code-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Document Q&A - Multi-turn conversation over a document\n",
    "doc_path = f\"{SAMPLES_DIR}/sample1_cms-1500-P.pdf\"\n",
    "doc_block = build_content_block(doc_path)\n",
    "\n",
    "QA_SYSTEM = \"You are a medical claims analyst. Answer questions about the attached document accurately and concisely.\"\n",
    "\n",
    "# Build conversation turns\n",
    "conversation_turns = [\n",
    "    \"Please review this CMS-1500 claim form and give me a brief overview: Who is the patient, what is the diagnosis, and what is the total amount claimed?\",\n",
    "    \"What specific procedure codes are listed? Are they consistent with the diagnosis codes?\",\n",
    "    \"Break down the charges by service line. Which service has the highest charge?\",\n",
    "    \"Based on your analysis, are there any red flags or concerns with this claim that should be reviewed?\"\n",
    "]\n",
    "\n",
    "# Run multi-turn conversation\n",
    "messages = []\n",
    "\n",
    "for turn_num, question in enumerate(conversation_turns, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Turn {turn_num}: USER\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(question)\n",
    "    \n",
    "    # First turn includes the document; subsequent turns are text-only\n",
    "    if turn_num == 1:\n",
    "        user_content = [{\"text\": question}, doc_block]\n",
    "    else:\n",
    "        user_content = [{\"text\": question}]\n",
    "    \n",
    "    messages.append({\"role\": \"user\", \"content\": user_content})\n",
    "    \n",
    "    # Call Nova\n",
    "    resp = bedrock_client.converse(\n",
    "        modelId=NOVA_LITE_MODEL_ID,\n",
    "        messages=messages,\n",
    "        system=[{\"text\": QA_SYSTEM}],\n",
    "        inferenceConfig={\"maxTokens\": 2000, \"temperature\": 0, \"topP\": 0.9}\n",
    "    )\n",
    "    \n",
    "    # Extract assistant response\n",
    "    assistant_content = resp[\"output\"][\"message\"][\"content\"]\n",
    "    answer = \"\\n\".join(c[\"text\"] for c in assistant_content if \"text\" in c)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Turn {turn_num}: ASSISTANT\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(answer[:1000])\n",
    "    if len(answer) > 1000:\n",
    "        print(f\"... ({len(answer) - 1000} more chars)\")\n",
    "    show_usage(resp)\n",
    "    \n",
    "    # Append assistant response to conversation history\n",
    "    messages.append({\"role\": \"assistant\", \"content\": assistant_content})\n",
    "\n",
    "print(f\"\\n\\nConversation complete: {len(messages)} messages ({len(messages)//2} turns)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qa-md-3",
   "metadata": {},
   "source": [
    "### 8.2 Comparing Documents in Conversation\n",
    "\n",
    "Nova can also compare multiple documents within a conversation — useful for detecting differences between document versions, cross-referencing claims with supporting evidence, or validating data across related documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qa-code-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare bank statement (PDF) with its scanned image version\n",
    "pdf_block = build_content_block(f\"{SAMPLES_DIR}/BankStatement.pdf\")\n",
    "img_block = build_content_block(f\"{SAMPLES_DIR}/BankStatement.jpg\")\n",
    "\n",
    "compare_messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "        {\"text\": \"\"\"I have two versions of the same bank statement - one is the original PDF \n",
    "and the other is a scanned image. Please compare them:\n",
    "\n",
    "1. Can you read both documents clearly?\n",
    "2. Are there any differences in the data between the two versions?\n",
    "3. Which version provides better data quality for automated extraction?\n",
    "4. Are there any fields that are readable in one version but not the other?\n",
    "\n",
    "Document 1 (PDF):\"\"\"},\n",
    "        pdf_block,\n",
    "        {\"text\": \"\\nDocument 2 (Scanned Image):\"},\n",
    "        img_block\n",
    "    ]\n",
    "}]\n",
    "\n",
    "resp = bedrock_client.converse(\n",
    "    modelId=NOVA_LITE_MODEL_ID,\n",
    "    messages=compare_messages,\n",
    "    system=[{\"text\": \"You are a document quality analyst. Compare the documents thoroughly.\"}],\n",
    "    inferenceConfig={\"maxTokens\": 2000, \"temperature\": 0, \"topP\": 0.9}\n",
    ")\n",
    "\n",
    "comparison = extract_text(resp)\n",
    "print(\"DOCUMENT COMPARISON RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(comparison)\n",
    "show_usage(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new-md-0",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Extended Thinking (Reasoning Modes)\n",
    "\n",
    "**Nova 2 Lite** supports **extended thinking** — an optional capability that enables deeper reasoning for complex problems. You control the reasoning depth with three effort levels:\n",
    "\n",
    "| Effort Level | Best For | Token Cost |\n",
    "|---|---|---|\n",
    "| **Low** | Tasks with moderate complexity, code review, structured analysis | Low overhead |\n",
    "| **Medium** | Multi-step tasks, debugging, planning with constraints | Moderate overhead |\n",
    "| **High** | STEM reasoning, advanced problem-solving, deep validation | Highest (up to 128K tokens) |\n",
    "\n",
    "For IDP, extended thinking is valuable for:\n",
    "- **Validating** extracted data for consistency and completeness\n",
    "- **Cross-referencing** fields within complex documents\n",
    "- **Detecting anomalies** or potential fraud indicators\n",
    "\n",
    "The reasoning content is currently `[REDACTED]` in responses but you are charged for reasoning tokens as they improve output quality.\n",
    "\n",
    "> **API**: `additionalModelRequestFields={\"reasoningConfig\": {\"type\": \"enabled\", \"maxReasoningEffort\": \"low|medium|high\"}}`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new-md-1",
   "metadata": {},
   "source": [
    "### 9.1 Comparing Reasoning Levels on Complex Document Extraction\n",
    "\n",
    "Let's compare all 3 reasoning levels on the CMS-1500 medical claim form — a complex, densely structured document where deeper reasoning should yield more accurate and complete extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "new-code-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "REASONING_PROMPT = \"\"\"Analyze this CMS-1500 medical claim form and extract ALL data.\n",
    "Then perform a thorough validation:\n",
    "\n",
    "1. Extract every field from the form including patient info, insured info, diagnosis codes, procedure codes, charges\n",
    "2. Validate: Do the line item charges sum to the total? Are all required fields present?\n",
    "3. Cross-reference: Are the diagnosis codes consistent with the procedures?\n",
    "4. Flag any anomalies, missing data, or potential issues\n",
    "\n",
    "Return your findings as a JSON object with keys: extracted_data, validation_results, anomalies_found\"\"\"\n",
    "\n",
    "REASONING_SYSTEM = \"You are a medical billing auditor. Extract data accurately and validate thoroughly. Return valid JSON only.\"\n",
    "\n",
    "doc_path = f\"{SAMPLES_DIR}/sample1_cms-1500-P.pdf\"\n",
    "results_by_effort = {}\n",
    "\n",
    "for effort in [\"low\", \"medium\", \"high\"]:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Reasoning Effort: {effort.upper()}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    start = time.time()\n",
    "    resp = invoke_nova_with_reasoning(\n",
    "        bedrock_client,\n",
    "        prompt=REASONING_PROMPT,\n",
    "        file_paths=doc_path,\n",
    "        reasoning_effort=effort,\n",
    "        system_prompt=REASONING_SYSTEM\n",
    "    )\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    reasoning, answer = extract_reasoning_and_text(resp)\n",
    "    show_usage(resp)\n",
    "    print(f\"Response time: {elapsed:.2f}s\")\n",
    "    print(f\"Reasoning content: {reasoning[:100]}...\")\n",
    "    print(f\"\\nAnswer (first 1500 chars):\")\n",
    "    print(answer[:1500])\n",
    "    if len(answer) > 1500:\n",
    "        print(f\"... ({len(answer) - 1500} more characters)\")\n",
    "\n",
    "    results_by_effort[effort] = {\n",
    "        \"time\": elapsed,\n",
    "        \"answer_length\": len(answer),\n",
    "        \"tokens\": resp.get(\"usage\", {}) if resp else {},\n",
    "        \"answer\": answer\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "new-code-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare results across reasoning levels\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"REASONING LEVEL COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Effort':<10} {'Time (s)':<12} {'Input Tok':<12} {'Output Tok':<12} {'Answer Len'}\")\n",
    "print(\"-\" * 60)\n",
    "for effort, data in results_by_effort.items():\n",
    "    tokens = data[\"tokens\"]\n",
    "    print(f\"{effort:<10} {data['time']:<12.2f} {tokens.get('inputTokens','?'):<12} \"\n",
    "          f\"{tokens.get('outputTokens','?'):<12} {data['answer_length']}\")\n",
    "\n",
    "print(\"\\nKey insight: Higher reasoning effort typically produces more thorough\")\n",
    "print(\"validation, catches more anomalies, and provides more detailed cross-referencing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new-md-4",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Built-in Code Interpreter\n",
    "\n",
    "Amazon Nova 2 includes a **built-in code interpreter** (`nova_code_interpreter`) that can generate and execute Python code in an isolated sandbox. This is powerful for IDP because after extracting data from documents, you can:\n",
    "\n",
    "- **Compute statistics** from financial data (totals, averages, trends)\n",
    "- **Validate calculations** (verify line items sum to totals)\n",
    "- **Generate visualizations** (charts, graphs from extracted data)\n",
    "- **Run data transformations** (normalize dates, currencies, formats)\n",
    "\n",
    "> **API**: `toolConfig={\"tools\": [{\"systemTool\": {\"name\": \"nova_code_interpreter\"}}]}`\n",
    "\n",
    "The code interpreter runs Python in a sandbox and returns `{stdOut, stdErr, exitCode, isError}`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new-md-5",
   "metadata": {},
   "source": [
    "### 10.1 Financial Analysis from Extracted Bank Statement Data\n",
    "\n",
    "We'll provide the previously extracted bank statement data to Nova with the code interpreter enabled, asking it to compute statistics and generate analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "new-code-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use code interpreter to analyze extracted bank statement data\n",
    "CI_PROMPT = \"\"\"I have extracted the following bank statement data. \n",
    "Please analyze it using Python code:\n",
    "\n",
    "1. Calculate total credits, total debits, and net change\n",
    "2. Find the largest and smallest transactions\n",
    "3. Group transactions by type and compute subtotals\n",
    "4. Identify any recurring transactions (similar descriptions)\n",
    "5. Print a clear summary table of the analysis\n",
    "\n",
    "Here is the extracted data (from a bank statement):\n",
    "\"\"\" + json.dumps(bank_data if bank_data else {\"note\": \"bank_data not available - use sample data\",\n",
    "    \"transactions\": [\n",
    "        {\"date\": \"2024-01-05\", \"description\": \"SALARY DEPOSIT\", \"amount\": 5000.00, \"type\": \"credit\"},\n",
    "        {\"date\": \"2024-01-07\", \"description\": \"RENT PAYMENT\", \"amount\": 1500.00, \"type\": \"debit\"},\n",
    "        {\"date\": \"2024-01-10\", \"description\": \"GROCERY STORE\", \"amount\": 125.50, \"type\": \"debit\"},\n",
    "        {\"date\": \"2024-01-12\", \"description\": \"UTILITY BILL\", \"amount\": 200.00, \"type\": \"debit\"},\n",
    "        {\"date\": \"2024-01-15\", \"description\": \"ONLINE TRANSFER IN\", \"amount\": 750.00, \"type\": \"credit\"},\n",
    "        {\"date\": \"2024-01-18\", \"description\": \"GROCERY STORE\", \"amount\": 98.75, \"type\": \"debit\"},\n",
    "        {\"date\": \"2024-01-20\", \"description\": \"INSURANCE PREMIUM\", \"amount\": 350.00, \"type\": \"debit\"},\n",
    "        {\"date\": \"2024-01-25\", \"description\": \"SALARY DEPOSIT\", \"amount\": 5000.00, \"type\": \"credit\"},\n",
    "        {\"date\": \"2024-01-28\", \"description\": \"UTILITY BILL\", \"amount\": 180.00, \"type\": \"debit\"},\n",
    "    ]\n",
    "}, indent=2)\n",
    "\n",
    "CI_SYSTEM = \"You are a financial analyst. Use the code interpreter to perform calculations and analysis. Show your work with Python code.\"\n",
    "\n",
    "print(\"Requesting code interpreter analysis of bank statement data...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": [{\"text\": CI_PROMPT}]}]\n",
    "resp = invoke_nova_with_code_interpreter(bedrock_client, messages, system_prompt=CI_SYSTEM)\n",
    "\n",
    "show_usage(resp)\n",
    "\n",
    "# Parse the response to find code and results\n",
    "content_list = resp[\"output\"][\"message\"][\"content\"] if resp else []\n",
    "for item in content_list:\n",
    "    if \"text\" in item:\n",
    "        print(\"\\n--- Model Response ---\")\n",
    "        print(item[\"text\"])\n",
    "    elif \"toolUse\" in item:\n",
    "        tu = item[\"toolUse\"]\n",
    "        print(f\"\\n--- Generated Python Code ---\")\n",
    "        print(tu.get(\"input\", {}).get(\"code\", \"\"))\n",
    "        print(f\"\\n(Tool Use ID: {tu.get('toolUseId', 'N/A')})\")\n",
    "    elif \"toolResult\" in item:\n",
    "        tr = item[\"toolResult\"]\n",
    "        print(f\"\\n--- Code Execution Result ---\")\n",
    "        for c in tr.get(\"content\", []):\n",
    "            if \"text\" in c:\n",
    "                try:\n",
    "                    result = json.loads(c[\"text\"])\n",
    "                    print(f\"Exit Code: {result.get('exitCode', 'N/A')}\")\n",
    "                    if result.get(\"stdOut\"):\n",
    "                        print(f\"Output:\\n{result['stdOut']}\")\n",
    "                    if result.get(\"stdErr\"):\n",
    "                        print(f\"Errors:\\n{result['stdErr']}\")\n",
    "                except json.JSONDecodeError:\n",
    "                    print(c[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new-md-7",
   "metadata": {},
   "source": [
    "### 10.2 Medical Claim Validation with Code Interpreter\n",
    "\n",
    "Use the code interpreter to validate extracted CMS-1500 data — checking that charges sum correctly, required fields are present, and codes are properly formatted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "new-code-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CI_VALIDATE_PROMPT = \"\"\"I have extracted the following CMS-1500 medical claim data.\n",
    "Use Python code to perform thorough validation:\n",
    "\n",
    "1. Verify that individual line item charges sum to total_charges\n",
    "2. Check that all required fields are present and non-empty\n",
    "3. Validate diagnosis code format (should be ICD-10 format like X##.## or X##)\n",
    "4. Validate procedure code format (should be 5-digit CPT codes)\n",
    "5. Check date formats are consistent\n",
    "6. Print a validation report with PASS/FAIL for each check\n",
    "\n",
    "Extracted CMS-1500 data:\n",
    "\"\"\" + json.dumps(cms_data if cms_data else {\"note\": \"using sample data\",\n",
    "    \"patient\": {\"name\": \"John Smith\", \"date_of_birth\": \"1985-03-15\", \"gender\": \"M\"},\n",
    "    \"diagnosis_codes\": [\"J06.9\", \"R05.9\"],\n",
    "    \"service_lines\": [\n",
    "        {\"procedure_code\": \"99213\", \"charges\": 150.00, \"units\": 1, \"date_from\": \"2024-01-15\"},\n",
    "        {\"procedure_code\": \"87081\", \"charges\": 25.00, \"units\": 1, \"date_from\": \"2024-01-15\"}\n",
    "    ],\n",
    "    \"total_charges\": 175.00\n",
    "}, indent=2)\n",
    "\n",
    "print(\"Requesting code interpreter validation of CMS-1500 data...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": [{\"text\": CI_VALIDATE_PROMPT}]}]\n",
    "resp = invoke_nova_with_code_interpreter(bedrock_client, messages,\n",
    "    system_prompt=\"You are a medical billing auditor. Validate claim data rigorously using Python code.\")\n",
    "\n",
    "show_usage(resp)\n",
    "\n",
    "content_list = resp[\"output\"][\"message\"][\"content\"] if resp else []\n",
    "for item in content_list:\n",
    "    if \"text\" in item:\n",
    "        print(\"\\n--- Model Response ---\")\n",
    "        print(item[\"text\"])\n",
    "    elif \"toolUse\" in item:\n",
    "        tu = item[\"toolUse\"]\n",
    "        print(f\"\\n--- Validation Code ---\")\n",
    "        print(tu.get(\"input\", {}).get(\"code\", \"\"))\n",
    "    elif \"toolResult\" in item:\n",
    "        tr = item[\"toolResult\"]\n",
    "        print(f\"\\n--- Validation Results ---\")\n",
    "        for c in tr.get(\"content\", []):\n",
    "            if \"text\" in c:\n",
    "                try:\n",
    "                    result = json.loads(c[\"text\"])\n",
    "                    if result.get(\"stdOut\"):\n",
    "                        print(result[\"stdOut\"])\n",
    "                    if result.get(\"stdErr\"):\n",
    "                        print(f\"Errors: {result['stdErr']}\")\n",
    "                except json.JSONDecodeError:\n",
    "                    print(c[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new-md-9",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. End-to-End IDP Pipeline\n",
    "\n",
    "This section combines all Nova 2 Lite capabilities into a realistic **end-to-end Intelligent Document Processing pipeline**:\n",
    "\n",
    "```\n",
    "┌─────────────┐     ┌──────────────┐     ┌─────────────────┐     ┌────────────────┐     ┌─────────────┐\n",
    "│  1. CLASSIFY│ ──▶ │  2. EXTRACT  │ ──▶ │  3. VALIDATE    │ ──▶ │  4. ANALYZE    │ ──▶ │  5. REPORT  │\n",
    "│  (Standard) │     │  (Blueprint) │     │  (Extended      │     │  (Code         │     │  (Standard) │\n",
    "│             │     │              │     │   Thinking)     │     │   Interpreter) │     │             │\n",
    "└─────────────┘     └──────────────┘     └─────────────────┘     └────────────────┘     └─────────────┘\n",
    "```\n",
    "\n",
    "Each stage uses a different Nova 2 Lite capability for its task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "new-code-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_idp_pipeline(file_path):\n",
    "    \"\"\"\n",
    "    Run the complete IDP pipeline on a single document.\n",
    "    \n",
    "    Steps:\n",
    "    1. Classify (standard mode - fast)\n",
    "    2. Extract (blueprint mode - structured)\n",
    "    3. Validate (Nova Lite + Extended Thinking - deep reasoning)\n",
    "    4. Analyze (Nova Lite + Code Interpreter - compute & verify)\n",
    "    5. Report (standard mode - comprehensive summary)\n",
    "    \"\"\"\n",
    "    pipeline_results = {\"file\": str(file_path), \"stages\": {}}\n",
    "    \n",
    "    print(f\"\\n{'#'*70}\")\n",
    "    print(f\"# IDP PIPELINE: {Path(file_path).name}\")\n",
    "    print(f\"{'#'*70}\")\n",
    "    \n",
    "    # ── STAGE 1: CLASSIFY ──\n",
    "    print(f\"\\n▶ STAGE 1: Document Classification (Nova 2 Lite)\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    resp = invoke_nova(bedrock_client,\n",
    "        prompt=CLASSIFY_PROMPT, file_paths=str(file_path),\n",
    "        model_id=NOVA_LITE_MODEL_ID, system_prompt=CLASSIFY_SYSTEM, max_tokens=512\n",
    "    )\n",
    "    txt = extract_text(resp)\n",
    "    show_usage(resp)\n",
    "    try:\n",
    "        clean = txt.strip()\n",
    "        if clean.startswith(\"```\"):\n",
    "            clean = clean.split(\"\\n\", 1)[1].rsplit(\"```\", 1)[0]\n",
    "        classification = json.loads(clean)\n",
    "    except json.JSONDecodeError:\n",
    "        classification = {\"document_type\": \"UNKNOWN\", \"raw\": txt[:200]}\n",
    "    \n",
    "    doc_type = classification.get(\"document_type\", \"UNKNOWN\")\n",
    "    print(f\"  Classified as: {doc_type} (confidence: {classification.get('confidence', '?')})\")\n",
    "    pipeline_results[\"stages\"][\"classify\"] = classification\n",
    "    \n",
    "    # ── STAGE 2: EXTRACT ──\n",
    "    print(f\"\\n▶ STAGE 2: Structured Extraction (Blueprint)\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    extract_prompt = f\"\"\"Extract ALL structured data from this {doc_type} document.\n",
    "Return a comprehensive JSON object with every field, value, date, amount, \n",
    "name, address, and identifier found in the document.\n",
    "Include a 'metadata' key with document_type, page_count estimate, and extraction_confidence.\"\"\"\n",
    "\n",
    "    resp = invoke_nova(bedrock_client,\n",
    "        prompt=extract_prompt, file_paths=str(file_path),\n",
    "        model_id=NOVA_LITE_MODEL_ID,\n",
    "        system_prompt=\"You are an expert document data extraction system. Return only valid JSON.\",\n",
    "        max_tokens=6000\n",
    "    )\n",
    "    txt = extract_text(resp)\n",
    "    show_usage(resp)\n",
    "    try:\n",
    "        clean = txt.strip()\n",
    "        if clean.startswith(\"```\"):\n",
    "            clean = clean.split(\"\\n\", 1)[1].rsplit(\"```\", 1)[0]\n",
    "        extracted = json.loads(clean)\n",
    "    except json.JSONDecodeError:\n",
    "        extracted = {\"raw_extraction\": txt[:1000]}\n",
    "    \n",
    "    print(f\"  Extracted {len(json.dumps(extracted))} chars of structured data\")\n",
    "    pipeline_results[\"stages\"][\"extract\"] = extracted\n",
    "    \n",
    "    # ── STAGE 3: VALIDATE ──\n",
    "    print(f\"\\n▶ STAGE 3: Deep Validation (Nova 2 Lite + Extended Thinking)\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    validate_prompt = f\"\"\"You are a document validation expert. Carefully validate this extracted data:\n",
    "\n",
    "{json.dumps(extracted, indent=2, default=str)[:8000]}\n",
    "\n",
    "Perform these validation checks:\n",
    "1. COMPLETENESS: Are all expected fields present for a {doc_type}?\n",
    "2. CONSISTENCY: Do numeric values add up? Are dates logical?\n",
    "3. FORMAT: Are IDs, codes, and references in expected formats?\n",
    "4. ANOMALIES: Anything unusual or potentially incorrect?\n",
    "\n",
    "Return a JSON object with: \n",
    "{{\"validation_status\": \"PASS|WARN|FAIL\", \"checks\": [...], \"issues_found\": [...], \"confidence_score\": 0-100}}\"\"\"\n",
    "\n",
    "    resp = invoke_nova_with_reasoning(bedrock_client,\n",
    "        prompt=validate_prompt,\n",
    "        reasoning_effort=\"medium\",\n",
    "        system_prompt=\"You are a meticulous document auditor. Validate data thoroughly. Return valid JSON only.\"\n",
    "    )\n",
    "    reasoning, answer = extract_reasoning_and_text(resp)\n",
    "    show_usage(resp)\n",
    "    try:\n",
    "        clean = answer.strip()\n",
    "        if clean.startswith(\"```\"):\n",
    "            clean = clean.split(\"\\n\", 1)[1].rsplit(\"```\", 1)[0]\n",
    "        validation = json.loads(clean)\n",
    "    except json.JSONDecodeError:\n",
    "        validation = {\"validation_status\": \"UNKNOWN\", \"raw\": answer[:500]}\n",
    "    \n",
    "    print(f\"  Validation status: {validation.get('validation_status', '?')}\")\n",
    "    print(f\"  Confidence score: {validation.get('confidence_score', '?')}\")\n",
    "    issues = validation.get(\"issues_found\", [])\n",
    "    if issues:\n",
    "        print(f\"  Issues found: {len(issues)}\")\n",
    "        for issue in issues[:3]:\n",
    "            print(f\"    - {issue if isinstance(issue, str) else json.dumps(issue)[:100]}\")\n",
    "    pipeline_results[\"stages\"][\"validate\"] = validation\n",
    "    \n",
    "    # ── STAGE 4: ANALYZE ──\n",
    "    print(f\"\\n▶ STAGE 4: Computational Analysis (Code Interpreter)\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    analyze_prompt = f\"\"\"Analyze this extracted document data using Python code:\n",
    "\n",
    "{json.dumps(extracted, indent=2, default=str)[:6000]}\n",
    "\n",
    "Write Python code to:\n",
    "1. Count and categorize all fields found\n",
    "2. Sum any monetary amounts\n",
    "3. List all dates found and check chronological order\n",
    "4. Print a structured analysis summary\"\"\"\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": [{\"text\": analyze_prompt}]}]\n",
    "    resp = invoke_nova_with_code_interpreter(bedrock_client, messages,\n",
    "        system_prompt=\"You are a data analyst. Use Python to analyze document data.\")\n",
    "    show_usage(resp)\n",
    "    \n",
    "    analysis_output = \"\"\n",
    "    content_list = resp[\"output\"][\"message\"][\"content\"] if resp else []\n",
    "    for item in content_list:\n",
    "        if \"text\" in item:\n",
    "            analysis_output += item[\"text\"]\n",
    "        elif \"toolResult\" in item:\n",
    "            for c in item[\"toolResult\"].get(\"content\", []):\n",
    "                if \"text\" in c:\n",
    "                    try:\n",
    "                        r = json.loads(c[\"text\"])\n",
    "                        if r.get(\"stdOut\"):\n",
    "                            analysis_output += r[\"stdOut\"]\n",
    "                    except json.JSONDecodeError:\n",
    "                        analysis_output += c[\"text\"]\n",
    "    \n",
    "    print(f\"  Analysis output ({len(analysis_output)} chars):\")\n",
    "    print(analysis_output[:800] if analysis_output else \"  [No output]\")\n",
    "    pipeline_results[\"stages\"][\"analyze\"] = {\"output\": analysis_output[:2000]}\n",
    "    \n",
    "    # ── STAGE 5: REPORT ──\n",
    "    print(f\"\\n▶ STAGE 5: Final Report Generation\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    report_prompt = f\"\"\"Generate a comprehensive processing report for this document.\n",
    "\n",
    "Classification: {json.dumps(classification, default=str)}\n",
    "Extraction summary: {len(json.dumps(extracted))} chars extracted\n",
    "Validation: {json.dumps(validation, default=str)[:2000]}\n",
    "\n",
    "Create a concise executive summary covering:\n",
    "1. Document type and key identifiers\n",
    "2. Critical data points extracted\n",
    "3. Validation status and any concerns\n",
    "4. Recommended next steps\n",
    "\n",
    "Format as clear markdown.\"\"\"\n",
    "\n",
    "    resp = invoke_nova(bedrock_client,\n",
    "        prompt=report_prompt,\n",
    "        model_id=NOVA_LITE_MODEL_ID,\n",
    "        system_prompt=\"You are a document processing report writer. Be concise and actionable.\",\n",
    "        max_tokens=2000\n",
    "    )\n",
    "    report = extract_text(resp)\n",
    "    show_usage(resp)\n",
    "    print(report[:1000])\n",
    "    pipeline_results[\"stages\"][\"report\"] = report\n",
    "    \n",
    "    return pipeline_results\n",
    "\n",
    "print(\"IDP Pipeline function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new-md-11",
   "metadata": {},
   "source": [
    "### 11.1 Run Pipeline on Sample Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "new-code-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the full pipeline on two different document types\n",
    "pipeline_outputs = {}\n",
    "\n",
    "# Pipeline on CMS-1500 (complex medical form)\n",
    "result = run_idp_pipeline(f\"{SAMPLES_DIR}/sample1_cms-1500-P.pdf\")\n",
    "pipeline_outputs[\"cms1500\"] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "new-code-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline on bank statement\n",
    "result = run_idp_pipeline(f\"{SAMPLES_DIR}/BankStatement.pdf\")\n",
    "pipeline_outputs[\"bank_statement\"] = result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new-md-15",
   "metadata": {},
   "source": [
    "### 11.2 Pipeline Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "new-code-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"IDP PIPELINE - COMPLETE SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nDocuments processed: {len(pipeline_outputs)}\")\n",
    "print()\n",
    "\n",
    "for doc_name, result in pipeline_outputs.items():\n",
    "    stages = result.get(\"stages\", {})\n",
    "    classify = stages.get(\"classify\", {})\n",
    "    validate = stages.get(\"validate\", {})\n",
    "    print(f\"📄 {result.get('file', doc_name)}\")\n",
    "    print(f\"   Type: {classify.get('document_type', '?')}\")\n",
    "    print(f\"   Confidence: {classify.get('confidence', '?')}\")\n",
    "    print(f\"   Validation: {validate.get('validation_status', '?')} \"\n",
    "          f\"(score: {validate.get('confidence_score', '?')})\")\n",
    "    issues = validate.get(\"issues_found\", [])\n",
    "    print(f\"   Issues: {len(issues)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-37",
   "metadata": {},
   "source": [
    "---\n",
    "## 13. Conclusion\n",
    "\n",
    "This notebook demonstrated the art of the possible with **Amazon Nova 2 Lite** for Intelligent Document Processing:\n",
    "\n",
    "| Capability | Implementation |\n",
    "|---|---|\n",
    "| **Document Classification** | Multimodal classification with JSON output |\n",
    "| **Summarization** | Rich markdown summaries from PDFs and images |\n",
    "| **Structured Extraction** | Tool configs (blueprints) forcing schema-compliant JSON |\n",
    "| **Bounding Box Visualization** | Spatial field localization with image overlay |\n",
    "| **Mode Comparison** | Standard vs Extended Thinking quality/speed analysis |\n",
    "| **Batch Processing** | Multi-document extraction with aggregate insights |\n",
    "| **Document Q&A** | Multi-turn conversation and document comparison |\n",
    "| **Extended Thinking** | 3-level reasoning for deep validation & anomaly detection |\n",
    "| **Code Interpreter** | Built-in Python sandbox for computation & validation |\n",
    "| **End-to-End Pipeline** | Classify → Extract → Validate → Analyze → Report |\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Nova 2 Lite** provides an excellent balance of capability and cost-efficiency for IDP workloads\n",
    "2. **Tool configurations** (Converse API) act as blueprints for enforcing structured output schemas\n",
    "3. **Extended thinking** enables quality/cost tradeoff control via low/medium/high reasoning effort levels\n",
    "4. **Built-in code interpreter** allows in-flight computation and validation without external infrastructure\n",
    "5. **Multimodal understanding** handles PDFs, scanned images, and multi-page documents seamlessly\n",
    "6. **Multi-turn conversation** enables interactive document investigation and cross-document comparison\n",
    "7. **End-to-end pipelines** combining multiple Nova capabilities deliver production-grade IDP workflows\n",
    "\n",
    "### What's Possible Beyond This Notebook\n",
    "\n",
    "- **Web Grounding** (`nova_grounding`) — Enrich extracted data with real-time web information (e.g., look up medical codes, verify company details)\n",
    "- **Fine-tuning** — Customize Nova 2 Lite via SFT/RFT on Amazon Bedrock for domain-specific extraction\n",
    "- **Batch Processing at Scale** — Integrate with Amazon S3 and AWS Step Functions for production pipelines\n",
    "- **Content Safety** — Add [Amazon Bedrock Guardrails](https://docs.aws.amazon.com/bedrock/latest/userguide/guardrails.html) for PII detection and content filtering\n",
    "- **Multimodal Embeddings** — Use Nova Multimodal Embeddings for semantic document search and retrieval\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [Amazon Nova Documentation](https://docs.aws.amazon.com/nova/latest/nova2-userguide/what-is-nova-2.html)\n",
    "- [Amazon Bedrock Converse API](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html)\n",
    "- [Extended Thinking Guide](https://docs.aws.amazon.com/nova/latest/nova2-userguide/extended-thinking.html)\n",
    "- [Built-in Tools (Code Interpreter & Web Grounding)](https://docs.aws.amazon.com/nova/latest/userguide/tool-built-in.html)\n",
    "- [Amazon Nova Samples on GitHub](https://github.com/aws-samples/amazon-nova-samples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
