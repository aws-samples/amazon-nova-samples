{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48191036-4ebd-4259-86e9-0144363d1cee",
   "metadata": {},
   "source": [
    "# Video Analysis with Amazon Nova 2 Lite\n",
    "\n",
    "In this notebook, we demonstrate how to use Amazon Nova 2 Lite for video understanding using the Converse API.\n",
    "\n",
    "## Use Case Description\n",
    "\n",
    "We will use Nova 2 Lite to analyze a video, exercising the video understanding capabilities documented in the [Nova 2 multimodal prompting guide](https://docs.aws.amazon.com/nova/latest/nova2-userguide/prompting-multimodal.html):\n",
    "\n",
    "- **Video Summarization:** Extracting a concise summary of the video content\n",
    "- **Dense Captioning:** Generating detailed, scene-by-scene descriptions\n",
    "- **Security Footage Analysis:** Detecting events in camera footage\n",
    "- **Timestamp Extraction:** Localizing events with start/end times as structured JSON\n",
    "- **Video Classification:** Categorizing the video from a predefined class list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb287a1b-0020-4ea4-af35-cfad70572c2b",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f7d97f-689e-4f8d-bc98-bcd81f790038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import uuid\n",
    "import base64\n",
    "import urllib.request\n",
    "from IPython.display import Video, Markdown, display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0680137a-dcd7-4abc-9af3-86d8a1ea4dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r MODEL_ID\n",
    "%store -r region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886ec430-65c0-47bc-b84f-14f53e87e74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock = boto3.client(\"bedrock-runtime\", region_name=region_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87d5e90-0ddd-4e12-a63d-63b320ce1f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the video and load bytes — reused across all exercises\n",
    "video_url = \"https://ws-assets-prod-iad-r-iad-ed304a55c2ca1aee.s3.us-east-1.amazonaws.com/8082573f-f39e-4e39-a48f-f3562cc6e597/aws-ads-rainy-day.mp4\"\n",
    "video_path = \"video/aws-ads-rainy-day.mp4\"\n",
    "\n",
    "os.makedirs(\"video\", exist_ok=True)\n",
    "if not os.path.exists(video_path):\n",
    "    print(\"Downloading video...\")\n",
    "    urllib.request.urlretrieve(video_url, video_path)\n",
    "    print(\"Download complete.\")\n",
    "\n",
    "with open(video_path, \"rb\") as f:\n",
    "    video_bytes = f.read()\n",
    "\n",
    "print(f\"Video loaded: {len(video_bytes) / (1024*1024):.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e61a338-87ea-4e06-9236-2cdb463086b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Video(video_path, embed=True, width=800, height=450,\n",
    "              html_attributes=\"controls autoplay loop\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7eef93c-2101-4519-914b-e8208cd855dc",
   "metadata": {},
   "source": [
    "## 1. Video Summarization\n",
    "\n",
    "Nova 2 can generate summaries of video content. Per the best practices guide:\n",
    "- Set `temperature: 0`\n",
    "- Place the user text prompt after the video content\n",
    "- Clearly specify the aspects of the video you care about in the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a01c28f-843b-4bb4-bb83-fb98db63eb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock.converse(\n",
    "    modelId=MODEL_ID,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"video\": {\"format\": \"mp4\", \"source\": {\"bytes\": video_bytes}}},\n",
    "                {\"text\": \"Could you provide a summary of the video, focusing on its key points?\"}\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    inferenceConfig={\"maxTokens\": 512, \"temperature\": 0}\n",
    ")\n",
    "\n",
    "display(Markdown(response[\"output\"][\"message\"][\"content\"][0][\"text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822bbfd8-8e87-427f-9b82-dbf0ceea2ba5",
   "metadata": {},
   "source": [
    "## 2. Dense Captioning\n",
    "\n",
    "Dense captioning generates detailed, scene-by-scene descriptions of the video. This is useful for creating searchable metadata or accessibility descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf8470c-4a0a-4abc-9a01-d2525c87a480",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock.converse(\n",
    "    modelId=MODEL_ID,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"video\": {\"format\": \"mp4\", \"source\": {\"bytes\": video_bytes}}},\n",
    "                {\"text\": \"Describe the video scene-by-scene, including details about characters, actions and settings.\"}\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    inferenceConfig={\"maxTokens\": 1024, \"temperature\": 0}\n",
    ")\n",
    "\n",
    "display(Markdown(response[\"output\"][\"message\"][\"content\"][0][\"text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ae6509-417d-477b-abc8-56c5d8b48340",
   "metadata": {},
   "source": [
    "## 3. Timestamp Extraction\n",
    "\n",
    "Nova 2 can identify timestamps related to events in a video. Following the best practices for structured extraction, we pass a JSON schema in the user prompt and set `temperature: 0`. The first cell extracts key moments as structured JSON, and the second localizes a specific event using the recommended prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf678a4b-5148-4399-b663-331ea75b7a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"description\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"A high-level summary of the entire video content.\"\n",
    "        },\n",
    "        \"key_moments\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"description\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"A detailed description of what happens in this moment.\"\n",
    "                    },\n",
    "                    \"start_time\": {\n",
    "                        \"type\": \"number\",\n",
    "                        \"description\": \"Start time of the moment in seconds.\"\n",
    "                    },\n",
    "                    \"end_time\": {\n",
    "                        \"type\": \"number\",\n",
    "                        \"description\": \"End time of the moment in seconds.\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"description\", \"start_time\", \"end_time\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"description\", \"key_moments\"]\n",
    "}\n",
    "\n",
    "prompt = f\"\"\"Segment this video into key moments with timestamps.\n",
    "\n",
    "Extract information in JSON format according to the given schema.\n",
    "\n",
    "Follow these guidelines:\n",
    "- Ensure that every field is populated.\n",
    "- Provide start_time and end_time in seconds.\n",
    "- The description field should be a detailed account of what happens in each moment.\n",
    "- Do not make up events not present in the video.\n",
    "\n",
    "JSON Schema:\n",
    "{json.dumps(json_schema, indent=2)}\"\"\"\n",
    "\n",
    "response = bedrock.converse(\n",
    "    modelId=MODEL_ID,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"video\": {\"format\": \"mp4\", \"source\": {\"bytes\": video_bytes}}},\n",
    "                {\"text\": prompt}\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    inferenceConfig={\"maxTokens\": 2048, \"temperature\": 0}\n",
    ")\n",
    "\n",
    "result_text = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "\n",
    "# Strip markdown code fences (```json ... ```) if present\n",
    "cleaned = result_text.strip()\n",
    "if cleaned.startswith(\"```\"):\n",
    "    cleaned = cleaned.split(\"\\n\", 1)[1]\n",
    "if cleaned.endswith(\"```\"):\n",
    "    cleaned = cleaned.rsplit(\"```\", 1)[0]\n",
    "\n",
    "result_json = json.loads(cleaned.strip())\n",
    "print(json.dumps(result_json, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3beda1af-89eb-4039-b8bf-dc2ed831d177",
   "metadata": {},
   "source": [
    "### 3.a. Event Locolization with Timestamp extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7efa65-7d1d-4b36-8b8d-3abe1881e005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Localize a specific event and display interactive video clips\n",
    "event_description = \"Watching tablet\"\n",
    "\n",
    "response = bedrock.converse(\n",
    "    modelId=MODEL_ID,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"video\": {\"format\": \"mp4\", \"source\": {\"bytes\": video_bytes}}},\n",
    "                {\"text\": f'Please localize the moment that the event \"{event_description}\" happens in the video. Answer with the starting and ending time of the event in seconds, such as [[72, 82]]. If the event happens multiple times, list all of them, such as [[40, 50], [72, 82]].'}\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    inferenceConfig={\"maxTokens\": 512, \"temperature\": 0}\n",
    ")\n",
    "\n",
    "result_text = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "print(f\"Raw response: {result_text}\")\n",
    "\n",
    "# Parse [[start, end], ...] timestamps from the response\n",
    "timestamps = [\n",
    "    (float(parts[0].strip()), float(parts[1].strip()))\n",
    "    for match in re.findall(r'\\[([^\\[\\]]*?)\\]', result_text)\n",
    "    if len(parts := match.split(',')) == 2\n",
    "]\n",
    "\n",
    "print(f\"Detected {len(timestamps)} clip(s) for: {event_description}\")\n",
    "\n",
    "# Build an inline video player with jump-to buttons using base64 embedded video\n",
    "video_b64 = base64.b64encode(video_bytes).decode('utf-8')\n",
    "video_id = f\"videoPlayer{uuid.uuid4().hex[:6]}\"\n",
    "\n",
    "buttons_html = ''.join([\n",
    "    f'<button onclick=\"jumpTo{video_id}({start})\" style=\"margin:4px;padding:6px 12px;cursor:pointer;\">'\n",
    "    f'{start}s - {end}s</button>'\n",
    "    for start, end in timestamps\n",
    "])\n",
    "\n",
    "html = f\"\"\"\n",
    "<video id=\"{video_id}\" width=\"640\" controls muted>\n",
    "  <source src=\"data:video/mp4;base64,{video_b64}\" type=\"video/mp4\">\n",
    "</video>\n",
    "<div style=\"margin-top:10px;\">\n",
    "  <strong>Jump to \\\"{event_description}\\\":</strong><br>\n",
    "  {buttons_html}\n",
    "</div>\n",
    "<script>\n",
    "  function jumpTo{video_id}(time) {{\n",
    "    var video = document.getElementById('{video_id}');\n",
    "    video.currentTime = time;\n",
    "    video.play();\n",
    "  }}\n",
    "</script>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31f9913-b8d2-4e71-9b1f-7fe4044ae630",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this notebook, we demonstrated Amazon Nova 2 Lite's video understanding capabilities using the Converse API:\n",
    "\n",
    "1. **Summarization** — concise overview of video content\n",
    "2. **Dense Captioning** — detailed scene-by-scene descriptions\n",
    "3. **Timestamp Extraction** — structured JSON output with key moments and event localization\n",
    "\n",
    "All exercises follow the [Nova 2 multimodal prompting best practices](https://docs.aws.amazon.com/nova/latest/nova2-userguide/prompting-multimodal.html): `temperature: 0`, text prompt after video content, and task instructions in the user prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6411b31a-9862-4735-ac77-ce2d64641e41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
