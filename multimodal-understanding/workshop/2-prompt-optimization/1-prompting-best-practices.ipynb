{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Prompting Best Practices with Amazon Nova Models\n",
    "\n",
    "The effectiveness of prompts depends greatly on the quality of information provided and the craftsmanship of the prompt itself. Effective prompts may include instructions, questions, contextual details, inputs, and examples to guide the model and enhance result quality. \n",
    "\n",
    "This notebook explores strategies and techniques for optimizing the performance of Amazon Nova models. The methods presented can be combined in various ways to maximize their effectiveness. We encourage experimentation to identify approaches best suited to your specific needs.\n",
    "\n",
    "For more comprehensive guidance, refer to the [Amazon Nova prompting documentation](https://docs.aws.amazon.com/nova/latest/userguide/prompting-text-understanding.html).\n",
    "\n",
    "## Before Starting Prompt Engineering\n",
    "\n",
    "Before diving into prompt engineering, it's recommended to establish:\n",
    "\n",
    "1. **Define Your Use Case** along four key dimensions:\n",
    "   - **Task**: What specific outcome do you want to achieve?\n",
    "   - **Role**: What persona should the model adopt?\n",
    "   - **Response Style**: What format or structure should the output follow?\n",
    "   - **Instructions**: What guidelines should the model adhere to?\n",
    "\n",
    "2. **Success Criteria**: Clearly define what constitutes a successful response. This might include:\n",
    "   - Qualitative measures (format, factuality, faithfulness)\n",
    "   - Quantitative metrics (length requirements, BLEU/Rouge scores)\n",
    "   - Specific output requirements\n",
    "\n",
    "3. **Draft Prompt**: Create an initial prompt to begin the iterative optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in variables and config from previous notebook\n",
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "import base64\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "# Set up the boto3 client with the proper region\n",
    "boto3.setup_default_session(region_name=region_name)\n",
    "client = boto3.client(\"bedrock-runtime\")\n",
    "\n",
    "\n",
    "def call_nova(\n",
    "    model,\n",
    "    messages,\n",
    "    system_message=\"\",\n",
    "    streaming=False,\n",
    "    max_tokens=512,\n",
    "    temp=0,\n",
    "    top_p=0.1,\n",
    "    top_k=1,\n",
    "    tools=None,\n",
    "    stop_sequences=[],\n",
    "    verbose=False,\n",
    "):\n",
    "    \"\"\"Call Amazon Nova models with various parameters.\n",
    "    \n",
    "    Args:\n",
    "        model (str): The model ID to use\n",
    "        messages (list): List of message objects with role and content\n",
    "        system_message (str, optional): System prompt. Defaults to \"\".\n",
    "        streaming (bool, optional): Whether to use streaming API. Defaults to False.\n",
    "        max_tokens (int, optional): Maximum tokens to generate. Defaults to 512.\n",
    "        temp (float, optional): Temperature parameter. Defaults to 0.\n",
    "        top_p (float, optional): Top-p parameter. Defaults to 0.1.\n",
    "        top_k (int, optional): Top-k parameter. Defaults to 1.\n",
    "        tools (list, optional): List of tool specifications. Defaults to None.\n",
    "        stop_sequences (list, optional): List of stop sequences. Defaults to [].\n",
    "        verbose (bool, optional): Whether to print request body. Defaults to False.\n",
    "        \n",
    "    Returns:\n",
    "        tuple or stream: Model response and content text if not streaming, else stream\n",
    "    \"\"\"\n",
    "    # Prepare system prompt\n",
    "    system_list = [{\"text\": system_message}]\n",
    "    \n",
    "    # Prepare inference parameters\n",
    "    inf_params = {\n",
    "        \"max_new_tokens\": max_tokens,\n",
    "        \"top_p\": top_p,\n",
    "        \"top_k\": top_k,\n",
    "        \"temperature\": temp,\n",
    "        \"stopSequences\": stop_sequences,\n",
    "    }\n",
    "    \n",
    "    # Build request body\n",
    "    request_body = {\n",
    "        \"messages\": messages,\n",
    "        \"system\": system_list,\n",
    "        \"inferenceConfig\": inf_params,\n",
    "    }\n",
    "    \n",
    "    # Add tool configuration if provided\n",
    "    if tools is not None:\n",
    "        tool_config = []\n",
    "        for tool in tools:\n",
    "            tool_config.append({\"toolSpec\": tool})\n",
    "        request_body[\"toolConfig\"] = {\"tools\": tool_config}\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Request Body\", request_body)\n",
    "    \n",
    "    if not streaming:\n",
    "        # Use synchronous API\n",
    "        response = client.invoke_model(modelId=model, body=json.dumps(request_body))\n",
    "        model_response = json.loads(response[\"body\"].read())\n",
    "        return model_response, model_response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "    else:\n",
    "        # Use streaming API\n",
    "        response = client.invoke_model_with_response_stream(\n",
    "            modelId=model, body=json.dumps(request_body)\n",
    "        )\n",
    "        return response[\"body\"]\n",
    "\n",
    "\n",
    "def get_base64_encoded_value(media_path):\n",
    "    \"\"\"Convert media file to base64 encoded string.\n",
    "    \n",
    "    Args:\n",
    "        media_path (str): Path to the media file\n",
    "        \n",
    "    Returns:\n",
    "        str: Base64 encoded string\n",
    "    \"\"\"\n",
    "    with open(media_path, \"rb\") as media_file:\n",
    "        binary_data = media_file.read()\n",
    "        base_64_encoded_data = base64.b64encode(binary_data)\n",
    "        base64_string = base_64_encoded_data.decode(\"utf-8\")\n",
    "        return base64_string\n",
    "\n",
    "\n",
    "def print_output(content_text):\n",
    "    \"\"\"Display model output as Markdown.\n",
    "    \n",
    "    Args:\n",
    "        content_text (str): Text to display\n",
    "    \"\"\"\n",
    "    display(Markdown(content_text))\n",
    "\n",
    "\n",
    "def validate_json(json_string):\n",
    "    \"\"\"Validates if a string is properly formatted JSON.\n",
    "    \n",
    "    Args:\n",
    "        json_string (str): String to validate as JSON\n",
    "        \n",
    "    Returns:\n",
    "        dict or None: Parsed JSON object if valid, None if invalid\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Attempt to parse the JSON string\n",
    "        parsed_json = json.loads(json_string)\n",
    "        \n",
    "        # If successful, return the parsed JSON\n",
    "        print(\"Valid JSON\")\n",
    "        return parsed_json\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        # If parsing fails, print an error message\n",
    "        print(f\"Invalid JSON: {e}\")\n",
    "        \n",
    "        # Print the location of the error\n",
    "        print(f\"Error at line {e.lineno}, column {e.colno}\")\n",
    "        \n",
    "        # Return None to indicate failure\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Structured Outputs\n",
    "\n",
    "For many applications, it's crucial to ensure that model responses follow a specific output format that works seamlessly with downstream processing. This is particularly important for automated workflows where inputs and outputs must adhere to strict formatting requirements.\n",
    "\n",
    "### Techniques for Generating Structured Output\n",
    "\n",
    "1. **Output Schema Specification**: Explicitly define the expected output structure in your prompt\n",
    "2. **Elimination of Preambles**: Instruct the model to output only the required format without explanatory text\n",
    "3. **Response Prefilling**: Guide the model's initial response format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Using Response Prefilling\n",
    "\n",
    "A particularly effective technique is to \"nudge\" the model's response by prefilling the assistant's content. This approach allows you to:\n",
    "\n",
    "1. **Direct the Model's Behavior**: Put words in the model's mouth to guide its response\n",
    "2. **Skip Unwanted Preamble**: Bypass explanatory text that might precede the desired output\n",
    "3. **Enforce Output Format**: Ensure specific formats like JSON or XML\n",
    "\n",
    "For example, prefilling with `{` or ``\\`json` can guide the model to immediately begin generating JSON without explanatory text.\n",
    "\n",
    "**Pro Tip**: When specifically extracting JSON, a common pattern is to prefill with ``\\`json` and add a stop sequence on ``\\`` to ensure the output is programmatically parseable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an unoptimized prompt for getting camera information\n",
    "unoptimized_prompt = \"\"\"Provide details about the best selling full-frame cameras in past three years.\n",
    "Answer in JSON format with keys like name, brand, price and a summary.\n",
    "\"\"\"\n",
    "\n",
    "# Create a messages object for the model\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": [{\"text\": unoptimized_prompt}]},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_response, content_text = call_nova(LITE_MODEL_ID, messages)\n",
    "print(\"\\n[Response Content Text]\")\n",
    "print(\"-\" * 40)\n",
    "print(content_text)\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Improving Schema Definition with Data Types and Prefill\n",
    "\n",
    "Let's enhance our approach by:\n",
    "1. Adding explicit schema definitions with correct data types\n",
    "2. Using response prefilling to ensure proper JSON formatting\n",
    "3. Adding a stop sequence to control output length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an optimized prompt with explicit schema definition\n",
    "optimized_prompt = \"\"\"Provide 5 examples of the best selling full-frame cameras in past three years.\n",
    "Follow the Output Schema as described below:\n",
    "Output Schema:\n",
    "{\n",
    "\"name\" : <string, the name of product>,\n",
    "\"brand\" : <string, the name of product>,\n",
    "\"price\" : <integer price>,\n",
    "\"summary\": <string, the product summary>\n",
    "}\n",
    "Only Respond in Valid JSON, without Markdown\n",
    "\"\"\"\n",
    "\n",
    "# Create a messages object with prefilled assistant response to guide the model\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": [{\"text\": optimized_prompt}]},\n",
    "    {\"role\": \"assistant\", \"content\": [{\"text\": \"```json\"}]},\n",
    "]\n",
    "\n",
    "# Call the model with a stop sequence to ensure proper JSON formatting\n",
    "model_response, content_text = call_nova(LITE_MODEL_ID, messages, stop_sequences=[\"]\"])\n",
    "print(\"\\n[Response Content Text]\")\n",
    "print(\"-\" * 40)\n",
    "print(content_text)\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Validate that the response is proper JSON\n",
    "print(\"Testing valid JSON:\")\n",
    "parsed_json = validate_json(content_text)\n",
    "if parsed_json:\n",
    "    print(parsed_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Few-Shot Examples\n",
    "\n",
    "Including examples in your prompt can significantly improve the model's ability to generate responses aligned with your expectations. This technique, known as \"few-shot prompting,\" helps the model understand the desired output format, style, and reasoning pattern.\n",
    "\n",
    "### Benefits of Few-Shot Examples\n",
    "\n",
    "- **Consistent Responses**: Creates uniformity in style and format\n",
    "- **Enhanced Performance**: Reduces misinterpretation of instructions\n",
    "- **Reduced Hallucinations**: Provides concrete guidance for outputs\n",
    "\n",
    "### Characteristics of Effective Examples\n",
    "\n",
    "1. **Diversity**: Include a range of examples that represent various use cases (from common to edge cases)\n",
    "2. **Complexity Alignment**: Match the complexity of examples to the target task\n",
    "3. **Relevance**: Ensure examples directly relate to the problem at hand\n",
    "\n",
    "**Advanced Tip**: For dynamic applications, consider implementing a RAG-based system that selects relevant examples based on similarity to the current query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a zero-shot prompt for sentiment classification\n",
    "no_shot = \"\"\"Your task is to Classify the following texts into the appropriate sentiment classes. The categories to classify are:\n",
    "\n",
    "Sentiment Classes:\n",
    "- Positive\n",
    "- Negative\n",
    "- Neutral\n",
    "\n",
    "Query:\n",
    "Input: The movie makes users think about their lives with the teenagers while still making audience unclear on the storyline.\n",
    "\"\"\"\n",
    "\n",
    "# Create a messages object\n",
    "messages = [{\"role\": \"user\", \"content\": [{\"text\": no_shot}]}]\n",
    "\n",
    "# Call the model with the zero-shot prompt\n",
    "model_response, content_text = call_nova(LITE_MODEL_ID, messages)\n",
    "print(\"\\n[Response Content Text]\")\n",
    "print(\"-\" * 40)\n",
    "print_output(content_text)\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Zero-Shot Sentiment Classification\n",
    "\n",
    "Let's start with a simple sentiment classification task without providing examples. Notice how the model responds without clear guidance on output format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a few-shot prompt with 4 examples for sentiment classification\n",
    "four_shot = \"\"\"Your task is to Classify the following texts into the appropriate sentiment classes. The categories to classify are:\n",
    "\n",
    "Sentiment Classes:\n",
    "- Positive\n",
    "- Negative\n",
    "- Neutral\n",
    "\n",
    "Please refer to some examples mentioned below.\n",
    "\n",
    "## Examples\n",
    "### Example 1\n",
    "Input: The movie was crazy good! I loved it\n",
    "Output: Positive\n",
    "Explaination: The text said \"good\" and \"loved\" so its positive\n",
    "\n",
    "### Example 2\n",
    "Input: The movie was scary and I got scared!\n",
    "Output: Neutral\n",
    "Explaination: The text said \"scary\" and \"scared\" which can be both positive and negative depending on people who like scary movies or one who hate\n",
    "\n",
    "### Example 3\n",
    "Input: The movie was pathetic not worth the time or money!\n",
    "Output: Negative\n",
    "Explaination: The text said \"pathetic\" and \"not worth\" which is negative sentiment\n",
    "\n",
    "### Example 4\n",
    "Input: The movie had some plots which were interesting and great while there were some gaps which needed more drama!\n",
    "Output: Neutral\n",
    "Explaination: The text said \"interesting and great\" and \"some gaps\" making it a mixed opinion hence neutral\n",
    "\n",
    "Query:\n",
    "Input: The movie makes users think about their lives with the teenagers while still making audience unclear on the storyline.\n",
    "\"\"\"\n",
    "\n",
    "# Create a messages object\n",
    "messages = [{\"role\": \"user\", \"content\": [{\"text\": four_shot}]}]\n",
    "\n",
    "# Call the model with the few-shot prompt\n",
    "model_response, content_text = call_nova(LITE_MODEL_ID, messages)\n",
    "print(\"\\n[Response Content Text]\")\n",
    "print(\"-\" * 40)\n",
    "print_output(content_text)\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a prompt without Chain of Thought (CoT) reasoning\n",
    "no_cot = \"\"\"You are a project manager for a small software development team tasked with launching a new app feature.\n",
    "You want to streamline the development process and ensure timely delivery. Draft a project plan\n",
    "\"\"\"\n",
    "\n",
    "# Create a messages object\n",
    "messages = [{\"role\": \"user\", \"content\": [{\"text\": no_cot}]}]\n",
    "\n",
    "# Call the model with the no-CoT prompt\n",
    "model_response, content_text = call_nova(LITE_MODEL_ID, messages, max_tokens=1024)\n",
    "print(\"\\n[Response Content Text]\")\n",
    "print(\"-\" * 40)\n",
    "print_output(content_text)\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Chain of Thought (CoT) Reasoning\n",
    "\n",
    "Chain of Thought prompting is a technique that encourages the model to work through a problem step-by-step, showing its reasoning process before arriving at a conclusion. This approach often leads to more accurate results, especially for complex tasks requiring multi-step reasoning.\n",
    "\n",
    "Let's first examine a response without explicit CoT guidance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a prompt with guided Chain of Thought (CoT) reasoning\n",
    "guided_cot = \"\"\"You are a project manager for a small software development team tasked with launching a new app feature.\n",
    "You want to streamline the development process and ensure timely delivery.\n",
    "Your task is to draft a project plan.\n",
    "\n",
    "But first do some thinking on how you want to structure and go through below questions before starting the draft.\n",
    "Please follow these steps:\n",
    "1. Think about who the audience is (this is for CEOs, CTOs and other executives)\n",
    "2. Think about what to start with\n",
    "3. Think about what Challenges you want to solve with this app\n",
    "4. Think about the Tasks that will be needed to be completed\n",
    "5. Create Milestones\n",
    "6. Monitor Progress and Optimize\n",
    "Explain all your thinking in <thinking></thinking> XML Tags and then write the final copy of project plan for executives in <project_plan></project_plan> XML Tag.\n",
    "\n",
    "Output Schema:\n",
    "<thinking>\n",
    "( thoughts to above questions)\n",
    "</thinking>\n",
    "<project_plan>\n",
    "( project plan)\n",
    "</project_plan>\n",
    "\"\"\"\n",
    "\n",
    "# Create a messages object\n",
    "messages = [{\"role\": \"user\", \"content\": [{\"text\": guided_cot}]}]\n",
    "\n",
    "# Call the model with the guided CoT prompt\n",
    "model_response, content_text = call_nova(LITE_MODEL_ID, messages, max_tokens=2048)\n",
    "print(\"\\n[Response Content Text]\")\n",
    "print(\"-\" * 40)\n",
    "print_output(content_text)\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffbbsgvzzi",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "This notebook has demonstrated several key prompting best practices that significantly enhance the performance and output quality of Amazon Nova models. By implementing these techniques, you can achieve more precise, consistent, and useful responses tailored to your specific requirements.\n",
    "\n",
    "## Summary of Techniques Explored\n",
    "\n",
    "### 1. Structured Outputs\n",
    "- **Output Schema Definition:** Providing explicit JSON schema definitions with specific data types guides the model to produce properly formatted outputs.\n",
    "- **Prefill Technique:** Using assistant prefill (`\"```json\"`) combined with appropriate stop sequences helps bypass preamble text and ensures clean, parseable JSON.\n",
    "- **Benefits:** Consistent data structure for downstream processing, reduced parsing errors, and improved automation capabilities.\n",
    "\n",
    "### 2. Few-Shot Examples\n",
    "- **Example-Based Learning:** Providing diverse, relevant examples helps the model understand expected patterns and response styles.\n",
    "- **Format Consistency:** Examples establish a clear template for responses, leading to more uniform outputs.\n",
    "- **Demonstrating Edge Cases:** Good examples showcase both common scenarios and edge cases, improving model robustness.\n",
    "- **Benefits:** More predictable outputs, reduced ambiguity, and higher accuracy across various inputs.\n",
    "\n",
    "### 3. Chain of Thought (CoT) Reasoning\n",
    "- **Guided Thinking Process:** Structured thinking steps help the model break down complex tasks into manageable components.\n",
    "- **XML Tags for Separation:** Using tags like `<thinking>` and `<project_plan>` separates internal reasoning from final outputs.\n",
    "- **Step-by-Step Instructions:** Clear, sequential instructions guide the model through logical reasoning paths.\n",
    "- **Benefits:** More thoughtful responses, better handling of complex scenarios, and clearer separation between reasoning and final output.\n",
    "\n",
    "## Nest steps\n",
    "\n",
    "For more Nova specific prompting guide, please check out [Nova user guide](https://docs.aws.amazon.com/nova/latest/userguide/prompting.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
