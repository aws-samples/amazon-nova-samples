{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83cd93c5",
   "metadata": {},
   "source": [
    "# (Optional Lab) Nova Prompt Optimizer\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The Nova Prompt Optimizer is a powerful tool that automatically improves your prompts for Amazon Nova models using your own datasets. This workshop demonstrates how to:\n",
    "\n",
    "1. Transform manual prompt engineering into an efficient, data-driven process\n",
    "2. Optimize prompts specifically tailored to your use case and data\n",
    "3. Evaluate the performance improvements from optimization\n",
    "\n",
    "By the end of this notebook, you'll understand how to leverage automated prompt optimization to unlock the full potential of Amazon Nova models for your specific applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24872c14-366c-418f-8707-52347f7ad3e5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>Optional Lab</b> \n",
    "    \n",
    "    This notebook takes 15 mins to run. Recommend to treat it as an optional notebook for AWS hosted event.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db810145",
   "metadata": {},
   "source": [
    "## Section 1: Setup and Installation\n",
    "\n",
    "We'll start by installing the Nova Prompt Optimizer SDK, which provides the tools needed to automatically optimize prompts based on your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5668aa9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-25T19:36:09.251412Z",
     "iopub.status.busy": "2025-07-25T19:36:09.251111Z",
     "iopub.status.idle": "2025-07-25T19:36:14.946902Z",
     "shell.execute_reply": "2025-07-25T19:36:14.945988Z",
     "shell.execute_reply.started": "2025-07-25T19:36:09.251389Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install nova-prompt-optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fc38c9-7b03-4c7b-824c-dac92201aa15",
   "metadata": {},
   "source": [
    "## Section 2: Initialize the Input Adapters\n",
    "\n",
    "The Nova Prompt Optimizer uses adapters to standardize inputs from different sources. These adapters help connect your data, prompts, and evaluation metrics into the optimization pipeline.\n",
    "\n",
    "![adapters](nova_prompt_optimizer/docs/adapters.png)\n",
    "\n",
    "### 2.1 Dataset Adapter\n",
    "\n",
    "The Dataset Adapter converts your data into a standardized format for optimization and evaluation:\n",
    "\n",
    "- **Input Columns**: Specify which fields from your data will be used as inputs to the model\n",
    "- **Output Columns**: Specify which fields contain the expected outputs for comparison\n",
    "- **Train/Test Split**: Divide your dataset for optimization and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da047488-0220-4e49-b8d0-361c5afcdfb4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-25T19:36:14.948639Z",
     "iopub.status.busy": "2025-07-25T19:36:14.948286Z",
     "iopub.status.idle": "2025-07-25T19:36:14.959121Z",
     "shell.execute_reply": "2025-07-25T19:36:14.958381Z",
     "shell.execute_reply.started": "2025-07-25T19:36:14.948612Z"
    }
   },
   "outputs": [],
   "source": [
    "from amzn_nova_prompt_optimizer.core.input_adapters.dataset_adapter import JSONDatasetAdapter\n",
    "\n",
    "# Define which columns in our dataset contain inputs and expected outputs\n",
    "input_columns = {\"input\"}  # The field containing the user's query\n",
    "output_columns = {\"answer\"}  # The field containing the expected model response\n",
    "\n",
    "# Initialize the dataset adapter for our JSONL dataset\n",
    "dataset_adapter = JSONDatasetAdapter(input_columns, output_columns)\n",
    "\n",
    "# Load and process the dataset\n",
    "dataset_adapter.adapt(\"nova_prompt_optimizer/data/FacilitySupportAnalyzer.jsonl\")\n",
    "\n",
    "# Split into training data (for optimization) and test data (for evaluation)\n",
    "train_set, test_set = dataset_adapter.split(0.5)  # 50/50 split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22305416-5912-429f-b47b-04592a0966da",
   "metadata": {},
   "source": [
    "### 2.2 Prompt Adapter\n",
    "\n",
    "The Prompt Adapter standardizes your existing prompt template:\n",
    "\n",
    "- **Prompt Variables**: Identify placeholders in your prompt that should be replaced with data from input columns\n",
    "- **File Path**: Provide the path to your original prompt template\n",
    "- **Adapt**: Process the prompt into a standardized format for optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621dc01d-6896-4a37-ad64-63f27c597f75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-25T19:36:14.961109Z",
     "iopub.status.busy": "2025-07-25T19:36:14.960398Z",
     "iopub.status.idle": "2025-07-25T19:36:14.968384Z",
     "shell.execute_reply": "2025-07-25T19:36:14.967682Z",
     "shell.execute_reply.started": "2025-07-25T19:36:14.960988Z"
    }
   },
   "outputs": [],
   "source": [
    "from amzn_nova_prompt_optimizer.core.input_adapters.prompt_adapter import TextPromptAdapter\n",
    "\n",
    "# Define which variables in our prompt will be replaced with data from input columns\n",
    "prompt_variables = input_columns\n",
    "\n",
    "# Initialize the prompt adapter for a text prompt file\n",
    "prompt_adapter = TextPromptAdapter()\n",
    "\n",
    "# Load the original prompt template and specify which variables to replace\n",
    "prompt_adapter.set_user_prompt(file_path=\"nova_prompt_optimizer/original_prompt/user_prompt_template.txt\", variables=prompt_variables)\n",
    "\n",
    "# Process the prompt into a standardized format\n",
    "prompt_adapter.adapt()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8349aeee-4f9a-49e0-94cb-029f2c4fb05f",
   "metadata": {},
   "source": [
    "### 2.3 Metric Adapter\n",
    "\n",
    "The Metric Adapter defines how to evaluate prompt performance:\n",
    "\n",
    "- **Custom Metrics**: Create evaluation metrics specific to your task\n",
    "- **Apply Function**: Evaluate a single model response against the expected output\n",
    "- **Batch Apply Function**: Evaluate multiple responses at once\n",
    "\n",
    "For this example, we'll create a custom metric for the Facility Support Analyzer task that measures:\n",
    "1. JSON validity\n",
    "2. Correctness of categories\n",
    "3. Accuracy of sentiment classification\n",
    "4. Accuracy of urgency classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f093a481-31a0-4871-99a0-670a60d67b1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-25T19:12:01.578429Z",
     "iopub.status.busy": "2025-07-25T19:12:01.577634Z",
     "iopub.status.idle": "2025-07-25T19:12:01.598742Z",
     "shell.execute_reply": "2025-07-25T19:12:01.597484Z",
     "shell.execute_reply.started": "2025-07-25T19:12:01.578362Z"
    }
   },
   "outputs": [],
   "source": [
    "from amzn_nova_prompt_optimizer.core.input_adapters.metric_adapter import MetricAdapter\n",
    "from typing import List, Any, Dict\n",
    "import re\n",
    "import json\n",
    "\n",
    "class FacilitySupportAnalyzerMetric(MetricAdapter):\n",
    "    def parse_json(self, input_string: str):\n",
    "        \"\"\"\n",
    "        Attempts to parse the given string as JSON. If direct parsing fails,\n",
    "        it tries to extract a JSON snippet from code blocks formatted as:\n",
    "            ```json\n",
    "            ... JSON content ...\n",
    "            ```\n",
    "        or any code block delimited by triple backticks and then parses that content.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return json.loads(input_string)\n",
    "        except json.JSONDecodeError as err:\n",
    "            error = err\n",
    "\n",
    "        patterns = [\n",
    "            re.compile(r\"```json\\s*(.*?)\\s*```\", re.DOTALL | re.IGNORECASE),\n",
    "            re.compile(r\"```(.*?)```\", re.DOTALL)\n",
    "        ]\n",
    "\n",
    "        for pattern in patterns:\n",
    "            match = pattern.search(input_string)\n",
    "            if match:\n",
    "                json_candidate = match.group(1).strip()\n",
    "                try:\n",
    "                    return json.loads(json_candidate)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "\n",
    "        raise error\n",
    "\n",
    "    def _calculate_metrics(self, y_pred: Any, y_true: Any) -> Dict:\n",
    "        strict_json = False\n",
    "        result = {\n",
    "            \"is_valid_json\": False,\n",
    "            \"correct_categories\": 0.0,\n",
    "            \"correct_sentiment\": False,\n",
    "            \"correct_urgency\": False,\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            y_true = y_true if isinstance(y_true, dict) else (json.loads(y_true) if strict_json else self.parse_json(y_true))\n",
    "            y_pred = y_pred if isinstance(y_pred, dict) else (json.loads(y_pred) if strict_json else self.parse_json(y_pred))\n",
    "        except json.JSONDecodeError:\n",
    "            result[\"total\"] = 0\n",
    "            return result  # Return result with is_valid_json = False\n",
    "        else:\n",
    "            result[\"is_valid_json\"] = True\n",
    "\n",
    "            categories_true = y_true.get(\"categories\", {})\n",
    "            categories_pred = y_pred.get(\"categories\", {})\n",
    "\n",
    "            if isinstance(categories_true, dict) and isinstance(categories_pred, dict):\n",
    "                correct = sum(\n",
    "                    categories_true.get(k, False) == categories_pred.get(k, False)\n",
    "                    for k in categories_true\n",
    "                )\n",
    "                result[\"correct_categories\"] = correct / len(categories_true) if categories_true else 0.0\n",
    "            else:\n",
    "                result[\"correct_categories\"] = 0.0  # or raise an error if you prefer\n",
    "\n",
    "            result[\"correct_sentiment\"] = y_pred.get(\"sentiment\", \"\") == y_true.get(\"sentiment\", \"\")\n",
    "            result[\"correct_urgency\"] = y_pred.get(\"urgency\", \"\") == y_true.get(\"urgency\", \"\")\n",
    "\n",
    "        # Compute overall metric score\n",
    "        result[\"total\"] = sum(\n",
    "            float(result[k]) for k in [\"correct_categories\", \"correct_sentiment\", \"correct_urgency\"]\n",
    "        ) / 3.0\n",
    "\n",
    "        return result\n",
    "\n",
    "    def apply(self, y_pred: Any, y_true: Any):\n",
    "        return self._calculate_metrics(y_pred, y_true)\n",
    "\n",
    "    def batch_apply(self, y_preds: List[Any], y_trues: List[Any]):\n",
    "        evals = [self.apply(y_pred, y_true) for y_pred, y_true in zip(y_preds, y_trues)]\n",
    "        float_keys = [k for k, v in evals[0].items() if isinstance(v, (int, float, bool))]\n",
    "        return {k: sum(e[k] for e in evals) / len(evals) for k in float_keys}\n",
    "\n",
    "metric_adapter = FacilitySupportAnalyzerMetric()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2852c904-42e8-46a2-ad9e-a2e88d8eba6a",
   "metadata": {},
   "source": [
    "### 2.4 Inference Adapter\n",
    "\n",
    "The Inference Adapter connects to the model service:\n",
    "\n",
    "- **Backend**: Currently supports Amazon Bedrock\n",
    "- **Region**: Specify which AWS region to use for inference\n",
    "- **Configuration**: Set up the connection to the inference service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea654ddd-59d0-495e-8e56-29fe0ed6dd7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-25T19:12:01.607930Z",
     "iopub.status.busy": "2025-07-25T19:12:01.606821Z",
     "iopub.status.idle": "2025-07-25T19:12:02.376388Z",
     "shell.execute_reply": "2025-07-25T19:12:02.375554Z",
     "shell.execute_reply.started": "2025-07-25T19:12:01.607897Z"
    }
   },
   "outputs": [],
   "source": [
    "from amzn_nova_prompt_optimizer.core.inference.adapter import BedrockInferenceAdapter\n",
    "\n",
    "# Initialize the inference adapter to connect to Amazon Bedrock\n",
    "# We're using us-west-2 region for this example\n",
    "inference_adapter = BedrockInferenceAdapter(region_name=\"us-west-2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb20883-793c-4b38-8e2c-7f2d461cc2fb",
   "metadata": {},
   "source": [
    "## Section 3: Evaluate the Original Prompt\n",
    "\n",
    "Before optimization, we'll establish a baseline by evaluating the original prompt's performance on our test dataset. This will help us measure the improvement from optimization.\n",
    "\n",
    "The Evaluator:\n",
    "- Takes our prompt, test data, metrics, and inference adapter\n",
    "- Generates predictions using the original prompt\n",
    "- Calculates evaluation metrics on these predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bb5ac6",
   "metadata": {},
   "source": [
    "#### Base Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f158430-ed74-470c-8657-2b49b57ae79f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-25T19:12:02.384988Z",
     "iopub.status.busy": "2025-07-25T19:12:02.384497Z",
     "iopub.status.idle": "2025-07-25T19:12:02.391811Z",
     "shell.execute_reply": "2025-07-25T19:12:02.390955Z",
     "shell.execute_reply.started": "2025-07-25T19:12:02.384939Z"
    }
   },
   "outputs": [],
   "source": [
    "from amzn_nova_prompt_optimizer.core.evaluation import Evaluator\n",
    "\n",
    "# Initialize the evaluator with all our components\n",
    "# - prompt_adapter: The prompt to evaluate\n",
    "# - test_set: Data to run the evaluation on\n",
    "# - metric_adapter: How to calculate performance metrics\n",
    "# - inference_adapter: Connection to the model service\n",
    "evaluator = Evaluator(prompt_adapter, test_set, metric_adapter, inference_adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33974a07-c556-4238-a39c-12601fa01e14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-25T19:12:02.394647Z",
     "iopub.status.busy": "2025-07-25T19:12:02.394263Z",
     "iopub.status.idle": "2025-07-25T19:12:41.743665Z",
     "shell.execute_reply": "2025-07-25T19:12:41.742790Z",
     "shell.execute_reply.started": "2025-07-25T19:12:02.394617Z"
    }
   },
   "outputs": [],
   "source": [
    "# Run evaluation of the original prompt with Amazon Nova Lite\n",
    "# This will generate predictions and calculate metrics\n",
    "original_prompt_score = evaluator.aggregate_score(model_id=\"us.amazon.nova-lite-v1:0\")\n",
    "\n",
    "print(f\"Original Prompt Evaluation Score = {original_prompt_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa60377",
   "metadata": {},
   "source": [
    "## Section 4: Optimize the Prompt\n",
    "\n",
    "Now we'll use the Nova Prompt Optimizer to automatically improve our prompt based on the training data.\n",
    "\n",
    "### 4.1 Optimization Metric\n",
    "\n",
    "First, we need to adapt our metric for the optimizer, which requires a single numerical score instead of multiple metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46adefe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-25T19:12:41.745833Z",
     "iopub.status.busy": "2025-07-25T19:12:41.745584Z",
     "iopub.status.idle": "2025-07-25T19:12:41.751325Z",
     "shell.execute_reply": "2025-07-25T19:12:41.749957Z",
     "shell.execute_reply.started": "2025-07-25T19:12:41.745812Z"
    }
   },
   "outputs": [],
   "source": [
    "class FacilitySupportAnalyzerNovaPromptOptimizerMetric(FacilitySupportAnalyzerMetric):\n",
    "    def apply(self, y_pred: Any, y_true: Any):\n",
    "        \"\"\"\n",
    "        Returns a single numerical value for the optimizer to use.\n",
    "        The optimizer needs a single score to maximize during optimization.\n",
    "        \n",
    "        Args:\n",
    "            y_pred: The model's prediction\n",
    "            y_true: The expected output\n",
    "            \n",
    "        Returns:\n",
    "            float: A score between 0 and 1, with higher being better\n",
    "        \"\"\"\n",
    "        # Calculate metrics and return the total score (average of all metrics)\n",
    "        return self._calculate_metrics(y_pred, y_true)[\"total\"]\n",
    "        \n",
    "    def batch_apply(self, y_preds: List[Any], y_trues: List[Any]):\n",
    "        # Not used during optimization\n",
    "        pass\n",
    "    \n",
    "# Create the metric adapter for optimization\n",
    "nova_prompt_optimizer_metric_adapter = FacilitySupportAnalyzerNovaPromptOptimizerMetric()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab372d4-8ffc-4104-a5f6-30160a48f9f6",
   "metadata": {},
   "source": [
    "### 4.2 Optimization Adapters\n",
    "\n",
    "Next, we'll set up the optimization process. The Nova Prompt Optimizer takes:\n",
    "\n",
    "- **Prompt Adapter**: The original prompt to optimize\n",
    "- **Inference Adapter**: Connection to the model service\n",
    "- **Dataset Adapter**: Training data to learn from\n",
    "- **Metric Adapter**: How to evaluate prompt performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bbc2fe-e95e-4cc0-9087-eaf0b111aa37",
   "metadata": {},
   "source": [
    "### 4.3 Nova Prompt Optimizer\n",
    "\n",
    "The Nova Prompt Optimizer uses a two-stage approach:\n",
    "\n",
    "1. **Meta Prompting**: Analyzes your prompt to identify system instructions and user template patterns\n",
    "2. **MIPROv2 Optimization**: Improves system instructions and adds few-shot examples based on your dataset\n",
    "\n",
    "The optimizer can run in different modes based on your Nova model:\n",
    "- **Lite mode**: Optimized for Nova Lite, faster optimization with fewer resources\n",
    "- **Pro mode**: Optimized for Nova Pro, more thorough optimization that may take longer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c2d973",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-25T19:12:41.752800Z",
     "iopub.status.busy": "2025-07-25T19:12:41.752455Z",
     "iopub.status.idle": "2025-07-25T19:25:57.451200Z",
     "shell.execute_reply": "2025-07-25T19:25:57.450356Z",
     "shell.execute_reply.started": "2025-07-25T19:12:41.752769Z"
    }
   },
   "outputs": [],
   "source": [
    "from amzn_nova_prompt_optimizer.core.optimizers import NovaPromptOptimizer\n",
    "\n",
    "# Initialize the Nova Prompt Optimizer with our components\n",
    "nova_prompt_optimizer = NovaPromptOptimizer(\n",
    "    prompt_adapter=prompt_adapter,        # Original prompt to optimize\n",
    "    inference_adapter=inference_adapter,  # Connection to model service\n",
    "    dataset_adapter=train_set,            # Training data to learn from\n",
    "    metric_adapter=nova_prompt_optimizer_metric_adapter  # How to evaluate performance\n",
    ")\n",
    "\n",
    "# Run the optimization process in \"lite\" mode for Nova Lite\n",
    "# This will analyze the prompt, identify improvements, and generate few-shot examples\n",
    "optimized_prompt_adapter = nova_prompt_optimizer.optimize(mode=\"lite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecbfc1f",
   "metadata": {},
   "source": [
    "### 4.4 Examining the Optimized Prompt\n",
    "\n",
    "Let's examine what the optimizer has produced. First, the optimized system prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826f76fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-25T19:25:57.452207Z",
     "iopub.status.busy": "2025-07-25T19:25:57.452007Z",
     "iopub.status.idle": "2025-07-25T19:25:57.457117Z",
     "shell.execute_reply": "2025-07-25T19:25:57.456019Z",
     "shell.execute_reply.started": "2025-07-25T19:25:57.452189Z"
    }
   },
   "outputs": [],
   "source": [
    "optimized_prompt_adapter.system_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb8c6b5",
   "metadata": {},
   "source": [
    "### 4.5 Optimized User Prompt\n",
    "\n",
    "Now let's look at the optimized user prompt template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad2ac43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-25T19:25:57.458937Z",
     "iopub.status.busy": "2025-07-25T19:25:57.458509Z",
     "iopub.status.idle": "2025-07-25T19:25:57.464409Z",
     "shell.execute_reply": "2025-07-25T19:25:57.463792Z",
     "shell.execute_reply.started": "2025-07-25T19:25:57.458913Z"
    }
   },
   "outputs": [],
   "source": [
    "optimized_prompt_adapter.user_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da44cf06",
   "metadata": {},
   "source": [
    "### 4.6 Saving the Optimized Prompt\n",
    "\n",
    "Let's save the optimized prompt for future use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a6bce9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-25T19:30:25.234063Z",
     "iopub.status.busy": "2025-07-25T19:30:25.233762Z",
     "iopub.status.idle": "2025-07-25T19:30:25.238465Z",
     "shell.execute_reply": "2025-07-25T19:30:25.237627Z",
     "shell.execute_reply.started": "2025-07-25T19:30:25.234041Z"
    }
   },
   "outputs": [],
   "source": [
    "optimized_prompt_adapter.save(\"nova_prompt_optimizer/optimized_prompt/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d30a347",
   "metadata": {},
   "source": [
    "## Section 5: Evaluate the Optimized Prompt\n",
    "\n",
    "Now let's measure the performance of our optimized prompt on the test dataset to see how much improvement we've gained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65493b31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-25T19:30:27.518359Z",
     "iopub.status.busy": "2025-07-25T19:30:27.517650Z",
     "iopub.status.idle": "2025-07-25T19:30:27.522501Z",
     "shell.execute_reply": "2025-07-25T19:30:27.521661Z",
     "shell.execute_reply.started": "2025-07-25T19:30:27.518329Z"
    }
   },
   "outputs": [],
   "source": [
    "from amzn_nova_prompt_optimizer.core.evaluation import Evaluator\n",
    "\n",
    "# Create a new evaluator for the optimized prompt\n",
    "evaluator = Evaluator(\n",
    "    optimized_prompt_adapter,  # Now using the optimized prompt\n",
    "    test_set,                  # Same test data as before\n",
    "    metric_adapter,            # Same evaluation metrics\n",
    "    inference_adapter          # Same model service\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43b528d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-25T19:30:28.801209Z",
     "iopub.status.busy": "2025-07-25T19:30:28.800162Z",
     "iopub.status.idle": "2025-07-25T19:31:08.417251Z",
     "shell.execute_reply": "2025-07-25T19:31:08.416315Z",
     "shell.execute_reply.started": "2025-07-25T19:30:28.801173Z"
    }
   },
   "outputs": [],
   "source": [
    "# Run evaluation of the optimized prompt with Amazon Nova Lite\n",
    "nova_prompt_optimizer_eval_score = evaluator.aggregate_score(model_id=\"us.amazon.nova-lite-v1:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bfbfaf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-25T19:36:19.332607Z",
     "iopub.status.busy": "2025-07-25T19:36:19.332233Z",
     "iopub.status.idle": "2025-07-25T19:36:19.337394Z",
     "shell.execute_reply": "2025-07-25T19:36:19.336485Z",
     "shell.execute_reply.started": "2025-07-25T19:36:19.332582Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print the score and compare it to the original prompt\n",
    "print(f\"Optimized Prompt Evaluation Score = {nova_prompt_optimizer_eval_score}\")\n",
    "print(f\"Improvement: {nova_prompt_optimizer_eval_score['total'] - original_prompt_score['total']:.4f} ({(nova_prompt_optimizer_eval_score['total'] - original_prompt_score['total']) / original_prompt_score['total'] * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b345963-5516-49c8-9b7c-91eccbfe348c",
   "metadata": {},
   "source": [
    "### 5.1 Saving Evaluation Results\n",
    "\n",
    "Let's save the detailed evaluation results for analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2acd687-d68d-4b09-8bd3-7ae88626c9a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-25T19:36:26.715022Z",
     "iopub.status.busy": "2025-07-25T19:36:26.714665Z",
     "iopub.status.idle": "2025-07-25T19:36:26.724137Z",
     "shell.execute_reply": "2025-07-25T19:36:26.723461Z",
     "shell.execute_reply.started": "2025-07-25T19:36:26.714998Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluator.save(\"nova_prompt_optimizer/evals/nova_lite/nova_prompt_optimizer_eval.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b7af64",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this workshop, we've explored how to use the Nova Prompt Optimizer to automatically improve prompt performance for Amazon Nova models. Let's summarize what we've learned and the benefits of this approach.\n",
    "\n",
    "## Key Learnings\n",
    "\n",
    "### 1. The Power of Automated Optimization\n",
    "- **Data-Driven Improvements**: Rather than manual trial-and-error, we used our own dataset to guide prompt optimization\n",
    "- **Systematic Approach**: The optimizer methodically analyzes and enhances prompts through meta-prompting and few-shot learning\n",
    "- **Measurable Results**: We quantitatively measured performance gains between original and optimized prompts\n",
    "\n",
    "### 2. Components of the Nova Prompt Optimizer\n",
    "- **Dataset Adapter**: Standardized our dataset for use in optimization and evaluation\n",
    "- **Prompt Adapter**: Processed our original prompt into a format suitable for optimization\n",
    "- **Metric Adapter**: Provided custom evaluation metrics specific to our task\n",
    "- **Inference Adapter**: Connected us to Amazon Nova models for testing\n",
    "- **Optimization Process**: Combined meta-prompting and MIPROv2 techniques\n",
    "\n",
    "### 3. Optimization Techniques Applied\n",
    "- **System Prompt Refinement**: Improved the system instructions for better task understanding\n",
    "- **Few-Shot Example Selection**: Automatically identified the most helpful examples from our data\n",
    "- **Format Optimization**: Enhanced output formatting and structure\n",
    "- **Task-Specific Guidance**: Added task-specific tips and clarifications\n",
    "\n",
    "## Benefits for Production Applications\n",
    "\n",
    "1. **Reduced Engineering Time**: Automates the time-consuming process of prompt engineering\n",
    "2. **Consistent Performance**: Creates reliable, tested prompts for production use\n",
    "3. **Adaptability**: Easily update optimized prompts as your data or requirements change\n",
    "4. **Model Flexibility**: Works with different Amazon Nova models (Micro, Lite, Pro)\n",
    "5. **Customization**: Optimizes for your specific data and task requirements\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "As you apply the [Nova Prompt Optimizer](https://github.com/aws/nova-prompt-optimizer) to your own projects, consider:\n",
    "\n",
    "1. **Expand Your Dataset**: Larger, more diverse datasets often yield better optimization results\n",
    "2. **Test Different Metrics**: Create custom metrics that align closely with your business goals\n",
    "3. **Compare Models**: Try optimizing for different Nova models to find the best performance/cost balance\n",
    "4. **Periodic Re-optimization**: Update your prompts as your data or requirements evolve\n",
    "5. **Integration**: Incorporate optimized prompts into your production applications"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
