{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09c3cdab",
   "metadata": {},
   "source": [
    "# 01_Getting Started with Amazon Nova Models\n",
    "\n",
    "Amazon Nova is a new generation of multimodal understanding and creative content generation models that offer state-of-the-art quality, unparalleled customization, and the best price-performance. Amazon Nova models incorporate the same secure-by-design approach as all AWS services, with built-in controls for the safe and responsible use of AI.\n",
    "\n",
    "Amazon Nova has two categories of models: \n",
    " - **Understanding models** —These models are capable of reasoning over several input modalities, including text, video, and image, and output text. \n",
    "- **Creative Content Generation models** —These models generate images or videos based on a text or image prompt.\n",
    "  \n",
    "### Amazon Nova Models at Glance\n",
    "![media/model_intro.png](media/model_intro.png)\n",
    "\n",
    "**Multimodal Understanding Models**\n",
    "- **Amazon Nova Micro**: Lightening fast, cost-effective text-only model\n",
    "- **Amazon Nova Lite**: Fast, affordable multimodal FM for general intelligence tasks\n",
    "- **Amazon Nova 2.0 Lite**: Fastest, agentic-forward reasoning FM in the industry for its intelligence tier\n",
    "- **Amazon Nova Pro**:  The fastest, most cost-effective, state-of-the-art multimodal model in the industry\n",
    "- **Amazon Nova Premier**:  Most capable multimodal model for complex tasks and the best teacher for distilling custom models for cost-effective applications. \n",
    "\n",
    "**Creative Content Generation Models**\n",
    "- **Amazon Nova Canvas**:State-of-the-art image generation model\n",
    "- **Amazon Nova Reel**:State-of-the-art video generation model\n",
    "\n",
    "\n",
    "The following notebooks will be focused primarily on Amazon Nova Understanding Models. \n",
    "\n",
    "**Amazon Nova Multimodal understanding** foundation models (FMs) are a family of models that are capable of reasoning over several input modalities, including text, video, documents and/or images, and output text. You can access these models through the Bedrock Converse API and InvokeModel API.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c633ddd-299c-4fb6-98ba-b01bb8b48f50",
   "metadata": {},
   "source": [
    "## 2 When to Use What?\n",
    "\n",
    "### 2.1 When to Use Amazon Nova Micro 1.0 Model\n",
    "\n",
    "Amazon Nova Micro (Text Input Only) is the fastest and most affordable option, optimized for large-scale, latency-sensitive deployments like conversational interfaces, chats, and high-volume tasks, such as classification, routing, entity extraction, and document summarization.\n",
    "\n",
    "### 2.2 When to Use Amazon Nova Lite 2.0 Model\n",
    "\n",
    "Amazon Nova Lite balances intelligence, latency, and cost-effectiveness. It’s optimized for complex scenarios where low latency (minimal delay) is crucial, such as interactive agents that need to orchestrate multiple tool calls simultaneously. Amazon Nova Lite 2 supports larger context window, image understanding, video understanding, and text inputs and outputs. \n",
    "\n",
    "### 2.3 When to Use Amazon Nova Pro 1.0 Model\n",
    "Amazon Nova Pro is designed for highly complex use cases requiring advanced reasoning, creativity, and code generation. Amazon Nova pro supports image, video, and text inputs and outputs text. \n",
    "\n",
    "\n",
    "### 2.4 When to Use Amazon Nova Premier 1.0 Model\n",
    "Nova Premier is our best model for complex tasks like software development, multi-step function calling, and orchestrating multi-agent workflows. It is also our most capable teacher model and can be used with Amazon Bedrock Model Distillation to create custom distilled models for specific needs\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**In this notebook, we will explore Nova lite 2.0 model**\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "Run the cells in this section to install the required packages. ⚠️ You may see pip dependency errors, you can safely ignore these errors. ⚠️\n",
    "\n",
    "_IGNORE ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f288d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q --force-reinstall \\\n",
    "    \"botocore>=1.40.26\" \\\n",
    "    \"awscli>=1.29.57\" \\\n",
    "    \"requests\" \\\n",
    "    \"boto3>=1.40.26\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfe86f9-b9d3-4d10-b4c6-d80f38d52b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dd0c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import base64\n",
    "from datetime import datetime\n",
    "from botocore.config import Config\n",
    "from pprint import pprint\n",
    "\n",
    "LITE_MODEL_ID = \"us.amazon.nova-2-lite-v1:0\"\n",
    "\n",
    "# Create a Bedrock Runtime client\n",
    "client = boto3.client(\"bedrock-runtime\", \n",
    "                      region_name=\"us-east-1\", \n",
    "                      config=Config(read_timeout=10000))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e62778-e5f5-4664-a81c-ac27b54d3c2c",
   "metadata": {},
   "source": [
    "### InvokeModel body and output\n",
    "\n",
    "The invoke_model() method of the Amazon Bedrock runtime client (InvokeModel API) will be the primary method we use for most of our Text Generation and Processing tasks\n",
    "\n",
    "Although the method is shared, the format of input and output varies depending on the foundation model used - as described below:\n",
    "\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"system\": [\n",
    "    {\n",
    "      \"text\": string\n",
    "    }\n",
    "  ],\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",# first turn should always be the user turn\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"text\": string\n",
    "        },\n",
    "        {\n",
    "          \"image\": {\n",
    "            \"format\": \"jpeg\"| \"png\" | \"gif\" | \"webp\",\n",
    "            \"source\": {\n",
    "            # source can be s3 location or base64 bytes based on size of input file. \n",
    "               \"s3Location\": {\n",
    "                \"uri\": string, #  example: s3://my-bucket/object-key\n",
    "                \"bucketOwner\": string #  (Optional) example: 123456789012)\n",
    "               }\n",
    "              \"bytes\": \"base64EncodedImageDataHere...\" #  base64-encoded binary\n",
    "            }\n",
    "          }\n",
    "        },\n",
    "        {\n",
    "          \"video\": {\n",
    "            \"format\": \"mkv\" | \"mov\" | \"mp4\" | \"webm\" | \"three_gp\" | \"flv\" | \"mpeg\" | \"mpg\" | \"wmv\",\n",
    "            \"source\": {\n",
    "            # source can be s3 location or base64 bytes based on size of input file. \n",
    "               \"s3Location\": {\n",
    "                \"uri\": string, #  example: s3://my-bucket/object-key\n",
    "                \"bucketOwner\": string #  (Optional) example: 123456789012)\n",
    "               }\n",
    "              \"bytes\": \"base64EncodedImageDataHere...\" #  base64-encoded binary\n",
    "            }\n",
    "          }\n",
    "        }]}, \n",
    "      {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"text\": string # prefilling assistant turn\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    " \"inferenceConfig\":{ # all Optional\n",
    "    \"max_new_tokens\": int, #  greater than 0, equal or less than 5k (default: dynamic*)\n",
    "    \"temperature\": float, # greater then 0 and less than 1.0 (default: 0.7)\n",
    "    \"top_p\": float, #  greater than 0, equal or less than 1.0 (default: 0.9)\n",
    "    \"top_k\": int #  0 or greater (default: 50)\n",
    "    \"stopSequences\": [string]\n",
    " },\n",
    "  \"additionalModelRequestFields\": { # This section ONLY applies to Nova Lite 2 model\n",
    "    \"reasoningConfig\": {\n",
    "      \"type\": \"enabled\",\n",
    "      \"maxReasoningEffort\": \"low\" # Supported: low, medium and high \n",
    "    },\n",
    "  \"toolConfig\": { #  all Optional\n",
    "        \"tools\": [\n",
    "                {\n",
    "                    \"toolSpec\": {\n",
    "                        \"name\": string # menaingful tool name (Max char: 64)\n",
    "                        \"description\": string # meaningful description of the tool\n",
    "                        \"inputSchema\": {\n",
    "                            \"json\": { # The JSON schema for the tool. For more information, see JSON Schema Reference\n",
    "                                \"type\": \"object\",\n",
    "                                \"properties\": {\n",
    "                                    <args>: { # arguments \n",
    "                                        \"type\": string, # argument data type\n",
    "                                        \"description\": string # meaningful description\n",
    "                                    }\n",
    "                                },\n",
    "                                \"required\": [\n",
    "                                    string # args\n",
    "                                ]\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ],\n",
    "   \"toolChoice\": \"auto\" | \"tool\" | \"any\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "The following are required parameters.\n",
    "\n",
    "* `system` – (Optional) The system prompt for the request.\n",
    "    A system prompt is a way of providing context and instructions to Amazon Nova, such as specifying a particular goal or role.\n",
    "* `messages` – (Required) The input messages.\n",
    "    * `role` – The role of the conversation turn. Valid values are user and assistant. \n",
    "    * `content` – (required) The content of the conversation turn.\n",
    "        * `type` – (required) The type of the content. Valid values are image, text. , video\n",
    "            * if chosen text (text content)\n",
    "                * `text` - The content of the conversation turn. \n",
    "            * If chosen Image (image content)\n",
    "                * `source` – (required) The base64 encoded image bytes for the video or S3 URI and bucket owner as shown in the above schema\n",
    "                * `format` – (required) The type of the image. You can specify the following image formats. \n",
    "                    * `jpeg`\n",
    "                    * `png`\n",
    "                    * `webp`\n",
    "                    * `gif`\n",
    "            * If chosen video: (video content)\n",
    "                * `source` – (required) The base64 encoded image bytes for the video or S3 URI and bucket owner as shown in the above schema\n",
    "                * `format` – (required) The type of the video. You can specify the following video formats. \n",
    "                    * `mkv`\n",
    "                    *  `mov`  \n",
    "                    *  `mp4`\n",
    "                    *  `webm`\n",
    "                    *  `three_gp`\n",
    "                    *  `flv`  \n",
    "                    *  `mpeg`  \n",
    "                    *  `mpg`\n",
    "                    *  `wmv`\n",
    "* `inferenceConfig`: These are inference config values that can be passed in inference.\n",
    "    * `max_new_tokens` – (Optional) The maximum number of tokens to generate before stopping.\n",
    "        Note that Amazon Nova models might stop generating tokens before reaching the value of max_tokens. Maximum New Tokens value allowed is 5K.\n",
    "    * `temperature` – (Optional) The amount of randomness injected into the response.\n",
    "    * `top_p` – (Optional) Use nucleus sampling. Amazon Nova computes the cumulative distribution over all the options for each subsequent token in decreasing probability order and cuts it off once it reaches a particular probability specified by top_p. You should alter either temperature or top_p, but not both.\n",
    "    * `top_k` – (Optional) Only sample from the top K options for each subsequent token. Use top_k to remove long tail low probability responses.\n",
    "    * `stopSequences` – (Optional) Array of strings containing step sequences. If the model generates any of those strings, generation will stop and response is returned up until that point. \n",
    "    * `toolConfig` – (Optional) JSON object following ToolConfig schema,  containing the tool specification and tool choice. This schema is the same followed by the Converse API\n",
    "* `additionalModelRequestFields`: These are additional values should be passed in inference requests for Nova Lite 2 model\n",
    "    * `reasoningConfig` –  (required) The maximum number of tokens the model can use to reason for a task\n",
    "    * `type` –  (required) set it to \"enabled\" \n",
    "    * `maxReasoningEffort` –  (required) Allowed values are  or \"low\", \"medium\" or \"high\" \n",
    "       Choose “High” for complex tasks and for less complicated tasks “Low” should be selected.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9909492",
   "metadata": {},
   "source": [
    "### 3. Text Understanding\n",
    "The examples below demonstrates text understanding capabilities\n",
    "Note: Below examples are using Nova Lite for Illustrative Purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0264e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Utility functions to make synchronous and async calls to invoke the Nova models\n",
    "# \n",
    "\n",
    "def sync_nova_2_model_invocation(client, modelId, system_list, message_list, inf_params):\n",
    "    # Invoke the model synchrously and extract the response body.\n",
    "    native_request = {\n",
    "    \"messages\": message_list,\n",
    "    \"system\": system_list,\n",
    "    \"inferenceConfig\": inf_params,\n",
    "    }\n",
    "\n",
    "    response = client.invoke_model(modelId=modelId, body=json.dumps(native_request))\n",
    "    request_id = response[\"ResponseMetadata\"][\"RequestId\"]\n",
    "    print(f\"Request ID: {request_id}\")\n",
    "    # print the output\n",
    "    response = response['body'].read().decode('utf-8')\n",
    "    json_output = json.loads(response)\n",
    "    return json_output\n",
    "\n",
    "\n",
    "\n",
    "def async_nova_2_model_invocation(client, modelId, system_list, message_list, inf_params):\n",
    "    \n",
    "    time_to_first_token = None\n",
    "    native_request = {\n",
    "        \"messages\": message_list,\n",
    "        \"system\": system_list,\n",
    "        \"inferenceConfig\": inf_params,\n",
    "        }\n",
    "    start_time = datetime.now()    \n",
    "    # Invoke the model with the response stream\n",
    "    response = client.invoke_model_with_response_stream(modelId=modelId, body=json.dumps(native_request))\n",
    "    request_id = response.get(\"ResponseMetadata\").get(\"RequestId\")\n",
    "    print(f\"Request ID: {request_id}\")\n",
    "    print(\"Awaiting first token...\")\n",
    "    chunk_count = 0\n",
    "    # Process the response stream\n",
    "    stream = response.get(\"body\")\n",
    "    if stream:\n",
    "        for event in stream:\n",
    "            content = event['chunk']['bytes'].decode('utf-8')\n",
    "            content_json = json.loads(content)\n",
    "            if 'messageStart' in content_json:\n",
    "                pass\n",
    "            elif 'contentBlockDelta' in content_json:\n",
    "                content_block_delta = content_json[\"contentBlockDelta\"]\n",
    "                if content_block_delta:\n",
    "                    if time_to_first_token is None:\n",
    "                        time_to_first_token = datetime.now() - start_time\n",
    "                        print(f\"Time to first token: {time_to_first_token}\")\n",
    "                    chunk_count += 1\n",
    "                    current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S:%f\")\n",
    "                #    # print(f\"{current_time} - \", end=\"\")\n",
    "                    print(content_block_delta.get(\"delta\").get(\"text\"), end=\"\")\n",
    "    else:\n",
    "        print(\"No response stream received.\")\n",
    "    return chunk_count, time_to_first_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11fb70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#\n",
    "# Utility function to download contents of a web page\n",
    "# \n",
    "\n",
    "import requests\n",
    "\n",
    "def get_raw_html(url):\n",
    "    \"\"\"\n",
    "    Fetches the raw HTML content of a given URL.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the web page.\n",
    "\n",
    "    Returns:\n",
    "        str: The raw HTML content of the page, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an exception for bad status codes\n",
    "        return response.text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching URL {url}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f388623",
   "metadata": {},
   "source": [
    "#### 3.1 Invoke_model() API Call\n",
    "\n",
    "The example below demonstrates how to use a text-based prompt with the invoke_model API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353f4769-4397-4dae-a4de-8a295569f6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelId = LITE_MODEL_ID\n",
    "\n",
    "# Define your system prompt(s).\n",
    "system_list = [\n",
    "    { \"text\": \"You are expert in summarizing contents from long text provided as context. You do not reference any external source to answer the question.\" }\n",
    "]\n",
    "\n",
    "letter_text = get_raw_html(\"https://www.aboutamazon.com/news/company-news/amazon-ceo-andy-jassy-2024-letter-to-shareholders\")\n",
    "\n",
    "# Define one or more messages using the \"user\" and \"assistant\" roles.\n",
    "message_list = [\n",
    "    {\"role\": \"user\", \"content\": [{\"text\": f\"\"\"list the \"new whys\" disucssed in the text below\n",
    "                                             {letter_text}\n",
    "                                             \"\"\"}]},\n",
    "]\n",
    "\n",
    "# Configure the inference parameters.\n",
    "inf_params = {\n",
    "    \"maxTokens\": 2048, \n",
    "    \"topP\": 0.9, \n",
    "    \"temperature\": 0.7,\n",
    "    \"reasoningConfig\": {\n",
    "        \"type\": \"enabled\",\n",
    "        \"maxReasoningEffort\": \"low\"\n",
    "    }\n",
    "} \n",
    "\n",
    "json_output = sync_nova_2_model_invocation(client=client, \n",
    "                                            modelId=modelId,\n",
    "                                            system_list=system_list,\n",
    "                                            message_list=message_list,\n",
    "                                            inf_params=inf_params)\n",
    "print(json_output['output']['message']['content'][1]['text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9926bc",
   "metadata": {},
   "source": [
    "#### 3.2 Streaming Invocation API call\n",
    "\n",
    "The example below demonstrates how to use a text-based prompt with the streaming API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4728b5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define your system prompt(s).\n",
    "system_list = [\n",
    "    { \"text\": \"Act as a creative writing assistant. When the user provides you with a topic, write a short story about that topic in less than 200 words\" }\n",
    "]\n",
    "\n",
    "# Define one or more messages using the \"user\" and \"assistant\" roles.\n",
    "message_list = [{\"role\": \"user\", \"content\": [{\"text\": \"A camping trip\"}]}]\n",
    "\n",
    "\n",
    "# Configure the inference parameters.    \n",
    "inf_params = {\n",
    "    \"maxTokens\": 1024, \n",
    "    \"topP\": 0.9, \n",
    "    \"temperature\": 0.7,\n",
    "    \"reasoningConfig\": {\n",
    "        \"type\": \"enabled\",\n",
    "        \"maxReasoningEffort\": \"low\"\n",
    "    }\n",
    "} \n",
    "\n",
    "# Invoke the model with the response stream\n",
    "total_chunks, time_to_first_token = async_nova_2_model_invocation(client=client,\n",
    "                                            modelId=modelId, \n",
    "                                            system_list=system_list, \n",
    "                                            message_list=message_list, \n",
    "                                            inf_params=inf_params)\n",
    "print(f\"\\n\\nTime to first token: {time_to_first_token}\")\n",
    "print(f\"\\n\\nTotal chunks: {total_chunks}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c25e1a9-912e-4bb5-82ef-9b3f673bd215",
   "metadata": {},
   "source": [
    "### 4. Multimodal Understanding \n",
    "\n",
    "The following examples show how to pass various media types to the model.\n",
    "Amazon Nova models allow you to include multiple images in the payload with a limitation of total payload size to not go beyond 25MB. However, you can specify an Amazon S3 URI for image understanding. This approach enables you to leverage the model for larger images as well as multiple images without being constrained by the overall payload size limitation. .Amazon Nova models can analyze the passed images and answer questions, classify an image, as well as summarize images based on provided instructions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0239ff-e022-462f-b44d-b6d840ceade5",
   "metadata": {},
   "source": [
    "#### 4.1 Image Understanding\n",
    "\n",
    "Lets see how Nova model does on image understanding use cases. \n",
    "\n",
    "\n",
    "![A Sunset Image](media/kites_and_plane.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01e09ea-112d-4cfd-a778-b06f6477be85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Open the image you'd like to use and encode it as a Base64 string.\n",
    "with open(\"media/kites_and_plane.png\", \"rb\") as image_file:\n",
    "    binary_data = image_file.read()\n",
    "    base_64_encoded_data = base64.b64encode(binary_data)\n",
    "    base64_string = base_64_encoded_data.decode(\"utf-8\")\n",
    "\n",
    "# Define your system prompt(s).\n",
    "system_list = [\n",
    "    { \"text\": \"You are an expert artist. When the user provides you with an image, provide 3 potential art titles\" }\n",
    "]\n",
    "\n",
    "# Define a \"user\" message including both the image and a text prompt.\n",
    "message_list = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"image\": {\n",
    "                    \"format\": \"png\",\n",
    "                    \"source\": {\"bytes\": base64_string},\n",
    "                }\n",
    "            },\n",
    "            {\"text\": \"Identify the main objects in the image and Provide art titles for this image.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Configure the inference parameters.\n",
    "inf_params = {\n",
    "    \"maxTokens\": 1000, \n",
    "    \"topP\": 0.9, \n",
    "    \"temperature\": 0.7,\n",
    "    \"reasoningConfig\": {\n",
    "        \"type\": \"enabled\",\n",
    "        \"maxReasoningEffort\": \"low\"\n",
    "    }\n",
    "} \n",
    "\n",
    "json_output = sync_nova_2_model_invocation(client=client, \n",
    "                                            modelId=modelId,\n",
    "                                            system_list=system_list,\n",
    "                                            message_list=message_list,\n",
    "                                            inf_params=inf_params)\n",
    "print(json_output['output']['message']['content'][1]['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fdc7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Another example of image understanding \n",
    "\n",
    "# Open the image you'd like to use and encode it as a Base64 string.\n",
    "with open(\"media/nutritional_benifits.png\", \"rb\") as image_file:\n",
    "    binary_data = image_file.read()\n",
    "    base_64_encoded_data = base64.b64encode(binary_data)\n",
    "    base64_string = base_64_encoded_data.decode(\"utf-8\")\n",
    "\n",
    "# Define your system prompt(s).\n",
    "system_list = [\n",
    "    { \"text\": \"You are an expert in extracing text from an image\" }\n",
    "]\n",
    "\n",
    "# Define a \"user\" message including both the image and a text prompt.\n",
    "message_list = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"image\": {\n",
    "                    \"format\": \"png\",\n",
    "                    \"source\": {\"bytes\": base64_string},\n",
    "                }\n",
    "            },\n",
    "            {\"text\": \"Read the text from the image and list the contents as a json payload\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Configure the inference parameters.\n",
    "inf_params = {\n",
    "    \"maxTokens\": 10000, \n",
    "    \"topP\": 0.9, \n",
    "    \"temperature\": 0.7,\n",
    "    \"reasoningConfig\": {\n",
    "        \"type\": \"enabled\",\n",
    "        \"maxReasoningEffort\": \"medium\"\n",
    "    }\n",
    "} \n",
    "\n",
    "json_output = sync_nova_2_model_invocation(client=client, \n",
    "                                            modelId=modelId,\n",
    "                                            system_list=system_list,\n",
    "                                            message_list=message_list,\n",
    "                                            inf_params=inf_params)\n",
    "print(json_output['output']['message']['content'][1]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309ffbd5-776c-4dfb-bbea-b8a16660ec49",
   "metadata": {},
   "source": [
    "#### 4.2 Multi-Image Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12c7267",
   "metadata": {},
   "source": [
    "There can be multiple image contents. In this example we ask the model to find what two images have in common:\n",
    "\n",
    "![](media/kites_and_plane.png)\n",
    "\n",
    "![](media/kites_and_plane2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b13815",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Open the image you'd like to use and encode it as a Base64 string.\n",
    "with open(\"media/kites_and_plane.png\", \"rb\") as image_file:\n",
    "    binary_data = image_file.read()\n",
    "    base_64_encoded_data = base64.b64encode(binary_data)\n",
    "    image1_base64_string = base_64_encoded_data.decode(\"utf-8\")\n",
    "\n",
    "with open(\"media/kites_and_plane2.png\", \"rb\") as image_file:\n",
    "    binary_data = image_file.read()\n",
    "    base_64_encoded_data = base64.b64encode(binary_data)\n",
    "    image2_base64_string = base_64_encoded_data.decode(\"utf-8\")\n",
    "\n",
    "# Define your system prompt(s).\n",
    "system_list = [\n",
    "    { \"text\": \"You are an expert artist and very good at identifying objects in an image,\" }\n",
    "]\n",
    "\n",
    "\n",
    "# Define a \"user\" message including both the image and a text prompt.\n",
    "message_list = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"image\": {\n",
    "                    \"format\": \"png\",\n",
    "                    \"source\": {\"bytes\": image1_base64_string},\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"image\": {\n",
    "                    \"format\": \"png\",\n",
    "                    \"source\": {\"bytes\": image2_base64_string},\n",
    "                }\n",
    "            },\n",
    "            {\"text\": \"What do these two images have in common?\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Configure the inference parameters.\n",
    "inf_params = {\n",
    "    \"maxTokens\": 1000, \n",
    "    \"topP\": 0.9, \n",
    "    \"temperature\": 0.7,\n",
    "    \"reasoningConfig\": {\n",
    "        \"type\": \"enabled\",\n",
    "        \"maxReasoningEffort\": \"low\"\n",
    "    }\n",
    "} \n",
    "\n",
    "\n",
    "json_output = sync_nova_2_model_invocation(client=client, \n",
    "                                            modelId=modelId,\n",
    "                                            system_list=system_list,\n",
    "                                            message_list=message_list,\n",
    "                                            inf_params=inf_params)\n",
    "print(json_output['output']['message']['content'][1]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cea383-4609-4b9d-b7a2-ff478b9992f2",
   "metadata": {},
   "source": [
    "#### 4.3 Image Understanding using S3 Path\n",
    "\n",
    "Replace the S3 URI below with the S3 URI where your image is located"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbad80b-d005-4090-ad20-4db795671f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define your system prompt(s).\n",
    "system_list = [\n",
    "    { \n",
    "        \"text\": \"You are an expert artist. When the user provides you with an image, provide 3 potential art titles\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Define a \"user\" message including both the image and a text prompt.\n",
    "message_list = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"image\": {\n",
    "                    \"format\": \"png\",\n",
    "                    \"source\": {\n",
    "                        \"s3Location\": {\n",
    "                            # Replace the S3 URI\n",
    "                            \"uri\": \"s3://s3-demo-bucket-nova-2/bluesky.png\"\n",
    "                        }\n",
    "                    },\n",
    "                }\n",
    "            },\n",
    "            {\"text\": \"Provide 3 potential art titles for this image.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Configure the inference parameters.\n",
    "inf_params = {\n",
    "    \"maxTokens\": 1000, \n",
    "    \"topP\": 0.9, \n",
    "    \"temperature\": 0.7,\n",
    "    \"reasoningConfig\": {\n",
    "        \"type\": \"enabled\",\n",
    "        \"maxReasoningEffort\": \"low\"\n",
    "    }\n",
    "} \n",
    "\n",
    "\n",
    "json_output = sync_nova_2_model_invocation(client=client, \n",
    "                                            modelId=modelId,\n",
    "                                            system_list=system_list,\n",
    "                                            message_list=message_list,\n",
    "                                            inf_params=inf_params)\n",
    "print(json_output['output']['message']['content'][1]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2976fe-29dc-477b-b27a-758f8f450a05",
   "metadata": {},
   "source": [
    "### 5 Video Understanding\n",
    "\n",
    "The Amazon Nova models allow you to include a single video in the payload, which can be provided either in base64 format or through an Amazon S3 URI. When using the base64 method, the overall payload size must remain within 25MB. However, you can specify an Amazon S3 URI for video understanding. This approach enables you to leverage the model for longer videos (up to 1GB in size) without being constrained by the overall payload size limitation. Amazon Nova models can analyze the passed video and answer questions, classify a video, and summarize information in the video based on provided instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cad3f1",
   "metadata": {},
   "source": [
    "#### 5.1 Video Understanding using local file Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f9a327-79b2-472b-911e-735546aa51bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "Video(\"media/ducks_in_pond.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9253bf6e-b2dc-41ef-ae5f-b06e35568eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Open the image you'd like to use and encode it as a Base64 string.\n",
    "with open(\"media/ducks_in_pond.mp4\", \"rb\") as video_file:\n",
    "    binary_data = video_file.read()\n",
    "    base_64_encoded_data = base64.b64encode(binary_data)\n",
    "    base64_string = base_64_encoded_data.decode(\"utf-8\")\n",
    "\n",
    "# Define your system prompt(s).\n",
    "system_list = [\n",
    "    { \"text\": \"You are an expert media analyst. When the user provides you with a video, You identify objects of interest in the image\" }\n",
    "]\n",
    "\n",
    "# Define a \"user\" message including both the image and a text prompt.\n",
    "message_list = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"video\": {\n",
    "                    \"format\": \"mp4\",\n",
    "                    \"source\": {\"bytes\": base64_string},\n",
    "                }\n",
    "            },\n",
    "            {\"text\": \"identify and describe the objects of interest in less than 300 words.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Configure the inference parameters.\n",
    "inf_params = {\n",
    "    \"maxTokens\": 2048, \n",
    "    \"topP\": 0.9, \n",
    "    \"temperature\": 0.7,\n",
    "    \"reasoningConfig\": {\n",
    "        \"type\": \"enabled\",\n",
    "        \"maxReasoningEffort\": \"low\"\n",
    "    }\n",
    "} \n",
    "\n",
    "json_output = sync_nova_2_model_invocation(client=client, \n",
    "                                            modelId=modelId,\n",
    "                                            system_list=system_list,\n",
    "                                            message_list=message_list,\n",
    "                                            inf_params=inf_params)\n",
    "print(json_output['output']['message']['content'][1]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a29615-4915-4475-8511-fff08eeb97ad",
   "metadata": {},
   "source": [
    "#### 5.2 Video Understanding using S3 Path\n",
    "Replace the S3 URI below with the S3 URI where your video is located"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2b1bc4-3e98-46ed-ab0c-a68e90f7c541",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define your system prompt(s).\n",
    "system_list = [\n",
    "    { \"text\": \"You are an expert media analyst. When the user provides you with a video, provide 3 potential video titles\" }\n",
    "]\n",
    "\n",
    "# Define a \"user\" message including both the image and a text prompt.\n",
    "message_list = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"video\": {\n",
    "                    \"format\": \"mp4\",\n",
    "                    \"source\": {\n",
    "                        \"s3Location\": {\n",
    "                            # Replace the S3 URI\n",
    "                            \"uri\": \"s3://s3-demo-bucket-nova-2/the-sea.mp4\"\n",
    "                        }\n",
    "                    },\n",
    "                }\n",
    "            },\n",
    "            {\"text\": \"Provide video titles for this clip.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Configure the inference parameters.\n",
    "inf_params = {\n",
    "    \"maxTokens\": 1000, \n",
    "    \"topP\": 0.9, \n",
    "    \"temperature\": 0.7,\n",
    "    \"reasoningConfig\": {\n",
    "        \"type\": \"enabled\",\n",
    "        \"maxReasoningEffort\": \"low\"\n",
    "    }\n",
    "} \n",
    "\n",
    "native_request = {\n",
    "    \"messages\": message_list,\n",
    "    \"system\": system_list,\n",
    "    \"inferenceConfig\": inf_params,\n",
    "}\n",
    "\n",
    "json_output = sync_nova_2_model_invocation(client=client, \n",
    "                                            modelId=modelId,\n",
    "                                            system_list=system_list,\n",
    "                                            message_list=message_list,\n",
    "                                            inf_params=inf_params)\n",
    "print(json_output['output']['message']['content'][1]['text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".nova-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
