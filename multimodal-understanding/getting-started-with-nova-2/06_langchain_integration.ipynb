{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d932ae5-c2b1-4de8-865f-aa28509c33ee",
   "metadata": {},
   "source": [
    "## Langchain/LangGraph with Nova Models\n",
    "\n",
    "To aid in the development of applications with Nova models; Langchain support has been added for multimodal and agentic workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7419d32-d280-41a9-922a-c210eae828b9",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0cdbc4-698b-4992-9468-a3152d40edd0",
   "metadata": {},
   "source": [
    "#### Install python libraries\n",
    "\n",
    "Run the cells in this section to install the packages needed by the notebooks in this workshop. ⚠️ You will see pip dependency errors, you can safely ignore these errors. ⚠️\n",
    "\n",
    "_IGNORE ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e090009-940a-4f23-a9b9-141542376c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q --upgrade langchain langchain_aws langchain_community matplotlib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdd20c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42115341-e153-45f8-9d3e-d2971f722c4b",
   "metadata": {},
   "source": [
    "## Model Invocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e03481-0e86-475d-b84a-dba277ba7eb4",
   "metadata": {},
   "source": [
    "#### Text Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aabe9a5-c1c5-4fcc-bfc0-822bbbc50119",
   "metadata": {},
   "outputs": [],
   "source": [
    "LITE_MODEL_ID = \"us.amazon.nova-2-lite-v1:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2138ae",
   "metadata": {},
   "source": [
    "## ChatBedrockConverse() API allows you to use Bedrock hosted LLM and create chat type interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f0bb9a-df70-484e-9823-1b474363678b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrockConverse\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "llm = ChatBedrockConverse(\n",
    "    model_id=LITE_MODEL_ID,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    (\"system\", \"Provide three alternative song titles for a given user title\"),\n",
    "    (\"user\", \"Teardrops on My Guitar\"),\n",
    "]\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "print(f\"Request ID: {response.id}\")\n",
    "response.pretty_print()\n",
    "\n",
    "\n",
    "# Here we can pass the chat history to the model to ask follow up questions\n",
    "multi_turn_messages = [\n",
    "    *messages,\n",
    "    response,\n",
    "    HumanMessage(content=\"Select your favorite and tell me why\"),\n",
    "]\n",
    "\n",
    "response = llm.invoke(multi_turn_messages)\n",
    "print(f\"\\n\\nRequest ID: {response.id}\")\n",
    "response.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31e2084-5240-4a4f-a2e5-56ed303fb5ac",
   "metadata": {},
   "source": [
    "#### Image Understanding\n",
    "\n",
    "You are able to pass various media types to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766bbdff-bdf5-4d5d-ab5b-a324622d40f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "image_path = \"media/sunset.png\"\n",
    "Image(filename=image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e552f1-40c1-4921-9668-44da4e079aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "from langchain_aws import ChatBedrockConverse\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "llm = ChatBedrockConverse(\n",
    "    model=LITE_MODEL_ID,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "with open(image_path, \"rb\") as image_file:\n",
    "    binary_data = image_file.read()\n",
    "\n",
    "message = HumanMessage(\n",
    "    content=[\n",
    "        {\"image\": {\"format\": \"png\", \"source\": {\"bytes\": binary_data}}},\n",
    "        {\"text\": \"Provide a summary of this photo\"},\n",
    "    ]\n",
    ")\n",
    "\n",
    "response = llm.invoke([message])\n",
    "print(f\"\\n\\nRequest ID: {response.id}\")\n",
    "response.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884decd3-7ea1-47b4-bff4-188d412664fb",
   "metadata": {},
   "source": [
    "#### Video Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee15b13-6064-4a31-98a1-d067edb8bacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"media/the-sea.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401e61fc-f354-4421-a46d-522d10d1a2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrockConverse\n",
    "\n",
    "llm = ChatBedrockConverse(\n",
    "    model=LITE_MODEL_ID,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "with open(video_path, \"rb\") as video_file:\n",
    "    binary_data = video_file.read()\n",
    "\n",
    "\n",
    "message = HumanMessage(\n",
    "    content=[\n",
    "        {\"video\": {\"format\": \"mp4\", \"source\": {\"bytes\": binary_data}}},\n",
    "        {\"type\": \"text\", \"text\": \"Describe the following video\"},\n",
    "    ]\n",
    ")\n",
    "\n",
    "response = llm.invoke([message])\n",
    "print(f\"\\n\\nRequest ID: {response.id}\")\n",
    "response.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace6d076-16dd-4285-b8cc-862f1c6e0e0d",
   "metadata": {},
   "source": [
    "#### Streaming\n",
    "\n",
    "Streaming is also supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f1db1f-01a4-43da-af72-a3a50954a8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrockConverse\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatBedrockConverse(\n",
    "    model=LITE_MODEL_ID,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "chain = llm | StrOutputParser()\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are an author with experience writing creative novels\"),\n",
    "    HumanMessage(\n",
    "        content=\"Write an outline for a novel about a wizard named Theodore graduating from college\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "for chunk in chain.stream(messages):\n",
    "    print(chunk, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fb633d-a49c-4a4b-9869-0fdb995ca8dc",
   "metadata": {},
   "source": [
    "## Agent Workflows\n",
    "\n",
    "The Nova model is capable of handling tool calling and agentic workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dbb61d-ec8e-489a-a012-fad5c6a35341",
   "metadata": {},
   "source": [
    "#### Binding Tools\n",
    "\n",
    "When using a model for tool calling you can take advantage of the bind tools method. This will pass a formatted tool config to the model. We recommend when taking advantage of tool calling or agentic workflows to use greedy decoding values. This means temperature=1, topP=1, topK=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288f6941-ebc7-45b0-b6dc-9fe201cc2197",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrockConverse\n",
    "from langchain_core.messages import ToolMessage\n",
    "import json\n",
    "\n",
    "def multiply_numbers(a: float, b: float) -> float:\n",
    "    \"\"\"Multiply two numbers\n",
    "        a: float, \"First number to multiply\"\n",
    "        b: float, \"Second number to multiply\"\n",
    "        returns multiplication of a and b (a * b)\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "    \n",
    "\n",
    "tools = [multiply_numbers]\n",
    "\n",
    "llm_with_tools = ChatBedrockConverse(\n",
    "    model=LITE_MODEL_ID,\n",
    "    temperature=1,\n",
    "    top_p=1,\n",
    "    additional_model_request_fields={\n",
    "        \"inferenceConfig\": {\n",
    "            \"topK\": 1\n",
    "        }\n",
    "    },\n",
    ").bind_tools(tools)\n",
    "\n",
    "messages = [(\"user\", \"What is 8*9.5\")]\n",
    "response = llm_with_tools.invoke(messages)\n",
    "print(\"=\" * 80)\n",
    "print(\"[Model Response]\\n\")\n",
    "print(json.dumps(response.content, indent=2))\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n[Tool Calls]\\n\")\n",
    "print(json.dumps(response.tool_calls, indent=2))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Handle the Tool Call and Execute the Tool\n",
    "if response.tool_calls:\n",
    "    for tool_call in response.tool_calls:\n",
    "        if tool_call['name'] == \"multiply_numbers\":\n",
    "            result = multiply_numbers(tool_call['args']['a'], tool_call['args']['b'])\n",
    "            # Send the tool's output back to the model\n",
    "            tool_message = ToolMessage(content=str(result), tool_call_id=tool_call['id'])\n",
    "            follow_up_response = llm_with_tools.invoke(messages + [response]+ [tool_message])\n",
    "            print(\"=\" * 80)\n",
    "            print(follow_up_response.content)\n",
    "            print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6f0cca-d9d8-4b1e-93f5-e3b8321ea754",
   "metadata": {},
   "source": [
    "#### Tool Calling Agents\n",
    "\n",
    "For full workflows you can take advantage of custom parsers that will intercept outputs of the stream and allow you to invoke tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e906d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import tools\n",
    "from langchain.tools import tool\n",
    "from langchain_classic.agents import AgentExecutor \n",
    "from langchain_classic.agents import create_tool_calling_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_aws import ChatBedrockConverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e07d53-8381-44cb-a7de-b6e3ffcd1ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## AgentExecutor() api is being deprecated. Use with Caution.\n",
    "## Recommended pattern is to use LangGraph \n",
    " \n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "tools = [multiply]\n",
    "\n",
    "llm_with_tools = ChatBedrockConverse(\n",
    "    model=LITE_MODEL_ID,\n",
    "    temperature=1,\n",
    "    top_p=1,\n",
    "    additional_model_request_fields={\n",
    "        \"inferenceConfig\": {\n",
    "            \"topK\": 1\n",
    "        }\n",
    "    },\n",
    ").bind_tools(tools)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "agent_executor.invoke({\"input\": \"What is 2*2?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e1ebb6",
   "metadata": {},
   "source": [
    "## Structured Output\n",
    "\n",
    "Structured output is a great way to force the model to return in a specific way. We use Greedy Decoding params here for more determinist results (Temperature = 1, Top P = 1, Top K = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7841a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_aws import ChatBedrockConverse\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    \"\"\"A joke to respond to the user\"\"\"\n",
    "    setup: str = Field(description=\"The setup of the joke\")\n",
    "    punchline: str = Field(description=\"The punchline to the joke\")\n",
    "\n",
    "llm = ChatBedrockConverse(\n",
    "    model=LITE_MODEL_ID,\n",
    "    temperature=1,\n",
    "    top_p=1,\n",
    "    additional_model_request_fields={\n",
    "        \"inferenceConfig\": {\n",
    "            \"topK\": 1\n",
    "        }\n",
    "    },\n",
    ")\n",
    "structured_llm = llm.with_structured_output(Joke)\n",
    "structured_llm.invoke(\"Tell me a joke about cats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b470a466",
   "metadata": {},
   "source": [
    "## A very simple Langchain Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332c2081",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "import json\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Get weather for a given city.\"\"\"\n",
    "    return f\"Test Response: It's always sunny in {city}!\"\n",
    "\n",
    "MODEL_ID = \"amazon.nova-2-lite-v1:0\"\n",
    "agent = create_agent(\n",
    "    model=MODEL_ID,\n",
    "    tools=[get_weather],\n",
    "    system_prompt=\"You are a helpful assistant\",\n",
    ")\n",
    "\n",
    "# Run the agent\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n",
    ")\n",
    "\n",
    "response['messages'][3].content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4350e563",
   "metadata": {},
   "source": [
    "## Create an agent using LangGraph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c6f9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries and methods\n",
    "from typing import List, Literal\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_aws import ChatBedrockConverse\n",
    "\n",
    "llm = ChatBedrockConverse(\n",
    "    model=LITE_MODEL_ID,\n",
    "    temperature=1,\n",
    "    top_p=1,\n",
    "    additional_model_request_fields={\n",
    "        \"inferenceConfig\": {\n",
    "            \"topK\": 1\n",
    "        }\n",
    "    },\n",
    ")\n",
    "\n",
    "@tool\n",
    "def get_weather(city: str) -> list:\n",
    "    \"\"\" get the current weather\"\"\"\n",
    "    if str is not None:\n",
    "        \"\"\"Get weather for a given city.\"\"\"\n",
    "        return f\"Custom Tool Response: It's always rainy in {city}!\"\n",
    "    else:\n",
    "        return \"Weather Data Not Found\"\n",
    "    \n",
    "tools = [get_weather]\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    messages = state[\"messages\"]\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def call_tools(state: MessagesState) -> Literal[\"tools\", END]:\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    if last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    return END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b941895d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize nodes and edges\n",
    "\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "# add a node named LLM, with call_model function. This node uses an LLM to make decisions based on the input given\n",
    "workflow.add_node(\"LLM\", call_model)\n",
    "\n",
    "# Our workflow starts with the LLM node\n",
    "workflow.add_edge(START, \"LLM\")\n",
    "\n",
    "# Add a tools node\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "\n",
    "# Add a conditional edge from LLM to call_tools function. It can go tools node or end depending on the output of the LLM. \n",
    "workflow.add_conditional_edges(\"LLM\", call_tools)\n",
    "\n",
    "# tools node sends the information back to the LLM\n",
    "workflow.add_edge(\"tools\", \"LLM\")\n",
    "\n",
    "agent = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b933b7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "# Generate the image \n",
    "display(Image(agent.get_graph().draw_mermaid_png()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5626b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in agent.stream(\n",
    "    {\"messages\": [(\"user\", \"Will it rain in Seattle today?\")]},\n",
    "    stream_mode=\"values\",):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".nova-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
