{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Strands Agents and Amazon Nova 2.0 Lite\n",
    "\n",
    "Build AI agents using the Strands framework with Amazon Nova 2.0 Lite. This notebook covers agent fundamentals, tools, and practical applications.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [**Installation and Setup**](#1.-Installation-and-Setup) — Environment configuration and API key authentication\n",
    "2. [**Creating Your First Agent**](#2.-Creating-Your-First-Agent) — Basic agent creation with Hybrid Reasoning\n",
    "3. [**Extending Agent Capabilities with Tools**](#3.-Extending-Agent-Capabilities-with-Tools) — System Tools, Community Tools, Custom Tools, and MCP\n",
    "4. [**Multimodal Content Processing**](#4.-Multimodal-Content-Processing) — Working with images and documents\n",
    "5. [**Streaming Responses**](#5.-Streaming-Responses) — Real-time response generation\n",
    "6. [**Structured Output**](#6.-Structured-Output) — Type-safe responses with Pydantic models\n",
    "7. [**Configuration and Best Practices**](#7.-Configuration-and-Best-Practices) — Model tuning and error handling\n",
    "8. [**Practical Example: Research Assistant**](#8.-Practical-Example:-Research-Assistant) — Building a complete application\n",
    "\n",
    "## What is an AI Agent?\n",
    "\n",
    "An **AI agent** is a program that understands natural language, makes decisions, and uses tools to perform tasks—like searching the web, analyzing data, or interacting with external services.\n",
    "\n",
    "## About Amazon Nova 2.0 Lite\n",
    "\n",
    "**Amazon Nova 2.0 Lite** is a fast, cost-effective multimodal model that processes text, images, and documents. Key features include **Hybrid Reasoning** for improved accuracy and **System Tools** for built-in capabilities like web search and code execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation and Setup\n",
    "\n",
    "First, let's install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Strands Agents and community tools\n",
    "!pip install strands-agents strands-agents-tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWS Setup Requirements\n",
    "\n",
    "Before using Nova 2.0 Lite, you need:\n",
    "\n",
    "1. **AWS Account** with Amazon Bedrock access\n",
    "2. **IAM Permissions** - Appropriate permissions to invoke Bedrock models\n",
    "3. **Amazon Bedrock API Key** - Generate an API key for authentication\n",
    "\n",
    "#### Step 1: Verify Model Access\n",
    "\n",
    "Access to Amazon Bedrock foundation models, including Amazon Nova 2.0 Lite, is enabled by default in all commercial AWS regions.\n",
    "\n",
    "**To get started:**\n",
    "1. Sign in to the [AWS Management Console](https://console.aws.amazon.com/)\n",
    "2. Navigate to the [Amazon Bedrock console](https://console.aws.amazon.com/bedrock/)\n",
    "3. Make sure you're in the **US East (N. Virginia)** region (us-east-1)\n",
    "4. Select **Nova 2.0 Lite** from the Model Catalog and open it in the Playground, or invoke it directly via API\n",
    "\n",
    "**IAM Permissions Required:**\n",
    "- `bedrock:InvokeModel` — Required to call models\n",
    "- `bedrock:InvokeTool` — Required for System Tools (web search, code interpreter)\n",
    "- `aws-marketplace:Subscribe` — One-time setup for AWS Marketplace models\n",
    "\n",
    "> **Note**: Once a model is enabled in your account, all users in the account can invoke it without needing Marketplace permissions.\n",
    "\n",
    "#### Step 2: Generate an Amazon Bedrock API Key\n",
    "\n",
    "Amazon Bedrock API keys provide a simple way to authenticate without complex credential configuration. For this tutorial, we'll create a **long-term API key** that expires in 30 days:\n",
    "\n",
    "1. In the [Amazon Bedrock console](https://console.aws.amazon.com/bedrock/), select **API keys** from the left navigation pane\n",
    "2. In the **Long-term API keys** tab, click **Generate long-term API keys**\n",
    "3. Under **API key expiration**, select **30 days**\n",
    "4. Click **Generate**\n",
    "5. **Important**: Copy your API key immediately and store it securely - it will only be shown once!\n",
    "\n",
    "> **Note**: Long-term API keys are recommended for exploration and development only. For production applications, use short-term API keys or IAM roles for enhanced security.\n",
    "\n",
    "#### Step 3: Configure Your API Key\n",
    "\n",
    "Once you have your API key, you'll set it as an environment variable that the Strands framework will automatically use for authentication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set your Amazon Bedrock API key here\n",
    "# Replace 'your-api-key-here' with the API key you generated from the Bedrock console\n",
    "os.environ['AWS_BEARER_TOKEN_BEDROCK'] = 'your-api-key-here'\n",
    "\n",
    "# The Strands framework uses boto3 under the hood, which automatically detects\n",
    "# this environment variable and uses it for authentication with Amazon Bedrock.\n",
    "\n",
    "print(\"Amazon Bedrock API key configured!\")\n",
    "print(\"Make sure you've replaced 'your-api-key-here' with your actual API key.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Set the API key in your terminal before running Jupyter\n",
    "# This keeps your API key out of your notebook code\n",
    "\n",
    "# For macOS/Linux, run this in your terminal:\n",
    "# export AWS_BEARER_TOKEN_BEDROCK=\"your-api-key-here\"\n",
    "\n",
    "# For Windows (Command Prompt):\n",
    "# setx AWS_BEARER_TOKEN_BEDROCK \"your-api-key-here\"\n",
    "\n",
    "# For Windows (PowerShell):\n",
    "# $env:AWS_BEARER_TOKEN_BEDROCK = \"your-api-key-here\"\n",
    "\n",
    "# Then start Jupyter notebook from the same terminal session.\n",
    "\n",
    "# Verify your API key is set (this will show if the environment variable exists)\n",
    "import os\n",
    "api_key = os.environ.get('AWS_BEARER_TOKEN_BEDROCK', '')\n",
    "if api_key and api_key != 'your-api-key-here':\n",
    "    print(\"API key is configured!\")\n",
    "else:\n",
    "    print(\"Warning: API key not set or still using placeholder value.\")\n",
    "    print(\"Please set your API key in the cell above or via environment variable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating Your First Agent\n",
    "\n",
    "Let's create a simple agent using Amazon Nova 2.0 Lite with **Hybrid Reasoning** enabled.\n",
    "\n",
    "### What is Hybrid Reasoning?\n",
    "\n",
    "Hybrid Reasoning is a powerful feature of Nova 2.0 Lite that allows the model to \"think through\" problems before providing a response. When enabled, the model generates internal reasoning steps that help it arrive at more accurate and well-considered answers.\n",
    "\n",
    "To enable reasoning, you configure it using the `additional_request_fields` parameter in the `BedrockModel`. The reasoning configuration has two key settings:\n",
    "\n",
    "- **type**: Set to `\"enabled\"` to turn on reasoning (default is `\"disabled\"`)\n",
    "- **maxReasoningEffort**: Controls how deeply the model should think through problems. Options are:\n",
    "  - `\"low\"` - Minimal reasoning, faster responses\n",
    "  - `\"medium\"` - Balanced reasoning depth\n",
    "  - `\"high\"` - Deep reasoning for complex problems\n",
    "\n",
    "The `maxReasoningEffort` parameter sets a limit on how many tokens the model uses in its reasoning process. Always start with `\"low\"` and increase only if you need more accuracy for complex tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from strands import Agent\n",
    "from strands.models import BedrockModel\n",
    "\n",
    "# Create Nova 2.0 Lite model with Hybrid Reasoning enabled\n",
    "nova_lite = BedrockModel(\n",
    "    model_id=\"amazon.nova-2-lite-v1:0\",  # Nova 2.0 Lite model ID\n",
    "    temperature=0.0,  # Controls randomness (0.0 = deterministic, 1.0 = very random)\n",
    "    region_name=\"us-east-1\",  # Nova 2.0 Lite is available in us-east-1\n",
    "    additional_request_fields={\n",
    "        \"reasoningConfig\": {\n",
    "            \"type\": \"enabled\",\n",
    "            \"maxReasoningEffort\": \"low\"\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create your first agent\n",
    "agent = Agent(\n",
    "    model=nova_lite,\n",
    "    name=\"My First Agent\",\n",
    "    system_prompt=\"You are a helpful AI assistant. Be concise and friendly.\"\n",
    ")\n",
    "\n",
    "print(\"Agent created successfully with Hybrid Reasoning enabled!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have your first conversation\n",
    "response = agent(\"Hello! Can you explain what you are in simple terms?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a follow-up question\n",
    "response = agent(\"What can you help me with?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extending Agent Capabilities with Tools\n",
    "\n",
    "Tools are functions that agents can call to perform specific tasks, extending capabilities beyond text generation.\n",
    "\n",
    "### Types of Tools\n",
    "\n",
    "| Tool Type | Description | Execution |\n",
    "|-----------|-------------|-----------|\n",
    "| **System Tools** | Built into Nova 2.0 Lite (`nova_grounding`, `nova_code_interpreter`) | Model executes internally |\n",
    "| **Community Tools** | Pre-built tools from `strands-agents-tools` | Agent framework executes |\n",
    "| **Custom Tools** | Your Python functions with `@tool` decorator | Agent framework executes |\n",
    "| **MCP Tools** | External tools via Model Context Protocol servers | Agent framework executes |\n",
    "\n",
    "**Key distinction:** System tools are unique to Nova 2.0—the model invokes and executes them internally during reasoning. All other tools follow the standard pattern where the agent framework handles execution based on the model's tool call requests.\n",
    "\n",
    "### Configuration Parameters\n",
    "\n",
    "When configuring `BedrockModel`, two parameters control different aspects:\n",
    "\n",
    "- **`additional_request_fields`** — Configures model behavior (e.g., `reasoningConfig` for Hybrid Reasoning)\n",
    "- **`additional_args`** — Configures API-level features (e.g., `toolConfig` for System Tools)\n",
    "\n",
    "These are used together when you want both reasoning and system tools enabled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System Tools\n",
    "\n",
    "System tools are model-managed capabilities that Nova 2.0 Lite can invoke directly during reasoning. No external code required—results are automatically incorporated into responses.\n",
    "\n",
    "**Available system tools:**\n",
    "- **`nova_grounding`** — Real-time web search with citations\n",
    "- **`nova_code_interpreter`** — Python code execution\n",
    "\n",
    "**Requirement:** IAM role needs `bedrock:InvokeTool` permission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Web Search (Nova Grounding)\n",
    "\n",
    "Enables real-time web search for current events and live data. The model formulates queries, executes searches, and incorporates results with citations.\n",
    "\n",
    "```python\n",
    "additional_args={\n",
    "    \"toolConfig\": {\n",
    "        \"tools\": [\n",
    "            {\n",
    "                \"systemTool\": {\n",
    "                    \"name\": \"nova_grounding\"\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model with web search (Nova Grounding) enabled\n",
    "nova_with_web_search = BedrockModel(\n",
    "    model_id=\"amazon.nova-2-lite-v1:0\",\n",
    "    temperature=0.7,\n",
    "    region_name=\"us-east-1\",\n",
    "    max_tokens=10000,  # Set higher max tokens to accommodate web search results\n",
    "    additional_request_fields={\n",
    "        \"reasoningConfig\": {\n",
    "            \"type\": \"enabled\",\n",
    "            \"maxReasoningEffort\": \"low\"\n",
    "        }\n",
    "    },\n",
    "    # Enable the nova_grounding system tool for web search\n",
    "    additional_args={\n",
    "        \"toolConfig\": {\n",
    "            \"tools\": [\n",
    "                {\n",
    "                    \"systemTool\": {\n",
    "                        \"name\": \"nova_grounding\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create an agent with web search capability\n",
    "web_search_agent = Agent(\n",
    "    model=nova_with_web_search,\n",
    "    system_prompt=\"You are a helpful assistant with access to real-time web search. Use web search to find current information when needed.\"\n",
    ")\n",
    "\n",
    "print(\"Web search agent created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the web search agent with a question about current events\n",
    "# The model will automatically search the web to find up-to-date information\n",
    "response = web_search_agent(\"What are the top AI announcements from the past week?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code Interpreter\n",
    "\n",
    "Enables Python code execution for calculations, data analysis, and algorithmic tasks. The model writes code, executes it, and uses results (stdout/stderr) in its response. Errors are visible to the model for automatic debugging.\n",
    "\n",
    "```python\n",
    "additional_args={\n",
    "    \"toolConfig\": {\n",
    "        \"tools\": [\n",
    "            {\n",
    "                \"systemTool\": {\n",
    "                    \"name\": \"nova_code_interpreter\"\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**Note:** Code execution may increase response time—consider higher timeout values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model with Code Interpreter enabled\n",
    "nova_with_code_interpreter = BedrockModel(\n",
    "    model_id=\"amazon.nova-2-lite-v1:0\",\n",
    "    temperature=0.7,\n",
    "    region_name=\"us-east-1\",\n",
    "    max_tokens=10000,  # Set higher max tokens to accommodate code execution results\n",
    "    additional_request_fields={\n",
    "        \"reasoningConfig\": {\n",
    "            \"type\": \"enabled\",\n",
    "            \"maxReasoningEffort\": \"low\"\n",
    "        }\n",
    "    },\n",
    "    # Enable the nova_code_interpreter system tool\n",
    "    additional_args={\n",
    "        \"toolConfig\": {\n",
    "            \"tools\": [\n",
    "                {\n",
    "                    \"systemTool\": {\n",
    "                        \"name\": \"nova_code_interpreter\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create an agent with code interpreter capability\n",
    "code_interpreter_agent = Agent(\n",
    "    model=nova_with_code_interpreter,\n",
    "    system_prompt=\"You are a helpful assistant that can write and execute Python code to solve problems.\"\n",
    ")\n",
    "\n",
    "print(\"Code interpreter agent created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the code interpreter with a complex calculation\n",
    "# The model will write and execute Python code to solve this problem\n",
    "response = code_interpreter_agent(\n",
    "    \"Calculate how many seconds are in a leap year, and what percentage more that is compared to a regular year.\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Community Tools\n",
    "\n",
    "The `strands-agents-tools` package provides pre-built tools including calculators, time functions, HTTP requests, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some useful tools from the community package\n",
    "from strands_tools import calculator, current_time\n",
    "\n",
    "# Create an agent with tools\n",
    "agent_with_tools = Agent(\n",
    "    model=nova_lite,\n",
    "    tools=[calculator, current_time],\n",
    "    system_prompt=\"You are a helpful assistant with access to a calculator and current time.\"\n",
    ")\n",
    "\n",
    "print(\"Agent with tools created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the calculator tool\n",
    "response = agent_with_tools(\"What's 15 * 23 + 47?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the current time tool\n",
    "response = agent_with_tools(\"What time is it right now?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Custom Tools\n",
    "\n",
    "You can create your own tools using the `@tool` decorator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from strands import tool\n",
    "\n",
    "@tool\n",
    "def weather_info(city: str) -> str:\n",
    "    \"\"\"Get weather information for a city.\n",
    "    \n",
    "    Args:\n",
    "        city: The name of the city\n",
    "    \"\"\"\n",
    "    # This is a mock function - in reality, you'd call a weather API\n",
    "    weather_data = {\n",
    "        \"new york\": \"Sunny, 72°F\",\n",
    "        \"london\": \"Cloudy, 15°C\",\n",
    "        \"tokyo\": \"Rainy, 18°C\",\n",
    "        \"paris\": \"Partly cloudy, 20°C\"\n",
    "    }\n",
    "    \n",
    "    city_lower = city.lower()\n",
    "    if city_lower in weather_data:\n",
    "        return f\"Weather in {city}: {weather_data[city_lower]}\"\n",
    "    else:\n",
    "        return f\"Weather data not available for {city}. Try: New York, London, Tokyo, or Paris.\"\n",
    "\n",
    "@tool\n",
    "def text_analyzer(text: str) -> str:\n",
    "    \"\"\"Analyze text and provide statistics.\n",
    "    \n",
    "    Args:\n",
    "        text: The text to analyze\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    chars = len(text)\n",
    "    chars_no_spaces = len(text.replace(\" \", \"\"))\n",
    "    sentences = text.count('.') + text.count('!') + text.count('?')\n",
    "    \n",
    "    return f\"\"\"Text Analysis:\n",
    "- Words: {len(words)}\n",
    "- Characters: {chars}\n",
    "- Characters (no spaces): {chars_no_spaces}\n",
    "- Sentences: {sentences}\n",
    "- Average word length: {chars_no_spaces / len(words):.1f} characters\"\"\"\n",
    "\n",
    "print(\"Custom tools created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an agent with custom tools\n",
    "custom_agent = Agent(\n",
    "    model=nova_lite,\n",
    "    tools=[weather_info, text_analyzer, calculator],\n",
    "    system_prompt=\"You are a helpful assistant with weather, text analysis, and calculation capabilities.\"\n",
    ")\n",
    "\n",
    "# Test the weather tool\n",
    "response = custom_agent(\"What's the weather like in Tokyo?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the text analyzer\n",
    "response = custom_agent(\"Can you analyze this text: 'The quick brown fox jumps over the lazy dog. This sentence contains every letter of the alphabet!'\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCP Tools\n",
    "\n",
    "**Model Context Protocol (MCP)** connects agents to external tool servers for capabilities like documentation lookup, database access, or API integration.\n",
    "\n",
    "**Prerequisites:**\n",
    "- Install `uv` package manager: `pip install uv`\n",
    "- Internet access to download MCP servers on first run\n",
    "- The MCP server runs as a subprocess, so ensure your environment allows spawning processes\n",
    "\n",
    "> **Note:** If the MCP server fails to start, check that `uvx` is available in your PATH and that you have network access. The first run may take longer as it downloads the server package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mcp import stdio_client, StdioServerParameters\n",
    "from strands.tools.mcp import MCPClient\n",
    "\n",
    "# Create MCP client for AWS documentation\n",
    "mcp_client = MCPClient(lambda: stdio_client(\n",
    "    StdioServerParameters(\n",
    "        command=\"uvx\",\n",
    "        args=[\"awslabs.aws-documentation-mcp-server@latest\"]\n",
    "    )\n",
    "))\n",
    "\n",
    "# Use with context manager\n",
    "with mcp_client:\n",
    "    # Get tools from MCP server\n",
    "    mcp_tools = mcp_client.list_tools_sync()\n",
    "    \n",
    "    # Create agent with MCP tools\n",
    "    mcp_agent = Agent(\n",
    "        model=nova_lite,\n",
    "        tools=mcp_tools,\n",
    "        system_prompt=\"You are an AWS expert assistant with access to AWS documentation.\"\n",
    "    )\n",
    "    \n",
    "    # Ask about AWS services\n",
    "    response = mcp_agent(\"What is AWS Lambda?\")\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multimodal Content Processing\n",
    "\n",
    "Nova 2.0 Lite can analyze images alongside text, making it ideal for tasks like architecture diagram review, document analysis, and visual content understanding.\n",
    "\n",
    "**Why use multimodal input?** Instead of manually describing visual content, you can pass images directly to the model for analysis, extraction, or question-answering—saving time and improving accuracy for tasks like diagram interpretation, screenshot analysis, or visual QA.\n",
    "\n",
    "### Image Input Format\n",
    "\n",
    "Images are passed as content blocks using the Bedrock Converse API format. Supported formats include PNG, JPEG, GIF, and WebP, with a maximum payload size of 25 MB.\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"image\": {\n",
    "        \"format\": \"png\",  # png, jpeg, gif, or webp\n",
    "        \"source\": {\n",
    "            \"bytes\": image_bytes\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "Let's demonstrate by analyzing an AWS architecture diagram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a multimodal agent for architecture analysis\n",
    "architecture_agent = Agent(\n",
    "    model=nova_lite,\n",
    "    system_prompt=\"\"\"You are an AWS Solutions Architect assistant. \n",
    "Analyze architecture diagrams to identify components, data flows, and provide recommendations.\"\"\"\n",
    ")\n",
    "\n",
    "# Load the AWS architecture diagram\n",
    "with open(\"deployment_dashboard_architecture.png\", \"rb\") as f:\n",
    "    architecture_image = f.read()\n",
    "\n",
    "# Analyze the architecture diagram\n",
    "response = architecture_agent([\n",
    "    {\n",
    "        \"image\": {\n",
    "            \"format\": \"png\",\n",
    "            \"source\": {\n",
    "                \"bytes\": architecture_image\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"\"\"Analyze this AWS architecture diagram and provide:\n",
    "1. A list of all AWS services shown\n",
    "2. The data flow from user to backend\n",
    "3. Key security components\n",
    "4. One recommendation for improvement\"\"\"\n",
    "    }\n",
    "])\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Streaming Responses\n",
    "\n",
    "**Why use streaming?** For longer responses, streaming displays output as it's generated rather than waiting for the complete response. This improves perceived latency and user experience, especially for complex queries that take several seconds to process.\n",
    "\n",
    "Streaming is particularly useful for:\n",
    "- Chat interfaces where users expect immediate feedback\n",
    "- Long-form content generation (stories, reports, analysis)\n",
    "- Applications where partial results are valuable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def stream_example():\n",
    "    \"\"\"Example of streaming responses from the agent.\"\"\"\n",
    "    print(\"Streaming response:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    async for event in agent.stream_async(\"Tell me a short story about a robot learning to paint.\"):\n",
    "        # Check if this is a text event\n",
    "        if text_event := event.get(\"text_event\"):\n",
    "            print(text_event[\"text\"], end=\"\", flush=True)\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "    print(\"Streaming complete!\")\n",
    "\n",
    "# Run the streaming example\n",
    "await stream_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Structured Output\n",
    "\n",
    "**Why use structured output?** Raw text responses require parsing and validation, which is error-prone. Structured output guarantees the model returns data in a predefined schema, making it reliable for programmatic use—like populating databases, generating API responses, or feeding data into downstream systems.\n",
    "\n",
    "Use Pydantic models to define your expected output structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Define output structure\n",
    "class PersonInfo(BaseModel):\n",
    "    name: str = Field(description=\"Name of the person\")\n",
    "    age: int = Field(description=\"Age of the person\")\n",
    "    occupation: str = Field(description=\"Occupation of the person\")\n",
    "\n",
    "# Get structured output\n",
    "result = agent(\n",
    "    \"John Smith is a 30 year-old software engineer\",\n",
    "    structured_output_model=PersonInfo\n",
    ")\n",
    "\n",
    "person: PersonInfo = result.structured_output\n",
    "print(f\"Name: {person.name}\")\n",
    "print(f\"Age: {person.age}\")\n",
    "print(f\"Job: {person.occupation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Configuration and Best Practices\n",
    "\n",
    "### Model Configuration\n",
    "\n",
    "You can fine-tune the model's behavior using various configuration parameters. One of the most powerful features of Nova 2.0 models is **Hybrid Reasoning**.\n",
    "\n",
    "### Understanding Hybrid Reasoning in Depth\n",
    "\n",
    "Hybrid Reasoning enables the Nova 2.0 model to perform internal \"thinking\" before generating its final response. This is especially useful for tasks that require:\n",
    "\n",
    "- **Complex problem-solving** (math, logic puzzles, multi-step reasoning)\n",
    "- **Careful analysis** (comparing options, weighing trade-offs)\n",
    "- **Accuracy-critical responses** (factual questions, technical explanations)\n",
    "\n",
    "#### How Reasoning Works\n",
    "\n",
    "When reasoning is enabled, the model's response includes two parts:\n",
    "1. **Reasoning Content**: The model's internal thought process (can be accessed separately)\n",
    "2. **Text Response**: The final answer presented to the user\n",
    "\n",
    "#### The `reasoningConfig` Parameter\n",
    "\n",
    "You enable reasoning through the `additional_request_fields` parameter with a `reasoningConfig` object:\n",
    "\n",
    "```python\n",
    "additional_request_fields={\n",
    "    \"reasoningConfig\": {\n",
    "        \"type\": \"enabled\",      # or \"disabled\" (default)\n",
    "        \"maxReasoningEffort\": \"low\"  # Required when type is \"enabled\"\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "#### Understanding `maxReasoningEffort` (Thinking Budget)\n",
    "\n",
    "The `maxReasoningEffort` parameter controls the **reasoning budget** — how many tokens the model allocates to its thinking process. This directly impacts:\n",
    "\n",
    "| Effort Level | Token Budget | Best For | Trade-offs |\n",
    "|--------------|--------------|----------|------------|\n",
    "| `\"low\"` | Minimal tokens | Simple tasks, quick responses | Fastest, but may miss nuances |\n",
    "| `\"medium\"` | Moderate tokens | Balanced tasks, most use cases | Good balance of speed and accuracy |\n",
    "| `\"high\"` | Maximum tokens | Complex problems, critical accuracy | Slowest, highest token usage |\n",
    "\n",
    "#### Best Practices for Reasoning Effort\n",
    "\n",
    "1. **Start with `\"low\"`**: Always begin with low effort and increase only if needed\n",
    "2. **Use `\"medium\"` for most tasks**: Provides a good balance for general-purpose applications\n",
    "3. **Reserve `\"high\"` for complex tasks**: Math problems, multi-step analysis, or when accuracy is critical\n",
    "4. **Consider latency**: Higher effort means longer response times\n",
    "5. **Monitor token usage**: Reasoning tokens count toward your usage, so balance cost vs. accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a more configured model with medium reasoning effort\n",
    "configured_nova = BedrockModel(\n",
    "    model_id=\"amazon.nova-2-lite-v1:0\",\n",
    "    temperature=0.3,  # Lower temperature for more focused responses\n",
    "    top_p=0.8,        # Controls diversity of responses\n",
    "    max_tokens=1000,  # Maximum response length\n",
    "    stop_sequences=[\"END\", \"STOP\"],  # Sequences that stop generation\n",
    "    region_name=\"us-east-1\",\n",
    "    additional_request_fields={\n",
    "        \"reasoningConfig\": {\n",
    "            \"type\": \"enabled\",\n",
    "            \"maxReasoningEffort\": \"medium\"  # Balanced reasoning for most tasks\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create agent with configured model\n",
    "precise_agent = Agent(\n",
    "    model=configured_nova,\n",
    "    system_prompt=\"You are a precise, factual assistant. Keep responses concise and accurate.\"\n",
    ")\n",
    "\n",
    "response = precise_agent(\"Explain quantum computing in exactly 3 sentences.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Handling\n",
    "\n",
    "When working with Bedrock models, you may encounter these common errors:\n",
    "\n",
    "| Error | Cause | Solution |\n",
    "|-------|-------|----------|\n",
    "| `AccessDeniedException` | Missing IAM permissions | Add `bedrock:InvokeModel` to your IAM policy |\n",
    "| `ValidationException` | Invalid model ID or parameters | Verify model ID and region |\n",
    "| `ResourceNotFoundException` | Model not available in region | Check model availability in your region |\n",
    "| `ThrottlingException` | Rate limit exceeded | Implement exponential backoff retry |\n",
    "| `ReadTimeoutError` | Response took too long | Increase timeout, especially for System Tools |\n",
    "\n",
    "Here's a basic error handling pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_agent_call(agent, message):\n",
    "    \"\"\"Safely call an agent with error handling.\"\"\"\n",
    "    try:\n",
    "        response = agent(message)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return f\"Error occurred: {str(e)}. Please check your AWS credentials and model access.\"\n",
    "\n",
    "# Test error handling\n",
    "response = safe_agent_call(agent, \"Hello, how are you?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Practical Example: Research Assistant\n",
    "\n",
    "Let's build a research assistant that combines Nova 2.0 Lite's **Hybrid Reasoning** with **Nova Grounding** (web search) to gather real-time information, analyze it, and provide well-sourced answers.\n",
    "\n",
    "This example demonstrates:\n",
    "- System tools for live web search with citations\n",
    "- Hybrid Reasoning for synthesizing information from multiple sources\n",
    "- Structured system prompts for consistent output formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Nova model with web search (Nova Grounding) and reasoning enabled\n",
    "research_model = BedrockModel(\n",
    "    model_id=\"amazon.nova-2-lite-v1:0\",\n",
    "    temperature=0.3,  # Lower temperature for factual research\n",
    "    region_name=\"us-east-1\",\n",
    "    max_tokens=10000,\n",
    "    additional_request_fields={\n",
    "        \"reasoningConfig\": {\n",
    "            \"type\": \"enabled\",\n",
    "            \"maxReasoningEffort\": \"medium\"  # Medium effort for thorough research\n",
    "        }\n",
    "    },\n",
    "    additional_args={\n",
    "        \"toolConfig\": {\n",
    "            \"tools\": [\n",
    "                {\n",
    "                    \"systemTool\": {\n",
    "                        \"name\": \"nova_grounding\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create the research assistant agent\n",
    "research_assistant = Agent(\n",
    "    model=research_model,\n",
    "    system_prompt=\"\"\"You are a research assistant with access to real-time web search.\n",
    "\n",
    "When researching a topic:\n",
    "1. Search for current, authoritative sources\n",
    "2. Cross-reference information from multiple sources when possible\n",
    "3. Cite your sources with URLs\n",
    "4. Distinguish between facts and opinions\n",
    "5. Note if information might be outdated or uncertain\n",
    "\n",
    "Format your responses with clear sections:\n",
    "- **Summary**: Brief overview of findings\n",
    "- **Key Points**: Main facts discovered\n",
    "- **Sources**: List of references used\n",
    "\n",
    "Be thorough but concise. Always prioritize accuracy over speed.\"\"\"\n",
    ")\n",
    "\n",
    "print(\"Research assistant created with web search and reasoning capabilities!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the research assistant with a current events question\n",
    "# The agent will search the web for real-time information\n",
    "response = research_assistant(\n",
    "    \"What are the latest developments in quantum computing? Focus on recent breakthroughs and their practical applications.\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've learned to build AI agents with Strands and Amazon Nova 2.0 Lite, including:\n",
    "\n",
    "- **Setup**: API key authentication and model configuration\n",
    "- **Hybrid Reasoning**: Enabling the model to \"think through\" problems with configurable effort levels\n",
    "- **Tools**: System Tools (web search, code interpreter), Community Tools, Custom Tools, and MCP integration\n",
    "- **Content Processing**: Multimodal inputs, streaming responses, and structured outputs\n",
    "\n",
    "### Resources\n",
    "- [Strands Documentation](https://strandsagents.com)\n",
    "- [Amazon Bedrock Documentation](https://docs.aws.amazon.com/bedrock/)\n",
    "- [Amazon Nova Documentation](https://docs.aws.amazon.com/nova/)\n",
    "\n",
    "### Next Steps\n",
    "1. Experiment with different tools and reasoning configurations\n",
    "2. Build an application for your use case\n",
    "3. Explore multi-agent patterns for complex workflows"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
