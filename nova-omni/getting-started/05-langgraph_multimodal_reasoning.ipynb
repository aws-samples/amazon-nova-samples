{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# LangGraph Integration with Nova 2 Omni - Stateful Multimodal Reasoning\n",
    "\n",
    "This notebook demonstrates using Amazon Nova 2 Omni with LangGraph for stateful workflows. We use direct boto3 calls for reasoning configuration and LangGraph for state management.\n",
    "\n",
    "**Key Features:**\n",
    "- Stateful workflow management with LangGraph\n",
    "- Direct boto3 calls with reasoning configuration\n",
    "- Multi-step reasoning with state persistence\n",
    "- Conditional routing based on outputs\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langgraph langchain-core -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "from typing import Annotated, Literal, TypedDict\n",
    "from botocore.config import Config\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "import nova_utils\n",
    "\n",
    "MODEL_ID = \"us.amazon.nova-2-omni-v1:0\"\n",
    "REGION_ID = \"us-west-2\"\n",
    "\n",
    "def get_bedrock_runtime():\n",
    "    config = Config(read_timeout=2 * 60)\n",
    "    return boto3.client(\n",
    "        service_name=\"bedrock-runtime\",\n",
    "        region_name=REGION_ID,\n",
    "        config=config\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "state-tools",
   "metadata": {},
   "source": [
    "## Define State and Tools\n",
    "\n",
    "Create state schema and tool definitions for the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define-state-tools",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReasoningState(TypedDict):\n",
    "    \"\"\"State for multimodal reasoning workflow.\"\"\"\n",
    "    messages: list\n",
    "    analysis_complete: bool\n",
    "    final_answer: str\n",
    "\n",
    "class VideoSegmentInput(BaseModel):\n",
    "    \"\"\"Schema for video segment analysis.\"\"\"\n",
    "    timestamp_range: str = Field(description=\"Time range in format MM:SS-MM:SS\")\n",
    "    action_description: str = Field(description=\"What action is happening\")\n",
    "    key_objects: list[str] = Field(description=\"Key objects visible in segment\")\n",
    "\n",
    "@tool(args_schema=VideoSegmentInput)\n",
    "def log_video_segment(timestamp_range: str, action_description: str, key_objects: list[str]) -> dict:\n",
    "    \"\"\"Log analysis of a video segment with timestamp and details.\"\"\"\n",
    "    return {\n",
    "        \"status\": \"segment_logged\",\n",
    "        \"timestamp\": timestamp_range,\n",
    "        \"action\": action_description,\n",
    "        \"objects\": key_objects\n",
    "    }\n",
    "\n",
    "class VideoSummaryInput(BaseModel):\n",
    "    \"\"\"Schema for complete video summary.\"\"\"\n",
    "    title: str = Field(description=\"Title or main topic of video\")\n",
    "    total_segments: int = Field(description=\"Number of segments analyzed\")\n",
    "    key_takeaways: list[str] = Field(description=\"Main takeaways from video\")\n",
    "\n",
    "@tool(args_schema=VideoSummaryInput)\n",
    "def submit_video_summary(title: str, total_segments: int, key_takeaways: list[str]) -> dict:\n",
    "    \"\"\"Submit final video summary after analyzing all segments.\"\"\"\n",
    "    return {\n",
    "        \"status\": \"summary_complete\",\n",
    "        \"title\": title,\n",
    "        \"segments\": total_segments,\n",
    "        \"takeaways\": key_takeaways\n",
    "    }\n",
    "\n",
    "def langchain_tool_to_bedrock(lc_tool):\n",
    "    schema = lc_tool.args_schema.model_json_schema()\n",
    "    return {\n",
    "        \"toolSpec\": {\n",
    "            \"name\": lc_tool.name,\n",
    "            \"description\": lc_tool.description,\n",
    "            \"inputSchema\": {\"json\": schema}\n",
    "        }\n",
    "    }\n",
    "\n",
    "bedrock_tools = [langchain_tool_to_bedrock(log_video_segment), langchain_tool_to_bedrock(submit_video_summary)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graph-definition",
   "metadata": {},
   "source": [
    "## Build the Reasoning Graph\n",
    "\n",
    "Create a stateful graph with reasoning nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-graph",
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock = get_bedrock_runtime()\n",
    "\n",
    "def reasoning_node(state: ReasoningState):\n",
    "    \"\"\"Main reasoning node using direct boto3 calls.\"\"\"\n",
    "    # Only use the first user message for reasoning (no assistant prefill)\n",
    "    user_messages = [msg for msg in state[\"messages\"] if msg[\"role\"] == \"user\"]\n",
    "    request = {\n",
    "        \"modelId\": MODEL_ID,\n",
    "        \"messages\": user_messages,\n",
    "        \"toolConfig\": {\"tools\": bedrock_tools},\n",
    "        \"additionalModelRequestFields\": {\n",
    "            \"reasoningConfig\": {\n",
    "                \"type\": \"enabled\",\n",
    "                \"maxReasoningEffort\": \"medium\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    response = bedrock.converse(**request)\n",
    "    \n",
    "    # Extract content, filtering out reasoningContent blocks\n",
    "    content = []\n",
    "    for item in response[\"output\"][\"message\"][\"content\"]:\n",
    "        if \"reasoningContent\" not in item:\n",
    "            content.append(item)\n",
    "    \n",
    "    new_message = {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": content\n",
    "    }\n",
    "    \n",
    "    return {\"messages\": state[\"messages\"] + [new_message]}\n",
    "\n",
    "def should_continue(state: ReasoningState) -> str:\n",
    "    \"\"\"Determine if workflow should continue.\"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    \n",
    "    for content in last_message.get(\"content\", []):\n",
    "        if \"toolUse\" in content:\n",
    "            if content[\"toolUse\"][\"name\"] == \"submit_video_summary\":\n",
    "                return \"end\"\n",
    "    return \"continue\"\n",
    "\n",
    "workflow = StateGraph(ReasoningState)\n",
    "workflow.add_node(\"reasoning\", reasoning_node)\n",
    "workflow.set_entry_point(\"reasoning\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"reasoning\",\n",
    "    should_continue,\n",
    "    {\"continue\": \"reasoning\", \"end\": END}\n",
    ")\n",
    "\n",
    "app = workflow.compile()\n",
    "print(\"âœ… Reasoning graph compiled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example-1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example: Video Analysis Workflow\n",
    "\n",
    "Run a stateful reasoning workflow for video analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "video-workflow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image\n",
    "image_path = \"media/man_crossing_street.png\"\n",
    "image_bytes, image_format = nova_utils.load_image_as_bytes(image_path)\n",
    "\n",
    "initial_state = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"image\": {\"format\": image_format, \"source\": {\"bytes\": image_bytes}}},\n",
    "                {\"text\": \"Analyze this image and provide a summary. Use submit_video_summary with a title describing the scene, number of key elements (3-5), and key observations.\"}\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"analysis_complete\": False,\n",
    "    \"final_answer\": \"\"\n",
    "}\n",
    "\n",
    "print(\"=== Running Workflow ===\")\n",
    "final_state = app.invoke(initial_state)\n",
    "\n",
    "print(\"\\n=== Final Answer ===\")\n",
    "for message in final_state[\"messages\"]:\n",
    "    if message.get(\"role\") == \"assistant\":\n",
    "        for content in message.get(\"content\", []):\n",
    "            if \"toolUse\" in content:\n",
    "                tool_use = content[\"toolUse\"]\n",
    "                print(f\"Tool: {tool_use['name']}\")\n",
    "                print(json.dumps(tool_use[\"input\"], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- **Stateful Workflows**: LangGraph maintains state across reasoning steps\n",
    "- **Direct boto3 Calls**: Enable reasoning configuration with additionalModelRequestFields\n",
    "- **Conditional Routing**: Route based on tool calls and outputs\n",
    "- **Hybrid Approach**: Combine LangGraph structure with boto3 API control\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Explore **06_strands_multimodal_reasoning.ipynb** for multi-agent patterns\n",
    "- Experiment with different reasoning effort levels\n",
    "- Build custom workflows for your use cases"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
