{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c80d6694",
   "metadata": {},
   "source": [
    "# Speech Understanding Examples\n",
    "\n",
    "This notebook demonstrates how to use Amazon Nova 2 Omni for speech understanding tasks. Nova 2 Omni can transcribe, summarize, analyze, answer questions about, and translate speech content in audio files.\n",
    "\n",
    "**Supported audio formats:** mp3, opus, wav, aac, flac, mp4, ogg, mkv\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfbc101",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Helper Functions\n",
    "\n",
    "Run the cell below to establish helper functions used by the examples in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d27624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "import nova_utils\n",
    "\n",
    "MODEL_ID = \"us.amazon.nova-2-omni-v1:0\"\n",
    "REGION_ID = \"us-west-2\"\n",
    "\n",
    "def get_bedrock_runtime():\n",
    "    \"\"\"Returns a properly configured Bedrock Runtime client.\"\"\"\n",
    "    config = Config(\n",
    "        read_timeout=2 * 60,\n",
    "    )\n",
    "    bedrock = boto3.client(\n",
    "        service_name=\"bedrock-runtime\",\n",
    "        region_name=REGION_ID,\n",
    "        config=config,\n",
    "    )\n",
    "    return bedrock\n",
    "\n",
    "\n",
    "def speech_to_text(\n",
    "    audio_path,\n",
    "    audio_type,\n",
    "    text_prompt,\n",
    "    temperature=None,\n",
    "    max_tokens=10000,\n",
    "    reasoning_effort=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates a text output from a text prompt and a single input audio.\n",
    "\n",
    "    Args:\n",
    "        audio_path: The path to the input audio.\n",
    "        audio_type: Type of the audio file (mp3, opus, wav, aac, flac, mp4, ogg, mkv)\n",
    "        text_prompt: The text prompt to use for speech understanding.\n",
    "        temperature: Optional temperature parameter (0.0-1.0). If None, uses model default.\n",
    "        max_tokens: Maximum number of tokens to generate (default: 10000).\n",
    "        reasoning_effort: Optional reasoning effort level (\"low\", \"medium\", \"high\"). If None, reasoning is disabled.\n",
    "\n",
    "    Returns:\n",
    "        (Dict) A dictionary with \"text\" and \"request_id\" keys\n",
    "    \"\"\"\n",
    "    audio_bytes = nova_utils.load_audio_as_bytes(audio_path)\n",
    "\n",
    "    # Build inference config\n",
    "    inference_config = {\"maxTokens\": max_tokens}\n",
    "    if temperature is not None:\n",
    "        inference_config[\"temperature\"] = temperature\n",
    "\n",
    "    # Build request\n",
    "    request = {\n",
    "        \"modelId\": MODEL_ID,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"audio\": {\"format\": audio_type, \"source\": {\"bytes\": audio_bytes}}},\n",
    "                    {\"text\": text_prompt},\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "        \"inferenceConfig\": inference_config,\n",
    "    }\n",
    "\n",
    "    # Add reasoning config if specified\n",
    "    if reasoning_effort is not None:\n",
    "        if reasoning_effort.lower() not in [\"low\", \"medium\", \"high\"]:\n",
    "            raise ValueError(\"reasoning_effort must be 'low', 'medium', or 'high'\")\n",
    "\n",
    "        request[\"additionalModelRequestFields\"] = {\n",
    "            \"reasoningConfig\": {\n",
    "                \"type\": \"enabled\",\n",
    "                \"maxReasoningEffort\": reasoning_effort.lower(),\n",
    "            }\n",
    "        }\n",
    "\n",
    "    bedrock_runtime = get_bedrock_runtime()\n",
    "\n",
    "    response = bedrock_runtime.converse(**request)\n",
    "    import json\n",
    "\n",
    "    return {\n",
    "        \"text\": nova_utils.extract_response_text(response),\n",
    "        \"request_id\": response.get(\"ResponseMetadata\", {}).get(\"RequestId\", \"N/A\"),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f5a496",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Use Case 1: Transcribing Speech from Audio Files\n",
    "\n",
    "Nova 2 Omni can transcribe speech content in audio files and can provide annotations indicating who is speaking, known as diarization.\n",
    "\n",
    "**Recommended inference parameters:**\n",
    "* `temperature`: 0 (greedy decoding)\n",
    "* Reasoning should not be used\n",
    "\n",
    "---\n",
    "\n",
    "### Example 1a: Speech Transcription (Without Diarization)\n",
    "\n",
    "**Recommended prompt template:**\n",
    "```\n",
    "Transcribe the audio.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36fa9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = \"media/call_1763087723216.wav\"\n",
    "audio_type = \"wav\"\n",
    "temperature = 0.0\n",
    "\n",
    "user_prompt = \"Transcribe the audio.\"\n",
    "\n",
    "try:\n",
    "    result = speech_to_text(audio_path, audio_type, user_prompt, temperature)\n",
    "\n",
    "    if result[\"text\"]:\n",
    "        print(f\"Request ID: {result['request_id']}\\n\")\n",
    "        print(\"== Transcription Output ==\\n\")\n",
    "        print(result[\"text\"])\n",
    "\n",
    "        # Store for later use in Q&A examples\n",
    "        transcription = result[\"text\"]\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y84ug4xk7xg",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Example 1b: Speech Transcription (With Diarization)\n",
    "\n",
    "**Recommended prompt template:**\n",
    "```\n",
    "For each speaker turn segment, transcribe, assign a speaker label, start and end timestamps. \n",
    "You must follow the exact XML format shown in the example below: \n",
    "'<segment><transcription speaker=\"speaker_id\" start=\"start_time\" end=\"end_time\">transcription_text</transcription></segment>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g8p0osrp35q",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = \"media/call_1763087723216.wav\"\n",
    "audio_type = \"wav\"\n",
    "temperature = 0.0\n",
    "\n",
    "user_prompt = \"\"\"For each speaker turn segment, transcribe, assign a speaker label, start and end timestamps. You must follow the exact XML format shown in the example below: '<segment><transcription speaker=\"speaker_id\" start=\"start_time\" end=\"end_time\">transcription_text</transcription></segment>'\"\"\"\n",
    "\n",
    "try:\n",
    "    result = speech_to_text(audio_path, audio_type, user_prompt, temperature)\n",
    "\n",
    "    if result[\"text\"]:\n",
    "        print(f\"Request ID: {result['request_id']}\\n\")\n",
    "        print(\"== Transcription with Diarization Output ==\\n\")\n",
    "        print(result[\"text\"])\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2byv2ph01dq",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Use Case 2: Summarizing Speech in Audio Files\n",
    "\n",
    "The Nova 2 Omni model is capable of understanding speech in audio files and generating concise summaries.\n",
    "\n",
    "**Recommended inference parameters:**\n",
    "* `temperature`: text default parameters\n",
    "* `topP`: text default parameters\n",
    "* Some use cases may benefit from enabling model reasoning; however, we recommend starting without reasoning first\n",
    "\n",
    "---\n",
    "\n",
    "### Example 2: Summarize Audio Content\n",
    "\n",
    "**Recommended prompt template:**\n",
    "```\n",
    "Extract and summarize the essential details from the audio.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ho2i85qkmih",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = \"media/call_1763087723216.wav\"\n",
    "audio_type = \"wav\"\n",
    "\n",
    "user_prompt = \"Extract and summarize the essential details from the audio.\"\n",
    "\n",
    "try:\n",
    "    result = speech_to_text(audio_path, audio_type, user_prompt)\n",
    "\n",
    "    if result[\"text\"]:\n",
    "        print(f\"Request ID: {result['request_id']}\\n\")\n",
    "        print(\"== Summary Output ==\\n\")\n",
    "        print(result[\"text\"])\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yryhbc7vevk",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Use Case 3: Analyzing Audio Calls\n",
    "\n",
    "The Nova 2 Omni model is capable of understanding speech in audio files and generating structured call analytics based on your business needs.\n",
    "\n",
    "**Recommended inference parameters:**\n",
    "* `temperature`: text default parameters\n",
    "* `topP`: text default parameters\n",
    "* Some use cases may benefit from enabling model reasoning; however, we recommend starting without reasoning first\n",
    "\n",
    "---\n",
    "\n",
    "### Example 3: Call Analytics with Structured JSON Output\n",
    "\n",
    "**Example prompt:**\n",
    "```\n",
    "Analyze the call and return JSON:\n",
    "{\n",
    "  \"call_summary\": \"Summarize the call\",\n",
    "  \"customer_intent\": \"What the customer wanted\",\n",
    "  \"resolution_status\": \"resolved/pending/escalated\",\n",
    "  \"key_topics\": [\"topic1\", \"topic2\"],\n",
    "  \"action_items\": [\n",
    "    {\"task\": \"description\", \"owner\": \"agent/customer\", \"priority\": \"high/medium/low\"}\n",
    "  ],\n",
    "  \"sentiment_analysis\": {\n",
    "    \"overall\": \"positive/neutral/negative\"\n",
    "  },\n",
    "  \"follow_up_required\": true/false\n",
    "}\n",
    "```\n",
    "\n",
    "**Note:** You can customize the JSON structure based on your specific business needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bkuhqtgzp0j",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = \"media/call_1763087723216.wav\"\n",
    "audio_type = \"wav\"\n",
    "\n",
    "user_prompt = \"\"\"Analyze the call and return JSON:\n",
    "{\n",
    "  \"call_summary\": \"Summarize the call\",\n",
    "  \"customer_intent\": \"What the customer wanted\",\n",
    "  \"resolution_status\": \"resolved/pending/escalated\",\n",
    "  \"key_topics\": [\"topic1\", \"topic2\"],\n",
    "  \"action_items\": [\n",
    "    {\"task\": \"description\", \"owner\": \"agent/customer\", \"priority\": \"high/medium/low\"}\n",
    "  ],\n",
    "  \"sentiment_analysis\": {\n",
    "    \"overall\": \"positive/neutral/negative\"\n",
    "  },\n",
    "  \"follow_up_required\": true/false\n",
    "}\"\"\"\n",
    "\n",
    "try:\n",
    "    result = speech_to_text(audio_path, audio_type, user_prompt)\n",
    "\n",
    "    if result[\"text\"]:\n",
    "        print(f\"Request ID: {result['request_id']}\\n\")\n",
    "        print(\"== Call Analytics Output ==\\n\")\n",
    "        print(result[\"text\"])\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cvu5w46tjdl",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Use Case 4: Asking Questions About Audio File Content\n",
    "\n",
    "You can leverage Nova 2 Omni's speech understanding capabilities for question and answer use cases.\n",
    "\n",
    "**Recommended inference parameters:**\n",
    "* `temperature`: text default parameters\n",
    "* `topP`: text default parameters\n",
    "* Some use cases may benefit from enabling model reasoning; however, we recommend starting without reasoning first\n",
    "\n",
    "**Note:** No specific prompting template is required. Simply ask your question naturally.\n",
    "\n",
    "---\n",
    "\n",
    "### Example 4a: Count Speakers in Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3svnjv7ntq9",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = \"media/call_1763087723216.wav\"\n",
    "audio_type = \"wav\"\n",
    "\n",
    "user_prompt = \"How many speakers are in the audio?\"\n",
    "\n",
    "try:\n",
    "    result = speech_to_text(audio_path, audio_type, user_prompt)\n",
    "\n",
    "    if result[\"text\"]:\n",
    "        print(f\"Request ID: {result['request_id']}\\n\")\n",
    "        print(\"== Q&A Output ==\\n\")\n",
    "        print(f\"Question: {user_prompt}\")\n",
    "        print(f\"Answer: {result['text']}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b8g44i9yav",
   "metadata": {},
   "source": [
    "### Example 4b: Analyze Emotional Tone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ytigscnws1",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = \"media/call_1763087723216.wav\"\n",
    "audio_type = \"wav\"\n",
    "\n",
    "user_prompt = \"What was the overall emotional tone of the speaker (e.g., frustrated, calm, excited)?\"\n",
    "\n",
    "try:\n",
    "    result = speech_to_text(audio_path, audio_type, user_prompt)\n",
    "\n",
    "    if result[\"text\"]:\n",
    "        print(f\"Request ID: {result['request_id']}\\n\")\n",
    "        print(\"== Q&A Output ==\\n\")\n",
    "        print(f\"Question: {user_prompt}\")\n",
    "        print(f\"Answer: {result['text']}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k2ppj1c642i",
   "metadata": {},
   "source": [
    "### Example 4c: List People Mentioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nc3fjru4vkh",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = \"media/call_1763087723216.wav\"\n",
    "audio_type = \"wav\"\n",
    "\n",
    "user_prompt = \"List the people mentioned in the audio.\"\n",
    "\n",
    "try:\n",
    "    result = speech_to_text(audio_path, audio_type, user_prompt)\n",
    "\n",
    "    if result[\"text\"]:\n",
    "        print(f\"Request ID: {result['request_id']}\\n\")\n",
    "        print(\"== Q&A Output ==\\n\")\n",
    "        print(f\"Question: {user_prompt}\")\n",
    "        print(f\"Answer: {result['text']}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18s3r2o71c9i",
   "metadata": {},
   "source": [
    "### Example 4d: Detect Background Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kkbrhrci0qi",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = \"media/call_1763087723216.wav\"\n",
    "audio_type = \"wav\"\n",
    "\n",
    "user_prompt = \"Is there background noise in the audio?\"\n",
    "\n",
    "try:\n",
    "    result = speech_to_text(audio_path, audio_type, user_prompt)\n",
    "\n",
    "    if result[\"text\"]:\n",
    "        print(f\"Request ID: {result['request_id']}\\n\")\n",
    "        print(\"== Q&A Output ==\\n\")\n",
    "        print(f\"Question: {user_prompt}\")\n",
    "        print(f\"Answer: {result['text']}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8krrufcof2y",
   "metadata": {},
   "source": [
    "### Example 4e: Describe Speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diuabbs86uq",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = \"media/call_1763087723216.wav\"\n",
    "audio_type = \"wav\"\n",
    "\n",
    "user_prompt = \"Describe the speakers in the audio.\"\n",
    "\n",
    "try:\n",
    "    result = speech_to_text(audio_path, audio_type, user_prompt)\n",
    "\n",
    "    if result[\"text\"]:\n",
    "        print(f\"Request ID: {result['request_id']}\\n\")\n",
    "        print(\"== Q&A Output ==\\n\")\n",
    "        print(f\"Question: {user_prompt}\")\n",
    "        print(f\"Answer: {result['text']}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad0bc6d-f356-460e-a567-6bbbec08076c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
