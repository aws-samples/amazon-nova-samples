{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91c7f66b-2820-4ac3-b031-7a04d3f3b237",
   "metadata": {},
   "source": [
    "# W2 Form OCR with Amazon Nova Lite - Model Evaluation\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this final notebook, we'll evaluate the performance of our fine-tuned Amazon Nova Lite model on W2 tax form OCR tasks. We'll compare the base model against our custom fine-tuned model to measure the improvements achieved through the fine-tuning process.\n",
    "\n",
    "The evaluation will focus on:\n",
    "- Overall field extraction accuracy\n",
    "- Accuracy by field category (employee information, employer information, earnings, etc.)\n",
    "- Analysis of numerical field errors and their distribution\n",
    "- Comparative performance metrics between base and fine-tuned models\n",
    "\n",
    "This analysis will help us understand how effectively our model has learned to extract structured data from tax documents and where further improvements might be needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b048dba9-75bb-4b82-9191-047360f0e32e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T19:11:44.342910Z",
     "iopub.status.busy": "2025-08-14T19:11:44.342296Z",
     "iopub.status.idle": "2025-08-14T19:11:44.544626Z",
     "shell.execute_reply": "2025-08-14T19:11:44.543826Z",
     "shell.execute_reply.started": "2025-08-14T19:11:44.342878Z"
    }
   },
   "source": [
    "## Environment Setup\n",
    "\n",
    "First, let's set up our environment by importing necessary libraries and initializing the AWS Bedrock client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f920d1e1-54d1-401e-9afc-4fbbe0355c32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T17:20:47.854380Z",
     "iopub.status.busy": "2025-08-15T17:20:47.854095Z",
     "iopub.status.idle": "2025-08-15T17:20:49.253712Z",
     "shell.execute_reply": "2025-08-15T17:20:49.249085Z",
     "shell.execute_reply.started": "2025-08-15T17:20:47.854348Z"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import io\n",
    "import uuid\n",
    "import warnings\n",
    "import numpy as np\n",
    "import re\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set AWS region\n",
    "region = \"us-east-1\"\n",
    "\n",
    "# Create AWS clients\n",
    "session = boto3.session.Session(region_name=region)\n",
    "s3_client = session.client('s3')\n",
    "sts_client = session.client('sts')\n",
    "bedrock = session.client(service_name=\"bedrock\")\n",
    "bedrock_runtime = session.client(service_name=\"bedrock-runtime\", region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9786aaf0-3852-4eb2-bb51-80308eb18fb6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T17:20:49.260239Z",
     "iopub.status.busy": "2025-08-15T17:20:49.259779Z",
     "iopub.status.idle": "2025-08-15T17:20:49.271422Z",
     "shell.execute_reply": "2025-08-15T17:20:49.270411Z",
     "shell.execute_reply.started": "2025-08-15T17:20:49.260173Z"
    }
   },
   "outputs": [],
   "source": [
    "# Retrieve stored variables from previous notebook\n",
    "%store -r bucket_name\n",
    "%store -r test_data_uri\n",
    "%store -r role_arn\n",
    "%store -r role_name\n",
    "%store -r policy_arn\n",
    "%store -r text_prompt\n",
    "%store -r test_s3_paths\n",
    "%store -r account_id\n",
    "%store -r deployment_arn\n",
    "\n",
    "print(f\"Test data URI: {test_data_uri}\")\n",
    "print(f\"Role ARN: {role_arn}\")\n",
    "print(f\"Custom model deployment arn: {deployment_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2965191a-45ea-4d94-8981-cf93650258b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T19:20:50.448973Z",
     "iopub.status.busy": "2025-08-14T19:20:50.448572Z",
     "iopub.status.idle": "2025-08-14T19:20:50.507341Z",
     "shell.execute_reply": "2025-08-14T19:20:50.505667Z",
     "shell.execute_reply.started": "2025-08-14T19:20:50.448948Z"
    }
   },
   "source": [
    "## Load Test Dataset\n",
    "\n",
    "We'll load our test dataset from the JSONL file we created in the data preparation notebook. This dataset contains W2 form images and their corresponding ground truth structured data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9ed11e-84fc-4df6-8c55-8f5e6f2cf883",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T17:20:52.692398Z",
     "iopub.status.busy": "2025-08-15T17:20:52.691998Z",
     "iopub.status.idle": "2025-08-15T17:20:52.701086Z",
     "shell.execute_reply": "2025-08-15T17:20:52.700024Z",
     "shell.execute_reply.started": "2025-08-15T17:20:52.692371Z"
    }
   },
   "outputs": [],
   "source": [
    "local_test_data_path = \"./test.jsonl\"\n",
    "test_data=[]\n",
    "with open(local_test_data_path, 'r') as f:\n",
    "    for line in f:\n",
    "          test_data.append(json.loads(line.strip()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07101d47-2c95-4779-adbc-a971f9428bd2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T19:45:20.156615Z",
     "iopub.status.busy": "2025-08-14T19:45:20.156273Z",
     "iopub.status.idle": "2025-08-14T19:45:20.161761Z",
     "shell.execute_reply": "2025-08-14T19:45:20.160842Z",
     "shell.execute_reply.started": "2025-08-14T19:45:20.156590Z"
    }
   },
   "source": [
    "### Helper function to help evaluation and format the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4470c1c-f662-4077-bd86-07bb674a09f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T17:21:12.640302Z",
     "iopub.status.busy": "2025-08-15T17:21:12.639465Z",
     "iopub.status.idle": "2025-08-15T17:21:12.645120Z",
     "shell.execute_reply": "2025-08-15T17:21:12.644306Z",
     "shell.execute_reply.started": "2025-08-15T17:21:12.640267Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to flatten nested dictionaries\n",
    "def flatten_dict(d, parent_key='', sep='_'):\n",
    "    items = []\n",
    "    for k, v in d.items():\n",
    "        new_key = parent_key + sep + k if parent_key else k\n",
    "        if isinstance(v, dict):\n",
    "            items.extend(flatten_dict(v, new_key, sep=sep).items())\n",
    "        else:\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9z4962px58i",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T17:21:14.240256Z",
     "iopub.status.busy": "2025-08-15T17:21:14.239719Z",
     "iopub.status.idle": "2025-08-15T17:21:14.247926Z",
     "shell.execute_reply": "2025-08-15T17:21:14.246942Z",
     "shell.execute_reply.started": "2025-08-15T17:21:14.240198Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to evaluate a single prediction against ground truth\n",
    "def evaluate_prediction(gt, pred):\n",
    "    # Flatten both dictionaries to make comparison easier\n",
    "    flat_gt = flatten_dict(gt)\n",
    "    flat_pred = flatten_dict(pred)\n",
    "    \n",
    "    # Get all unique keys\n",
    "    all_keys = set(flat_gt.keys()).union(set(flat_pred.keys()))\n",
    "    \n",
    "    # Track correct predictions\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    errors = {}\n",
    "    \n",
    "    # Compare each field\n",
    "    for key in all_keys:\n",
    "        total += 1\n",
    "        \n",
    "        # If key exists in both dictionaries\n",
    "        if key in flat_gt and key in flat_pred:\n",
    "            # For numerical values, allow small tolerance\n",
    "            if isinstance(flat_gt[key], (int, float)) and isinstance(flat_pred[key], (int, float)):\n",
    "                # Calculate percentage error for numerical values\n",
    "                if abs(flat_gt[key]) > 0:  # Avoid division by zero\n",
    "                    pct_error = abs(flat_gt[key] - flat_pred[key]) / abs(flat_gt[key]) * 100\n",
    "                    if pct_error < 0.1:  # 0.1% tolerance for numerical values\n",
    "                        correct += 1\n",
    "                    else:\n",
    "                        errors[key] = (flat_gt[key], flat_pred[key], pct_error)\n",
    "                else:\n",
    "                    # If ground truth is 0, check if prediction is very close to 0\n",
    "                    if abs(flat_pred[key]) < 0.1:\n",
    "                        correct += 1\n",
    "                    else:\n",
    "                        errors[key] = (flat_gt[key], flat_pred[key], float('inf'))\n",
    "            # For strings, exact match required\n",
    "            elif flat_gt[key] == flat_pred[key]:\n",
    "                correct += 1\n",
    "            else:\n",
    "                errors[key] = (flat_gt[key], flat_pred[key], \"mismatch\")\n",
    "        else:\n",
    "            # Missing or extra field\n",
    "            gt_val = flat_gt.get(key, None)\n",
    "            pred_val = flat_pred.get(key, None)\n",
    "            errors[key] = (gt_val, pred_val, \"missing\" if key in flat_gt else \"extra\")\n",
    "    \n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'correct': correct,\n",
    "        'total': total,\n",
    "        'errors': errors\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uz4bdaevtn",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T17:21:15.095646Z",
     "iopub.status.busy": "2025-08-15T17:21:15.094813Z",
     "iopub.status.idle": "2025-08-15T17:21:15.101920Z",
     "shell.execute_reply": "2025-08-15T17:21:15.101003Z",
     "shell.execute_reply.started": "2025-08-15T17:21:15.095602Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to categorize fields\n",
    "def get_field_category(field):\n",
    "    if field.startswith('employee'):\n",
    "        return 'Employee Information'\n",
    "    elif field.startswith('employer'):\n",
    "        return 'Employer Information'\n",
    "    elif field.startswith('earnings'):\n",
    "        return 'Earnings'\n",
    "    elif field.startswith('benefits'):\n",
    "        return 'Benefits'\n",
    "    elif field.startswith('multiStateEmployment'):\n",
    "        return 'Multi-State Employment'\n",
    "    else:\n",
    "        return 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e30ef48-7e47-474d-af97-260161f4ff3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T17:21:16.071964Z",
     "iopub.status.busy": "2025-08-15T17:21:16.071668Z",
     "iopub.status.idle": "2025-08-15T17:21:16.076287Z",
     "shell.execute_reply": "2025-08-15T17:21:16.075522Z",
     "shell.execute_reply.started": "2025-08-15T17:21:16.071940Z"
    }
   },
   "outputs": [],
   "source": [
    "def reorder_content(entry):\n",
    "    # Make a deep copy to avoid modifying the original\n",
    "    import copy\n",
    "    new_entry = copy.deepcopy(entry)\n",
    "\n",
    "    # Find text and image items in content\n",
    "    if 'content' in new_entry and isinstance(new_entry['content'], list):\n",
    "        text_items = [i for i in new_entry['content'] if 'text' in i]\n",
    "        image_items = [i for i in new_entry['content'] if 'image' in i]\n",
    "\n",
    "        # Create new content list with image first, then text\n",
    "        new_entry['content'] = image_items + text_items\n",
    "\n",
    "    return new_entry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ibgzpgf9djo",
   "metadata": {},
   "source": [
    "### Comprehensive Model Evaluation Function\n",
    "\n",
    "This is our main evaluation function that:\n",
    "1. Takes a sample of test data examples\n",
    "2. Passes them to the model for inference\n",
    "3. Compares the predictions against ground truth\n",
    "4. Computes overall and categorical accuracy metrics\n",
    "5. Analyzes numerical field errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6nbl0321zta",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T17:21:17.457786Z",
     "iopub.status.busy": "2025-08-15T17:21:17.457499Z",
     "iopub.status.idle": "2025-08-15T17:21:17.468296Z",
     "shell.execute_reply": "2025-08-15T17:21:17.467422Z",
     "shell.execute_reply.started": "2025-08-15T17:21:17.457765Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to run evaluation on a sample of the test dataset\n",
    "def evaluate_model_on_test_data(num_samples, model_id):\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    \n",
    "    # Select random samples from test data\n",
    "    if num_samples > len(test_data):\n",
    "        num_samples = len(test_data)\n",
    "    \n",
    "    sample_indices = np.random.choice(len(test_data), num_samples, replace=False)\n",
    "    \n",
    "    results = []\n",
    "    field_categories = {\n",
    "        'Employee Information': {'correct': 0, 'total': 0},\n",
    "        'Employer Information': {'correct': 0, 'total': 0},\n",
    "        'Earnings': {'correct': 0, 'total': 0},\n",
    "        'Benefits': {'correct': 0, 'total': 0},\n",
    "        'Multi-State Employment': {'correct': 0, 'total': 0},\n",
    "        'Other': {'correct': 0, 'total': 0}\n",
    "    }\n",
    "    \n",
    "    numerical_errors = []\n",
    "    \n",
    "    for i in tqdm(sample_indices, desc=\"Evaluating model\"):\n",
    "        # Get the test sample\n",
    "        test_sample = test_data[i]\n",
    "        messages = [reorder_content(entry) for entry in test_sample[\"messages\"][:1]]\n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [\n",
    "                    {\"text\": \"```json\"}\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "\n",
    "        pattern = r\"```json\\n(.*?)\\n```\"\n",
    "        match = re.search(pattern, test_sample[\"messages\"][1][\"content\"][0][\"text\"], re.DOTALL)\n",
    "        if match:\n",
    "          # Get the JSON string\n",
    "          json_string = match.group(1)\n",
    "          gt = json.loads(json_string)\n",
    "        \n",
    "        # Get model prediction\n",
    "        response = bedrock_runtime.converse(\n",
    "            modelId=model_id,\n",
    "            messages=messages,\n",
    "            inferenceConfig={\"maxTokens\": 5000, \"temperature\": 0.0, \"topP\": 0.1, \"stopSequences\": [\"```\"]},\n",
    "        )\n",
    "        # Extract the prediction from the response\n",
    "        prediction_text = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "        prediction = json.loads(prediction_text.replace(\"```\", \"\").strip())\n",
    "        \n",
    "        # Evaluate the prediction\n",
    "        eval_result = evaluate_prediction(gt, prediction)\n",
    "        results.append({\n",
    "            'index': i,\n",
    "            'accuracy': eval_result['accuracy'],\n",
    "            'correct': eval_result['correct'],\n",
    "            'total': eval_result['total'],\n",
    "            'errors': eval_result['errors']\n",
    "        })\n",
    "        \n",
    "        # Update field category stats\n",
    "        flat_gt = flatten_dict(gt)\n",
    "        for key in flat_gt:\n",
    "            category = get_field_category(key)\n",
    "            field_categories[category]['total'] += 1\n",
    "            if key not in eval_result['errors']:\n",
    "                field_categories[category]['correct'] += 1\n",
    "        \n",
    "        # Collect numerical errors\n",
    "        for key, (gt_val, pred_val, error) in eval_result['errors'].items():\n",
    "            if isinstance(error, (int, float)) and error != float('inf'):\n",
    "                numerical_errors.append({\n",
    "                    'field': key,\n",
    "                    'gt': gt_val,\n",
    "                    'pred': pred_val,\n",
    "                    'error_pct': error,\n",
    "                    'category': get_field_category(key)\n",
    "                })\n",
    "    \n",
    "    # Calculate overall accuracy\n",
    "    overall_accuracy = sum(r['correct'] for r in results) / sum(r['total'] for r in results)\n",
    "    \n",
    "    # Calculate category accuracies\n",
    "    category_accuracies = {}\n",
    "    for category, stats in field_categories.items():\n",
    "        accuracy = stats['correct'] / stats['total'] if stats['total'] > 0 else 0\n",
    "        category_accuracies[category] = accuracy\n",
    "    \n",
    "    return {\n",
    "        'results': results,\n",
    "        'overall_accuracy': overall_accuracy,\n",
    "        'category_accuracies': category_accuracies,\n",
    "        'numerical_errors': numerical_errors\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991bd71b-3281-4f32-a72c-a855ddc7078e",
   "metadata": {},
   "source": [
    "## Base Model Evaluation\n",
    "\n",
    "First, let's evaluate the performance of Amazon Nova Lite's base model without fine-tuning. This will serve as our baseline for comparison to measure the improvements achieved through fine-tuning.\n",
    "\n",
    "The base model evaluation will:\n",
    "1. Process 100 randomly selected test samples\n",
    "2. Record accuracy metrics for each field type\n",
    "3. Calculate overall extraction accuracy\n",
    "4. Analyze error patterns and trends\n",
    "\n",
    "This baseline measurement is critical for understanding the impact of our fine-tuning efforts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1c4824-b980-49bc-8882-ca28362237f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T17:21:18.989399Z",
     "iopub.status.busy": "2025-08-15T17:21:18.988943Z",
     "iopub.status.idle": "2025-08-15T17:21:18.993062Z",
     "shell.execute_reply": "2025-08-15T17:21:18.992277Z",
     "shell.execute_reply.started": "2025-08-15T17:21:18.989370Z"
    }
   },
   "outputs": [],
   "source": [
    "nova_lite_id = \"us.amazon.nova-lite-v1:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec370be-1e42-44fd-9e18-f1911f2ca643",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T17:27:04.070227Z",
     "iopub.status.busy": "2025-08-15T17:27:04.069843Z",
     "iopub.status.idle": "2025-08-15T17:35:49.607632Z"
    }
   },
   "outputs": [],
   "source": [
    "# Run evaluation on 100 test samples\n",
    "print(\"Running base model evaluation on test dataset...\")\n",
    "eval_results = evaluate_model_on_test_data(100, nova_lite_id)\n",
    "\n",
    "# Print overall accuracy\n",
    "print(f\"\\nOverall Field Extraction Accuracy: {eval_results['overall_accuracy']:.2%}\")\n",
    "\n",
    "# Print category accuracies\n",
    "print(\"\\nAccuracy by Field Category:\")\n",
    "for category, accuracy in eval_results['category_accuracies'].items():\n",
    "    print(f\"  - {category}: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201be025-351e-47eb-a024-b764a1eeeee8",
   "metadata": {},
   "source": [
    "## Custom Fine-tuned Model Evaluation\n",
    "\n",
    "Now, let's evaluate our fine-tuned model using the same methodology and test dataset. The custom model has been specifically trained on W2 tax form data to improve its extraction accuracy and field recognition.\n",
    "\n",
    "By using identical evaluation parameters and test samples, we can directly compare:\n",
    "1. Overall accuracy improvements\n",
    "2. Field-specific accuracy gains\n",
    "3. Reduction in error rates across different data types\n",
    "4. Improvements in handling complex structures like multi-state employment data\n",
    "\n",
    "The results will demonstrate the effectiveness of our fine-tuning approach and identify any remaining areas for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da997b9e-dc2a-4417-8817-9a3168211302",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T17:35:49.615673Z",
     "iopub.status.busy": "2025-08-15T17:35:49.615146Z",
     "iopub.status.idle": "2025-08-15T17:44:02.627364Z",
     "shell.execute_reply": "2025-08-15T17:44:02.626596Z",
     "shell.execute_reply.started": "2025-08-15T17:35:49.615649Z"
    }
   },
   "outputs": [],
   "source": [
    "# Run evaluation on 100 test samples\n",
    "print(\"Running custom model evaluation on test dataset...\")\n",
    "eval_results = evaluate_model_on_test_data(100, deployment_arn)\n",
    "\n",
    "# Print overall accuracy\n",
    "print(f\"\\nOverall Field Extraction Accuracy: {eval_results['overall_accuracy']:.2%}\")\n",
    "\n",
    "# Print category accuracies\n",
    "print(\"\\nAccuracy by Field Category:\")\n",
    "for category, accuracy in eval_results['category_accuracies'].items():\n",
    "    print(f\"  - {category}: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68700758-eb87-4d92-921b-07178cce0336",
   "metadata": {},
   "source": [
    "## Resource Cleanup\n",
    "\n",
    "After completing our evaluation, it's important to clean up the AWS resources we created to avoid unnecessary costs. This includes:\n",
    "\n",
    "1. Deleting the custom model deployment\n",
    "2. Cleaning up IAM resources (roles and policies)\n",
    "3. Properly terminating any active services\n",
    "\n",
    "This helps ensure we don't incur unexpected charges for resources we no longer need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c93991-0268-43dc-aff4-866884fc1dfe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T17:44:48.027124Z",
     "iopub.status.busy": "2025-08-15T17:44:48.026348Z",
     "iopub.status.idle": "2025-08-15T17:44:48.032780Z",
     "shell.execute_reply": "2025-08-15T17:44:48.031775Z",
     "shell.execute_reply.started": "2025-08-15T17:44:48.027087Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_up():\n",
    "    \"\"\"\n",
    "    Clean up all AWS resources created during the fine-tuning process to avoid unnecessary costs\n",
    "    \"\"\"\n",
    "    print(\"Cleaning up resources...\")\n",
    "    \n",
    "    # Delete on-demand deployment if it exists\n",
    "    if 'deployment_arn' in locals() or 'deployment_arn' in globals():\n",
    "        try:\n",
    "            print(f\"Deleting on-demand deployment: {deployment_arn}...\")\n",
    "            bedrock.delete_custom_model_deployment(\n",
    "                customModelDeploymentIdentifier=deployment_arn\n",
    "            )\n",
    "            print(\"On-demand deployment deletion initiated\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting on-demand deployment: {e}\")\n",
    "    \n",
    "    # Clean up IAM resources\n",
    "    iam = session.client('iam')\n",
    "    try:\n",
    "        print(\"Detaching policy from role...\")\n",
    "        iam.detach_role_policy(RoleName=role_name, PolicyArn=policy_arn)\n",
    "        \n",
    "        print(\"Deleting policy...\")\n",
    "        iam.delete_policy(PolicyArn=policy_arn)\n",
    "        \n",
    "        print(\"Deleting role...\")\n",
    "        iam.delete_role(RoleName=role_name)\n",
    "        \n",
    "        print(\"IAM resources cleaned up\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error cleaning up IAM resources: {e}\")\n",
    "    \n",
    "    # We're not deleting the S3 bucket here as you might want to keep your data and model\n",
    "    \n",
    "    print(\"Cleanup completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae2cb4b-671a-4ad8-9115-cc07dfcf4bc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T17:44:48.553404Z",
     "iopub.status.busy": "2025-08-15T17:44:48.552785Z",
     "iopub.status.idle": "2025-08-15T17:44:49.109835Z",
     "shell.execute_reply": "2025-08-15T17:44:49.108737Z",
     "shell.execute_reply.started": "2025-08-15T17:44:48.553368Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_up()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4a5e64-9ca0-41f7-8e20-cc102788307a",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've evaluated and compared the performance of the base Amazon Nova Lite model and our custom fine-tuned version for W2 tax form OCR tasks. \n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Overall Accuracy Improvement**: The fine-tuned model achieved 85.31% field extraction accuracy compared to 55.87% for the base model, representing a significant 29.44% absolute improvement.\n",
    "\n",
    "2. **Field Category Improvements**:\n",
    "   - Employee Information: 81.67% (↑ from 57.00%)\n",
    "   - Employer Information: 92.00% (↑ from 57.33%)\n",
    "   - Earnings Information: 85.14% (↑ from 60.57%)\n",
    "   - Benefits Information: 60.50% (↑ from 45.00%)\n",
    "   - Multi-State Employment: 94.19% (↑ from 61.88%)\n",
    "\n",
    "The results demonstrate that fine-tuning significantly improves the model's ability to extract structured information from tax forms, making it suitable for automated document processing workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a9d78a-bbb9-43ab-8350-b89ac046d284",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
