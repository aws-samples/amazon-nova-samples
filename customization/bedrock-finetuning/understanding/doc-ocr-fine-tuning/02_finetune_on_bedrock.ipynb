{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e4049b7-41f6-4b42-bf47-d3f544a2bed9",
   "metadata": {},
   "source": [
    "# Fine-tuning Amazon Nova Lite with Vision Capabilities - Model Training and Inference\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we'll use the data prepared in the previous notebook to fine-tune an Amazon Nova Lite multi-modal model using Amazon Bedrock. After fine-tuning, we'll test the model's performance using the test dataset.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's install and import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36ab4a7-1068-4f2c-8c60-b440e4569f3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T14:29:40.828464Z",
     "iopub.status.busy": "2025-08-15T14:29:40.827557Z",
     "iopub.status.idle": "2025-08-15T14:29:41.574185Z",
     "shell.execute_reply": "2025-08-15T14:29:41.573002Z",
     "shell.execute_reply.started": "2025-08-15T14:29:40.828417Z"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import base64\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c0f1f0-ddb1-45ce-bab1-665f2b8bacfb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T14:29:41.755858Z",
     "iopub.status.busy": "2025-08-15T14:29:41.755070Z",
     "iopub.status.idle": "2025-08-15T14:29:41.918225Z",
     "shell.execute_reply": "2025-08-15T14:29:41.917351Z",
     "shell.execute_reply.started": "2025-08-15T14:29:41.755824Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set AWS region\n",
    "region = \"us-east-1\"  # Nova Lite fine-tuning is available in this region\n",
    "\n",
    "# Create AWS clients\n",
    "session = boto3.session.Session(region_name=region)\n",
    "s3_client = session.client('s3')\n",
    "bedrock = session.client(service_name=\"bedrock\", region_name=region)\n",
    "bedrock_runtime = session.client(service_name=\"bedrock-runtime\", region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00890e32-562c-4420-a504-0016dfb73441",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T14:29:42.871812Z",
     "iopub.status.busy": "2025-08-15T14:29:42.871158Z",
     "iopub.status.idle": "2025-08-15T14:29:42.884493Z",
     "shell.execute_reply": "2025-08-15T14:29:42.883492Z",
     "shell.execute_reply.started": "2025-08-15T14:29:42.871779Z"
    }
   },
   "outputs": [],
   "source": [
    "# Retrieve stored variables from previous notebook\n",
    "%store -r bucket_name\n",
    "%store -r train_data_uri\n",
    "%store -r validation_data_uri\n",
    "%store -r test_data_uri\n",
    "%store -r role_arn\n",
    "%store -r role_name\n",
    "%store -r policy_arn\n",
    "%store -r text_prompt\n",
    "%store -r test_s3_paths\n",
    "%store -r account_id\n",
    "\n",
    "print(f\"Bucket name: {bucket_name}\")\n",
    "print(f\"Training data URI: {train_data_uri}\")\n",
    "print(f\"Validation data URI: {validation_data_uri}\")\n",
    "print(f\"Role ARN: {role_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d4c8c3-4170-4fd1-af2c-bfa2271433a3",
   "metadata": {},
   "source": [
    "## Create Fine-tuning Job\n",
    "\n",
    "Now, we'll create a fine-tuning job for the Amazon Nova Lite multi-modal model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bed667b-45ab-4e97-afe3-d5a9c7a2baa6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T14:29:44.138335Z",
     "iopub.status.busy": "2025-08-15T14:29:44.137931Z",
     "iopub.status.idle": "2025-08-15T14:29:44.142127Z",
     "shell.execute_reply": "2025-08-15T14:29:44.141362Z",
     "shell.execute_reply.started": "2025-08-15T14:29:44.138308Z"
    }
   },
   "outputs": [],
   "source": [
    "nova_lite_base_model_arn = \"arn:aws:bedrock:us-east-1::foundation-model/amazon.nova-lite-v1:0:300k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e00e12-bf0a-43af-aed6-77420b3b0114",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T14:29:45.856720Z",
     "iopub.status.busy": "2025-08-15T14:29:45.856129Z",
     "iopub.status.idle": "2025-08-15T14:29:45.861512Z",
     "shell.execute_reply": "2025-08-15T14:29:45.860645Z",
     "shell.execute_reply.started": "2025-08-15T14:29:45.856678Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate a timestamp for unique naming\n",
    "timestamp = time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "\n",
    "# Define job parameters\n",
    "job_name = f\"nova-multimodal-ft-{timestamp}\"\n",
    "custom_model_name = f\"nova-multimodel-{timestamp}\"\n",
    "\n",
    "# Define hyperparameters\n",
    "hyperparameters = {\n",
    "    \"epochCount\": \"2\",       # Number of training epochs\n",
    "    \"batchSize\": \"1\",        # Batch size for training\n",
    "    \"learningRate\": \"0.00001\"  # Learning rate\n",
    "}\n",
    "\n",
    "# Define output location\n",
    "output_s3_uri = f\"s3://{bucket_name}/output/\"\n",
    "\n",
    "# Create validation data config\n",
    "validation_data_config = {\n",
    "    \"validators\": [{\n",
    "        \"s3Uri\": validation_data_uri\n",
    "    }]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014a95dc-ad46-4bbf-aef5-335e6cf400ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T14:29:47.324132Z",
     "iopub.status.busy": "2025-08-15T14:29:47.323734Z",
     "iopub.status.idle": "2025-08-15T14:29:48.905828Z",
     "shell.execute_reply": "2025-08-15T14:29:48.904857Z",
     "shell.execute_reply.started": "2025-08-15T14:29:47.324100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create fine-tuning job\n",
    "try:\n",
    "    response = bedrock.create_model_customization_job(\n",
    "        customizationType=\"FINE_TUNING\",\n",
    "        jobName=job_name,\n",
    "        customModelName=custom_model_name,\n",
    "        roleArn=role_arn,\n",
    "        baseModelIdentifier=nova_lite_base_model_arn,\n",
    "        hyperParameters=hyperparameters,\n",
    "        trainingDataConfig={\"s3Uri\": train_data_uri},\n",
    "        validationDataConfig=validation_data_config,\n",
    "        outputDataConfig={\"s3Uri\": output_s3_uri}\n",
    "    )\n",
    "    \n",
    "    # Get job identifier\n",
    "    job_arn = response[\"jobArn\"]\n",
    "    print(f\"Fine-tuning job created: {job_arn}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error creating fine-tuning job: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fce36b0-f24c-42be-be51-f0757ea719ec",
   "metadata": {},
   "source": [
    "## Monitor Job Status\n",
    "\n",
    "Let's monitor the status of our fine-tuning job:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb11476-77b5-4e36-8b9d-9f9b1312372d",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "    background-color: #fcf8e3; \n",
    "    color: #8a6d3b;\n",
    "    padding: 15px;\n",
    "    margin-bottom: 20px;\n",
    "    border: 1px solid #faebcc;\n",
    "    border-radius: 4px;\n",
    "    font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif;\">\n",
    "    <span style=\"font-weight:bold;\">⚠️ Warning:</span> \n",
    "    <p>Fine-tuning jobs for Amazon Nova Lite multi-modal models may take <b>several hours to complete</b>. \n",
    "    The exact duration depends on your dataset size, model parameters, and current training resource availability.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f3ba73-ce64-447e-862d-99b81b249f34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T17:13:22.316843Z",
     "iopub.status.busy": "2025-08-15T17:13:22.315770Z",
     "iopub.status.idle": "2025-08-15T17:13:22.575388Z",
     "shell.execute_reply": "2025-08-15T17:13:22.574501Z",
     "shell.execute_reply.started": "2025-08-15T17:13:22.316802Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to check job status\n",
    "def check_job_status(job_arn):\n",
    "    response = bedrock.get_model_customization_job(jobIdentifier=job_arn)\n",
    "    return response[\"status\"]\n",
    "\n",
    "# Get current job status\n",
    "current_status = check_job_status(job_arn)\n",
    "print(f\"Current job status: {current_status}\")\n",
    "\n",
    "# If job completed successfully, get the model details\n",
    "if current_status == \"Completed\":\n",
    "    model_details = bedrock.get_model_customization_job(jobIdentifier=job_arn)\n",
    "    custom_model_arn = model_details[\"outputModelArn\"]\n",
    "    print(f\"Fine-tuned model ARN: {custom_model_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4b83ca-3f73-4224-9b9f-4affc8eda97f",
   "metadata": {},
   "source": [
    "## Visualize Training Metrics\n",
    "\n",
    "Let's download and visualize the training metrics:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8beb8884-07a6-4979-9c27-d4e1339c56b4",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "    background-color: #fcf8e3; \n",
    "    color: #8a6d3b;\n",
    "    padding: 15px;\n",
    "    margin-bottom: 20px;\n",
    "    border: 1px solid #faebcc;\n",
    "    border-radius: 4px;\n",
    "    font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif;\">\n",
    "    <span style=\"font-weight:bold;\">⚠️ Warning:</span> \n",
    "    <p>Please ensure the status is <b>\"Completed\"</b> before proceeding with the following cells. \n",
    "    You can re-run the status check cell above to update this status.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7207eeb-dc47-4722-bf02-fba5b7ee52d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T17:13:26.095760Z",
     "iopub.status.busy": "2025-08-15T17:13:26.095137Z",
     "iopub.status.idle": "2025-08-15T17:13:26.105434Z",
     "shell.execute_reply": "2025-08-15T17:13:26.101758Z",
     "shell.execute_reply.started": "2025-08-15T17:13:26.095727Z"
    }
   },
   "outputs": [],
   "source": [
    "# Download training metrics from S3\n",
    "def download_metrics():\n",
    "    # Get the job ID from the ARN\n",
    "    job_id = job_arn.split('/')[-1]\n",
    "    \n",
    "    # Define file paths\n",
    "    train_metrics_s3_key = f\"output/model-customization-job-{job_id}/training_artifacts/step_wise_training_metrics.csv\"\n",
    "    \n",
    "    local_train_metrics = \"train_metrics.csv\"\n",
    "    \n",
    "    # Download files\n",
    "    try:\n",
    "        s3_client.download_file(bucket_name, train_metrics_s3_key, local_train_metrics)\n",
    "        \n",
    "        print(\"Metrics downloaded successfully\")\n",
    "        return local_train_metrics\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading metrics: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e6f437-06f4-4ec2-97aa-6e7769dd9bd3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T17:13:26.579719Z",
     "iopub.status.busy": "2025-08-15T17:13:26.579144Z",
     "iopub.status.idle": "2025-08-15T17:13:27.343343Z",
     "shell.execute_reply": "2025-08-15T17:13:27.342528Z",
     "shell.execute_reply.started": "2025-08-15T17:13:26.579686Z"
    }
   },
   "outputs": [],
   "source": [
    "# Download metrics\n",
    "train_metrics_file = download_metrics()\n",
    "\n",
    "# Plot training and validation loss if metrics are available\n",
    "if train_metrics_file:\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Load metrics\n",
    "    train_data = pd.read_csv(train_metrics_file)\n",
    "\n",
    "    # Calculate step-level training loss\n",
    "    train_metrics_epoch = train_data.groupby('step_number').mean()\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_metrics_epoch.index, train_metrics_epoch.training_loss, label='Training')\n",
    "    plt.title('Training Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Step')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468fcd88-e3e3-487e-bff6-404b01e74c77",
   "metadata": {},
   "source": [
    "## Deploy the fine-tuned model\n",
    "\n",
    "We'll deploy the fine-tuned model with [On-demand inference option](https://docs.aws.amazon.com/nova/latest/userguide/custom-fine-tune-odi.html). \n",
    "\n",
    "On-demand (OD) inference allows you to run inference on your custom Amazon Nova models without maintaining provisioned throughput endpoints. This helps you optimize costs and scale efficiently. With On-demand inference, you are charged based on usage, measured in tokens, both in and out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257b6dab-198a-46c0-bb8b-18ca265d3bfe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T17:13:30.917754Z",
     "iopub.status.busy": "2025-08-15T17:13:30.917450Z",
     "iopub.status.idle": "2025-08-15T17:13:30.922592Z",
     "shell.execute_reply": "2025-08-15T17:13:30.921586Z",
     "shell.execute_reply.started": "2025-08-15T17:13:30.917729Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to check deployment status\n",
    "def check_deployment_status(deployment_arn):\n",
    "    \"\"\"\n",
    "    Check the status of a custom model deployment\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    deployment_arn : str\n",
    "        ARN of the deployment to check\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    status : str\n",
    "        Current status of the deployment\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = bedrock.get_custom_model_deployment(\n",
    "            customModelDeploymentIdentifier=deployment_arn\n",
    "        )\n",
    "        status = response.get('status')\n",
    "        print(f\"Deployment status: {status}\")\n",
    "        return status\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error checking deployment status: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i26t9zm5w3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T17:13:34.174458Z",
     "iopub.status.busy": "2025-08-15T17:13:34.173789Z",
     "iopub.status.idle": "2025-08-15T17:13:34.179464Z",
     "shell.execute_reply": "2025-08-15T17:13:34.178635Z",
     "shell.execute_reply.started": "2025-08-15T17:13:34.174425Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to create on-demand inferencing deployment for custom model\n",
    "def create_model_deployment(custom_model_arn):\n",
    "    \"\"\"\n",
    "    Create an on-demand inferencing deployment for the custom model\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    custom_model_arn : str\n",
    "        ARN of the custom model to deploy\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    deployment_arn : str\n",
    "        ARN of the created deployment\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Creating on-demand inferencing deployment for model: {custom_model_arn}\")\n",
    "        \n",
    "        # Generate a unique name for the deployment\n",
    "        deployment_name = f\"nova-ocr-deployment-{time.strftime('%Y%m%d-%H%M%S')}\"\n",
    "        \n",
    "        # Create the deployment\n",
    "        response = bedrock.create_custom_model_deployment(\n",
    "            modelArn=custom_model_arn,\n",
    "            modelDeploymentName=deployment_name,\n",
    "            description=f\"on-demand inferencing deployment for model: {custom_model_arn}\",\n",
    "        )\n",
    "        \n",
    "        # Get the deployment ARN\n",
    "        deployment_arn = response.get('customModelDeploymentArn')\n",
    "        \n",
    "        print(f\"Deployment request submitted. Deployment ARN: {deployment_arn}\")\n",
    "        return deployment_arn\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating deployment: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ef18e6-6117-4308-8d1a-6e45d67caa74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T17:13:35.247188Z",
     "iopub.status.busy": "2025-08-15T17:13:35.246784Z",
     "iopub.status.idle": "2025-08-15T17:13:35.713034Z",
     "shell.execute_reply": "2025-08-15T17:13:35.712242Z",
     "shell.execute_reply.started": "2025-08-15T17:13:35.247158Z"
    }
   },
   "outputs": [],
   "source": [
    "# If job completed successfully, get the model details and create deployment\n",
    "if current_status == \"Completed\":\n",
    "    # Get custom model ARN\n",
    "    model_details = bedrock.get_model_customization_job(jobIdentifier=job_arn)\n",
    "    custom_model_arn = model_details[\"outputModelArn\"]\n",
    "    print(f\"Fine-tuned model ARN: {custom_model_arn}\")\n",
    "    \n",
    "    # Create on-demand deployment\n",
    "    deployment_arn = create_model_deployment(custom_model_arn)\n",
    "    \n",
    "    if deployment_arn:\n",
    "        # Check initial status\n",
    "        initial_status = check_deployment_status(deployment_arn)\n",
    "        \n",
    "        # Store the deployment ARN for later use\n",
    "        %store deployment_arn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "x3rvnylijr",
   "metadata": {},
   "source": [
    "## Wait for Deployment to Complete\n",
    "\n",
    "ℹ️ **Info:** It takes about 30 mins to complete the deployment\n",
    "\n",
    "Let's monitor our deployment until it's ready for use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rfo8evi04ch",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T17:14:01.219774Z",
     "iopub.status.busy": "2025-08-15T17:14:01.219379Z",
     "iopub.status.idle": "2025-08-15T17:16:01.849240Z",
     "shell.execute_reply": "2025-08-15T17:16:01.848343Z",
     "shell.execute_reply.started": "2025-08-15T17:14:01.219750Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to wait for deployment to be ready\n",
    "def wait_for_deployment(deployment_arn, max_wait_seconds=3600, check_interval=60):\n",
    "    \"\"\"\n",
    "    Wait for a deployment to reach 'IN_SERVICE' status\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    deployment_arn : str\n",
    "        ARN of the deployment to monitor\n",
    "    max_wait_seconds : int\n",
    "        Maximum wait time in seconds (default: 1800s = 30 minutes)\n",
    "    check_interval : int\n",
    "        Interval between checks in seconds (default: 60s)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    success : bool\n",
    "        True if deployment is in service, False otherwise\n",
    "    \"\"\"\n",
    "    import time\n",
    "    from tqdm.notebook import tqdm\n",
    "    \n",
    "    start_time = time.time()\n",
    "    end_time = start_time + max_wait_seconds\n",
    "    \n",
    "    print(f\"Waiting for deployment to complete (max wait time: {max_wait_seconds/60:.1f} minutes)\")\n",
    "    \n",
    "    # Create a progress bar for the wait time\n",
    "    with tqdm(total=max_wait_seconds, desc=\"Waiting for deployment\", unit=\"sec\") as pbar:\n",
    "        elapsed = 0\n",
    "        while time.time() < end_time:\n",
    "            status = check_deployment_status(deployment_arn)\n",
    "            \n",
    "            if status == \"Active\":\n",
    "                print(f\"\\n✅ Deployment is now in service after {(time.time() - start_time)/60:.1f} minutes\")\n",
    "                return True\n",
    "                \n",
    "            if status == \"Failed\":\n",
    "                print(f\"\\n❌ Deployment failed or was deleted. Final status: {status}\")\n",
    "                return False\n",
    "                \n",
    "            # Update progress bar with time elapsed since last check\n",
    "            new_elapsed = int(time.time() - start_time)\n",
    "            pbar.update(new_elapsed - elapsed)\n",
    "            elapsed = new_elapsed\n",
    "            \n",
    "            # Wait before checking again\n",
    "            time.sleep(check_interval)\n",
    "    \n",
    "    print(f\"\\n⚠️ Timed out after waiting {max_wait_seconds/60:.1f} minutes. Deployment may still be in progress.\")\n",
    "    return False\n",
    "\n",
    "# Only attempt to wait for deployment if it was created\n",
    "if 'deployment_arn' in locals() and deployment_arn:\n",
    "    deployment_ready = wait_for_deployment(deployment_arn)\n",
    "    \n",
    "    if deployment_ready:\n",
    "        %store deployment_arn\n",
    "    else:\n",
    "        print(\"Deployment did not complete successfully. Using the base custom model for inference.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc29c55-1ef4-460d-9fc5-923bdf958406",
   "metadata": {},
   "source": [
    "## Test with Inference\n",
    "\n",
    "Now, let's test our fine-tuned model using the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04100116-5267-43f4-ace6-68e35891fd02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T17:16:01.873424Z",
     "iopub.status.busy": "2025-08-15T17:16:01.872985Z",
     "iopub.status.idle": "2025-08-15T17:16:01.879060Z",
     "shell.execute_reply": "2025-08-15T17:16:01.878140Z",
     "shell.execute_reply.started": "2025-08-15T17:16:01.873355Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_w2(s3_uri):\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"image\": {\n",
    "                    \"format\": \"png\",\n",
    "                    \"source\": {\n",
    "                        \"s3Location\": {\n",
    "                            \"uri\": s3_uri,\n",
    "                            \"bucketOwner\" : account_id\n",
    "                        }\n",
    "                    }\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"text\": text_prompt\n",
    "\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [\n",
    "                {\"text\": \"```json\"}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2735d5bd-e1cc-4f91-9e7d-f3c5ae0d4425",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T17:16:01.881707Z",
     "iopub.status.busy": "2025-08-15T17:16:01.881380Z",
     "iopub.status.idle": "2025-08-15T17:16:09.741924Z",
     "shell.execute_reply": "2025-08-15T17:16:09.740796Z",
     "shell.execute_reply.started": "2025-08-15T17:16:01.881678Z"
    }
   },
   "outputs": [],
   "source": [
    "bedrock_client = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "response = bedrock_client.converse(\n",
    "            modelId=deployment_arn,\n",
    "            messages=process_w2(test_s3_paths[0].get(\"s3_uri\")),\n",
    "            inferenceConfig={\"maxTokens\": 2048, \"temperature\": 0.0, \"topP\": 0.1, \"stopSequences\": [\"```\"]},\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d7210d-0a82-4c99-aae0-a2d24684fa9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T17:20:32.309169Z",
     "iopub.status.busy": "2025-08-15T17:20:32.308684Z",
     "iopub.status.idle": "2025-08-15T17:20:32.314891Z",
     "shell.execute_reply": "2025-08-15T17:20:32.314102Z",
     "shell.execute_reply.started": "2025-08-15T17:20:32.309125Z"
    }
   },
   "outputs": [],
   "source": [
    "prediction = json.loads(response[\"output\"][\"message\"][\"content\"][0][\"text\"].replace(\"```\", \"\"))\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48c75d5-fae3-488e-b9b2-926e4689bfae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T17:20:33.270233Z",
     "iopub.status.busy": "2025-08-15T17:20:33.269429Z",
     "iopub.status.idle": "2025-08-15T17:20:33.275466Z",
     "shell.execute_reply": "2025-08-15T17:20:33.274751Z",
     "shell.execute_reply.started": "2025-08-15T17:20:33.270198Z"
    }
   },
   "outputs": [],
   "source": [
    "# Store variables for the next notebook\n",
    "%store deployment_arn\n",
    "\n",
    "print(\"Variables saved for use in the next notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec4d8e9-f1ee-4173-bc6b-00e38ddf5d75",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we successfully fine-tuned an Amazon Nova Lite multi-modal model using Amazon Bedrock. We:\n",
    "\n",
    "- Set up and launched a fine-tuning job with our prepared dataset\n",
    "- Monitored the job progress and visualized training metrics\n",
    "- Created provisioned throughput for the fine-tuned model\n",
    "- Tested the model's performance with inference on test samples\n",
    "- Cleaned up resources we no longer needed\n",
    "\n",
    "The fine-tuned model can now answer questions about images based on the patterns it learned from our training data. For real-world applications, you may want to use a larger and more diverse dataset tailored to your specific use case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
