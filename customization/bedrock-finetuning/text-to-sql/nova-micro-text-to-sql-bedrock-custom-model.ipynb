{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Fine-Tuning Amazon Nova Micro for Text-to-SQL with Amazon Bedrock Custom Models\n",
    "\n",
    "This notebook demonstrates how to fine-tune Amazon Nova Micro for text-to-SQL generation using Amazon Bedrock Custom Models. This approach uses Supervised Fine-Tuning (SFT) with Parameter-Efficient Fine-Tuning (PEFT) , SFT is a technique that allows fine-tuning language models on specific tasks using labeled examples, while PEFT enables efficient fine-tuning by updating only a small subset of the model's parameters.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook illustrates the complete workflow from data preparation to model evaluation using Bedrock's custom model fine-tuning capabilities:\n",
    "\n",
    "- **Data Preparation**: SQL dataset converted to bedrock-conversation-2024 schema\n",
    "- **Training Method**: Bedrock Custom Model fine-tuning with hyperparameter configuration\n",
    "- **Use Case**: Text-to-SQL generation with comprehensive evaluation and inference pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93073ce2",
   "metadata": {},
   "source": [
    "## Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f58470b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install boto3 datasets pandas scikit-learn --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907bb7d9",
   "metadata": {},
   "source": [
    "## Setup and Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6618055",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import time\n",
    "import sagemaker\n",
    "import io\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize clients\n",
    "bedrock = boto3.client('bedrock')\n",
    "bedrock_runtime = boto3.client('bedrock-runtime')\n",
    "s3_client = boto3.client('s3')\n",
    "iam_client = boto3.client('iam')\n",
    "\n",
    "# SageMaker session for S3 operations\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sess.default_bucket()\n",
    "region = sess.boto_region_name\n",
    "\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"S3 Bucket: {bucket}\")\n",
    "print(f\"Role: {role}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb607b0e",
   "metadata": {},
   "source": [
    "## 1. Data Preparation\n",
    "\n",
    "We'll use the [sql-create-context](https://huggingface.co/datasets/b-mc2/sql-create-context) dataset and format it according to the bedrock-conversation-2024 schema that Nova expects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479a9aa9",
   "metadata": {},
   "source": [
    "### Step 1a: Load and Explore the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd9b23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SQL dataset\n",
    "dataset = load_dataset('b-mc2/sql-create-context', split='train')\n",
    "print(f'Dataset size: {len(dataset)}')\n",
    "print('Sample record:')\n",
    "print(dataset[19])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8219395",
   "metadata": {},
   "source": [
    "### Step 1b: Convert to Bedrock Conversation Format\n",
    "\n",
    "Nova expects data in the bedrock-conversation-2024 format:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"system\": [{\"text\": \"System prompt content\"}],\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [{\"text\": \"User question\"}]\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": [{\"text\": \"Assistant response\"}]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963c5661",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bedrock_conversation(record):\n",
    "    \"\"\"\n",
    "    Convert SQL dataset to Bedrock conversation format.\n",
    "    Format: bedrock-conversation-2024 schema\n",
    "    \"\"\"\n",
    "    system_prompt = \"\"\"You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\n",
    "\n",
    "You must output the SQL query that answers the question.\"\"\"\n",
    "    \n",
    "    user_message = f\"\"\"### Input:\n",
    "{record['question']}\n",
    "\n",
    "### Context:\n",
    "{record['context']}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "    \n",
    "    conversation = {\n",
    "        \"system\": [{\"text\": system_prompt}],\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\"text\": user_message}]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [{\"text\": record['answer']}]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return conversation\n",
    "\n",
    "# Test the conversion\n",
    "sample_converted = create_bedrock_conversation(dataset[0])\n",
    "print('Sample converted record:')\n",
    "print(json.dumps(sample_converted, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf5fce5",
   "metadata": {},
   "source": [
    "### Step 1c: Create Train/Test Split and Convert Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88c387f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bedrock has a limit of 20,000 training samples\n",
    "MAX_TOTAL_SAMPLES = 1000  # We will use a smaller subset of data for our finetuning job\n",
    "\n",
    "# Use 90/10 split, so we need max 20k total samples\n",
    "total_samples = min(len(dataset), MAX_TOTAL_SAMPLES)\n",
    "\n",
    "print(f\"Original dataset size: {len(dataset)}\")\n",
    "print(f\"Using {total_samples} samples (training + validation combined limit: {MAX_TOTAL_SAMPLES})\")\n",
    "\n",
    "# Convert to bedrock-conversation-2024 format\n",
    "print(\"\\nConverting to Bedrock Format...\")\n",
    "converted_data = []\n",
    "for i in range(total_samples):\n",
    "    converted_data.append(create_bedrock_conversation(dataset[i]))\n",
    "print(f\"Converted {len(converted_data)} records\")\n",
    "\n",
    "# Create train/test split (90/10)\n",
    "train_data, test_data = train_test_split(converted_data, test_size=0.1, random_state=42)\n",
    "\n",
    "print(f\"\\nFinal training samples: {len(train_data)}\")\n",
    "print(f\"Final test samples: {len(test_data)}\")\n",
    "print(f\"Total samples: {len(train_data) + len(test_data)}\")\n",
    "print(f\"\\n✓ Dataset is within Bedrock's limits (max 20,000 total samples)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda120d8",
   "metadata": {},
   "source": [
    "### Step 1d: Save Data in JSONL Format and Upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2e6056",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Save datasets locally\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "with open('data/train_dataset.jsonl', 'w') as f:\n",
    "    for item in train_data:\n",
    "        f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "with open('data/test_dataset.jsonl', 'w') as f:\n",
    "    for item in test_data:\n",
    "        f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "print(\"Datasets saved locally\")\n",
    "\n",
    "# Upload to S3\n",
    "timestamp = time.strftime('%Y%m%d-%H%M%S')\n",
    "s3_prefix = f'bedrock-nova-finetuning/{timestamp}'\n",
    "\n",
    "print(\"\\nUploading to S3...\")\n",
    "\n",
    "# Upload training data\n",
    "train_key = f'{s3_prefix}/train/train_dataset.jsonl'\n",
    "try:\n",
    "    s3_client.upload_file('data/train_dataset.jsonl', bucket, train_key)\n",
    "    print(f\"✓ Uploaded training data to s3://{bucket}/{train_key}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Failed to upload training data: {e}\")\n",
    "    raise\n",
    "\n",
    "# Upload validation data\n",
    "test_key = f'{s3_prefix}/validation/test_dataset.jsonl'\n",
    "try:\n",
    "    s3_client.upload_file('data/test_dataset.jsonl', bucket, test_key)\n",
    "    print(f\"✓ Uploaded validation data to s3://{bucket}/{test_key}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Failed to upload validation data: {e}\")\n",
    "    raise\n",
    "\n",
    "# Set the full S3 paths\n",
    "train_s3_path = f's3://{bucket}/{train_key}'\n",
    "test_s3_path = f's3://{bucket}/{test_key}'\n",
    "output_s3_path = f's3://{bucket}/{s3_prefix}/output/'\n",
    "\n",
    "print(f\"\\nS3 Paths:\")\n",
    "print(f\"Training data: {train_s3_path}\")\n",
    "print(f\"Validation data: {test_s3_path}\")\n",
    "print(f\"Output path: {output_s3_path}\")\n",
    "\n",
    "# Verify files exist in S3\n",
    "print(\"\\nVerifying S3 uploads...\")\n",
    "try:\n",
    "    response = s3_client.head_object(Bucket=bucket, Key=train_key)\n",
    "    print(f\"✓ Training data verified ({response['ContentLength']} bytes)\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Training data verification failed: {e}\")\n",
    "    \n",
    "try:\n",
    "    response = s3_client.head_object(Bucket=bucket, Key=test_key)\n",
    "    print(f\"✓ Validation data verified ({response['ContentLength']} bytes)\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Validation data verification failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3171fbaa",
   "metadata": {},
   "source": [
    "## 2. Create IAM Role for Bedrock\n",
    "\n",
    "Create an IAM role that allows Bedrock to access S3 data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fa9331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bedrock_role():\n",
    "    \"\"\"\n",
    "    Create IAM role for Bedrock to access S3 data.\n",
    "    This role allows Bedrock to read training data and write outputs.\n",
    "    \"\"\"\n",
    "    role_name = 'BedrockNovaCustomModelRole'\n",
    "    \n",
    "    # Check if role exists\n",
    "    try:\n",
    "        response = iam_client.get_role(RoleName=role_name)\n",
    "        role_arn = response['Role']['Arn']\n",
    "        print(f\"Using existing role: {role_arn}\")\n",
    "        return role_arn\n",
    "    except iam_client.exceptions.NoSuchEntityException:\n",
    "        pass\n",
    "    \n",
    "    # Trust policy for Bedrock\n",
    "    trust_policy = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": {\n",
    "                    \"Service\": \"bedrock.amazonaws.com\"\n",
    "                },\n",
    "                \"Action\": \"sts:AssumeRole\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Create role\n",
    "    response = iam_client.create_role(\n",
    "        RoleName=role_name,\n",
    "        AssumeRolePolicyDocument=json.dumps(trust_policy),\n",
    "        Description='Role for Bedrock custom model fine-tuning'\n",
    "    )\n",
    "    role_arn = response['Role']['Arn']\n",
    "    \n",
    "    # Attach S3 access policy\n",
    "    s3_policy = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"s3:GetObject\",\n",
    "                    \"s3:PutObject\",\n",
    "                    \"s3:ListBucket\"\n",
    "                ],\n",
    "                \"Resource\": [\n",
    "                    f\"arn:aws:s3:::{bucket}/*\",\n",
    "                    f\"arn:aws:s3:::{bucket}\"\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    policy_name = 'BedrockNovaS3Access'\n",
    "    iam_client.put_role_policy(\n",
    "        RoleName=role_name,\n",
    "        PolicyName=policy_name,\n",
    "        PolicyDocument=json.dumps(s3_policy)\n",
    "    )\n",
    "    \n",
    "    print(f\"Created role: {role_arn}\")\n",
    "    time.sleep(10)  # Wait for role to propagate\n",
    "    \n",
    "    return role_arn\n",
    "\n",
    "bedrock_role_arn = create_bedrock_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0101d3e2",
   "metadata": {},
   "source": [
    "## 3. Create Bedrock Custom Model Fine-Tuning Job\n",
    "\n",
    "Now we'll create a fine-tuning job using Bedrock's custom model API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a5a707",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting Bedrock Fine-tuning Job...\")\n",
    "\n",
    "job_name = f'nova-micro-sql-{timestamp}'\n",
    "custom_model_name = f'nova-micro-sql-custom-{timestamp}'\n",
    "\n",
    "# Create model customization job\n",
    "response = bedrock.create_model_customization_job(\n",
    "    jobName=job_name,\n",
    "    customModelName=custom_model_name,\n",
    "    roleArn=bedrock_role_arn,\n",
    "    baseModelIdentifier='arn:aws:bedrock:us-east-1::foundation-model/amazon.nova-micro-v1:0:128k', #Base model arn for nova micro 128k context length\n",
    "    trainingDataConfig={\n",
    "        's3Uri': train_s3_path\n",
    "    },\n",
    "    validationDataConfig={\n",
    "    'validators': [{\n",
    "        's3Uri': test_s3_path\n",
    "    }]\n",
    "},\n",
    "\n",
    "    outputDataConfig={\n",
    "        's3Uri': output_s3_path\n",
    "    },\n",
    "    hyperParameters={\n",
    "        'epochCount': '1',\n",
    "        'batchSize': '1',\n",
    "        'learningRate': '0.00001'\n",
    "    \n",
    "    },\n",
    "    customizationType='FINE_TUNING'\n",
    ")\n",
    "\n",
    "job_arn = response['jobArn']\n",
    "print(f\"Job ARN: {job_arn}\")\n",
    "print(f\"Job Name: {job_name}\")\n",
    "print(f\"Custom Model Name: {custom_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bfd24e",
   "metadata": {},
   "source": [
    "## 4. Monitor Training Job\n",
    "\n",
    "Monitor the fine-tuning job until completion. This may take 30-60 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d949ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_customization_job(job_arn, check_interval=60):\n",
    "    \"\"\"Monitor the fine-tuning job until completion.\"\"\"\n",
    "    print(\"Monitoring Fine-tuning Job...\")\n",
    "    print(\"This may take 30-60 minutes depending on dataset size...\")\n",
    "    \n",
    "    while True:\n",
    "        response = bedrock.get_model_customization_job(jobIdentifier=job_arn)\n",
    "        status = response['status']\n",
    "        \n",
    "        print(f\"Status: {status} - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        \n",
    "        if status == 'Completed':\n",
    "            print(\"\\n✓ Fine-tuning completed successfully!\")\n",
    "            return response['outputModelArn']\n",
    "        elif status in ['Failed', 'Stopped']:\n",
    "            failure_message = response.get('failureMessage', 'No failure message provided')\n",
    "            raise Exception(f\"Job {status}: {failure_message}\")\n",
    "        elif status == 'InProgress':\n",
    "            if 'trainingMetrics' in response:\n",
    "                metrics = response['trainingMetrics']\n",
    "                print(f\"  Training Loss: {metrics.get('trainingLoss', 'N/A')}\")\n",
    "        \n",
    "        time.sleep(check_interval)\n",
    "\n",
    "# Wait for job to complete\n",
    "custom_model_arn = wait_for_customization_job(job_arn)\n",
    "print(f\"\\nCustom Model ARN: {custom_model_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6faa649f",
   "metadata": {},
   "source": [
    "---\n",
    "## Wait Until the ^^ Training Job ^^ Completes Above!\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb283738",
   "metadata": {},
   "source": [
    "## 5. Deploy Model for Inference\n",
    "\n",
    "Deploy the fine-tuned model for on-demand inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c447e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create on-demand inferencing deployment for custom model\n",
    "def create_model_deployment(custom_model_arn):\n",
    "\n",
    "    try:\n",
    "        print(f\"Creating on-demand inferencing deployment for model: {custom_model_arn}\")\n",
    "        deployment_name = f'nova-sql-deployment-{timestamp}'\n",
    "        \n",
    "        # Create the deployment\n",
    "        response = bedrock.create_custom_model_deployment(\n",
    "            modelArn=custom_model_arn,\n",
    "            modelDeploymentName=deployment_name,\n",
    "            description=f\"finetuned text to sql model: {custom_model_arn}\",\n",
    "        )\n",
    "        print(response)\n",
    "        \n",
    "        # Get the deployment ARN\n",
    "        deployment_arn = response.get('customModelDeploymentArn')\n",
    "        \n",
    "        print(f\"Deployment request submitted. Deployment ARN: {deployment_arn}\")\n",
    "        return deployment_arn\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating deployment: {e}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6de1af1-89b3-4229-ac98-0fec637a89f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import check_deployment_status\n",
    "\n",
    "deployment_arn = create_model_deployment(custom_model_arn)\n",
    "\n",
    "if deployment_arn:\n",
    "    while True:\n",
    "        status = check_deployment_status(bedrock, deployment_arn)\n",
    "        print(f\"Model is in {status} phase\")\n",
    "        \n",
    "        if status == 'Active':\n",
    "            break\n",
    "        elif status == 'Failed':\n",
    "            raise Exception(f\"Deployment failed: {deployment_arn}\")\n",
    "        \n",
    "        time.sleep(15) #sleep for 15 seconds \n",
    "\n",
    "print(f\"Use the deployment Arn for inferencing: {deployment_arn}\")\n",
    "%store deployment_arn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a0b0e9",
   "metadata": {},
   "source": [
    "## 6. Test the Fine-Tuned Model\n",
    "\n",
    "Test the model with sample queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763f8ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_model(model_arn, system_prompt, user_prompt):\n",
    "    \"\"\"Invoke the fine-tuned model for inference.\"\"\"\n",
    "    request_body = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\"text\": user_prompt}]\n",
    "            }\n",
    "        ],\n",
    "        \"system\": [{\"text\": system_prompt}],\n",
    "        \"inferenceConfig\": {\n",
    "            \"maxTokens\": 512,\n",
    "            \"temperature\": 0.0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    response = bedrock_runtime.converse(\n",
    "        modelId=model_arn,\n",
    "        **request_body\n",
    "    )\n",
    "    \n",
    "    return response['output']['message']['content'][0]['text']\n",
    "\n",
    "print(\"Testing Fine-tuned Model...\")\n",
    "\n",
    "# Test with a sample from test set\n",
    "test_sample = test_data[0]\n",
    "system_prompt = test_sample['system'][0]['text']\n",
    "user_message = test_sample['messages'][0]['content'][0]['text']\n",
    "expected_sql = test_sample['messages'][1]['content'][0]['text']\n",
    "\n",
    "print(\"Question:\")\n",
    "print(user_message[:200] + \"...\")\n",
    "print(\"\\nExpected SQL:\")\n",
    "print(expected_sql)\n",
    "\n",
    "generated_sql = invoke_model(deployment_arn, system_prompt, user_message)\n",
    "print(\"\\nGenerated SQL:\")\n",
    "print(generated_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a097f9",
   "metadata": {},
   "source": [
    "## 7. Comprehensive Evaluation\n",
    "\n",
    "Run evaluation on multiple test samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dda8f1b-a71e-455f-a440-514a9f3427da",
   "metadata": {},
   "source": [
    "### Evaluation using an LLM as a judge\n",
    "\n",
    "Since we have access to the \"right\" answer, we can evaluate similarity between the SQL queries returned by the fine-tuned Llama model and the right answer. Evaluation can be a bit tricky, since there is no single metric that evaluates semantic and syntactic similarity between two SQL queries. One alternative is to use a more powerful LLM, like Claude 3 Sonnet, to measure the similarity between the two SQL queries (LLM as a judge)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f30b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare 100 evaluation samples, prompt our fine-tuned model for the sql generation task then ask our judge model to give a score\n",
    "from utils import ( ask_nova_micro,ask_claude,\n",
    "    prepare_evaluation_samples,\n",
    "    test_sql_generation,\n",
    "    get_score,\n",
    "    metrics_test,\n",
    "    run_cold_and_warm_benchmark,\n",
    "    plot_ttft_comparison\n",
    ")\n",
    "\n",
    "eval_samples = prepare_evaluation_samples(test_data, num_samples=100)\n",
    "# Show a sample\n",
    "print('\\nSample evaluation record:')\n",
    "print(json.dumps(eval_samples[0], indent=2))\n",
    "\n",
    "results = test_sql_generation(eval_samples, deployment_arn)\n",
    "\n",
    "scores = []\n",
    "print(\"Grading responses with LLM Judge model\")\n",
    "for result in results:\n",
    "    if result['status'] == 'success':\n",
    "        response = float(get_score(\n",
    "            result['system_prompt'],\n",
    "            result['query'],\n",
    "            result['expected_sql'],\n",
    "            result['generated_sql']\n",
    "        ))\n",
    "        scores.append(response)\n",
    "       \n",
    "\n",
    "print(\"Assigned scores: \", scores)\n",
    "print(\"The average score of the fine tuned model is: \", sum(scores)/float(len(scores)), \"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fc3f09",
   "metadata": {},
   "source": [
    "## Operational Metrics for Nova Micro SFT\n",
    "Now lets test the latency of our Fine tuned Nova Micro LLM by measuring:\n",
    "\n",
    "* Time To First Token (TTFS) - Cold start time to first token for loading Lora adapters and invoking the model should be is 1 second\n",
    "* Overall Throughput per Second (OTPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b01cf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_results= metrics_test(\n",
    "    model_id= deployment_arn,\n",
    "    system = \"You are a powerful text-to-SQL model. Your job is to answer questions about a database. You can use the following table schema for context: CREATE TABLE table_name_6 (winner_and_score VARCHAR, week VARCHAR)\", \n",
    "    prompt=\"Return the SQL query that answers the following question: who is the winner and score for the week of august 9?\"\n",
    ")\n",
    "print(f\"TTFT: {metrics_results['ttft_ms']:.2f}ms\")\n",
    "print(f\"OTPS: {metrics_results['otps']:.2f} tokens/s\")\n",
    "print(f\"Total end-to-end latency: {metrics_results['total_time_ms']:.2f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c490d9a3-5d7b-4a57-aeac-7b71c324e394",
   "metadata": {},
   "source": [
    "Now lets increase our test cases to get an average result for cold start time as well as warm start time to first token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1188a575-37f5-42c9-8294-6fd8039325a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For quick testing (3 cold starts, 10 warm calls, 2 min wait)\n",
    "\n",
    "results = run_cold_and_warm_benchmark(\n",
    "    model_id=deployment_arn,\n",
    "    system = \"You are a powerful text-to-SQL model. Your job is to answer questions about a database. You can use the following table schema for context: CREATE TABLE table_name_6 (winner_and_score VARCHAR, week VARCHAR)\", \n",
    "    prompt=\"Return the SQL query that answers the following question: who is the winner and score for the week of august 9?\",\n",
    "    num_cold_starts=5,\n",
    "    num_warm_calls=10,\n",
    "    cold_start_wait=600  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc81926-7cbc-416b-8f57-3a7f56ac678e",
   "metadata": {},
   "source": [
    "## Plot the results "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78bb2b8-7853-464b-99b8-db04f1f090fd",
   "metadata": {},
   "source": [
    "## Use case price comparison analysis \n",
    "\n",
    "Below we run an analysis of running a similar workload on a self-hosted ec2 instance as well as a Sagemaker real-time endpoint \n",
    "For this analysis we make the following assumptions \n",
    "\n",
    "* Users = 100\n",
    "* Queries per day = 10\n",
    "* Usage days 30 - 8(weekend) = 22\n",
    "* Total queries per month = users * queries per day * 22 = 22,000\n",
    "* Compute hours = 12\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72aa2c6d-35a0-4e8b-9ca0-764be665d950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Scenario\n",
    "users = 100\n",
    "queries_per_day = 10\n",
    "total_queries_per_month = users * queries_per_day * 22\n",
    "\n",
    "# Average tokens per query\n",
    "avg_input_tokens = 80\n",
    "avg_output_tokens = 60\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b894864c-336a-4872-9f58-7be69483188d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Bedrock On-Demand \n",
    "# ============================================================================\n",
    "\n",
    "input_cost = (total_queries_per_month * avg_input_tokens / 1000) * 0.000035\n",
    "output_cost = (total_queries_per_month * avg_output_tokens / 1000) * 0.00014\n",
    "bedrock_on_demand = input_cost + output_cost\n",
    "\n",
    "print(f\"\\n Bedrock On-Demand: ${bedrock_on_demand:.2f}/month\")\n",
    "print(f\"   Cost per query: ${bedrock_on_demand/total_queries_per_month:.6f}\")\n",
    "\n",
    "# Self-Hosted on EC2  g5.12xlarge\n",
    "# ============================================================================\n",
    "\n",
    "ec2_hourly = 5.672  \n",
    "ec2_compute = ec2_hourly * 12 * 22\n",
    "\n",
    "print(f\"\\n Self-Hosted (EC2 g5.12xlarge): ${ec2_compute:.2f}/month\")\n",
    "print(f\"   Cost per query: ${ec2_compute/total_queries_per_month:.4f}\")\n",
    "\n",
    "# SageMaker Endpoint\n",
    "# ============================================================================\n",
    "\n",
    "sagemaker_hourly = ec2_hourly+1.418  # (EC2 + SageMaker overhead)\n",
    "sagemaker_compute = sagemaker_hourly * 12 * 22\n",
    "sagemaker_total = sagemaker_compute + ec2_hourly\n",
    "\n",
    "print(f\"\\n SageMaker Endpoint: ${sagemaker_total:.2f}/month\")\n",
    "print(f\"   Compute: ${sagemaker_compute:.2f}\")\n",
    "print(f\"   Cost per query: ${sagemaker_total/total_queries_per_month:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# VISUALIZATION\n",
    "# ============================================================================\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Total Monthly Cost\n",
    "options = ['Bedrock\\nOn-Demand', 'Self-Hosted\\nEC2', 'SageMaker\\nEndpoint']\n",
    "costs = [bedrock_on_demand, ec2_compute, sagemaker_total]\n",
    "colors = ['#2ecc71', '#e74c3c', '#f39c12']\n",
    "\n",
    "bars = ax1.bar(options, costs, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "\n",
    "for bar, cost in zip(bars, costs):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'${cost:,.0f}',\n",
    "            ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "ax1.set_ylabel('Monthly Cost ($)', fontsize=13, fontweight='bold')\n",
    "ax1.set_title(f'Monthly Cost Comparison (Verified Pricing)\\n{users} users, {queries_per_day} queries/user/day',\n",
    "             fontsize=14, fontweight='bold')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Highlight winner\n",
    "winner_idx = np.argmin(costs)\n",
    "ax1.get_children()[winner_idx].set_edgecolor('green')\n",
    "ax1.get_children()[winner_idx].set_linewidth(4)\n",
    "\n",
    "# Plot 2: Cost per Query\n",
    "cost_per_query = [c / total_queries_per_month for c in costs]\n",
    "\n",
    "bars2 = ax2.barh(options, cost_per_query, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "\n",
    "for bar, cpq in zip(bars2, cost_per_query):\n",
    "    width = bar.get_width()\n",
    "    ax2.text(width, bar.get_y() + bar.get_height()/2.,\n",
    "            f' ${cpq:.5f}',\n",
    "            ha='left', va='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "ax2.set_xlabel('Cost Per Query ($)', fontsize=13, fontweight='bold')\n",
    "ax2.set_title('Cost Efficiency', fontsize=14, fontweight='bold')\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('cost_comparison_verified.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc08328",
   "metadata": {},
   "source": [
    "## Break Even Scale\n",
    "\n",
    "Now we can see that finetuning our model for on demand usage is significantly cheaper, but at what scale do the other options st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa5349d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BREAK-EVEN ANALYSIS\")\n",
    "\n",
    "cost_per_query_bedrock = bedrock_on_demand / total_queries_per_month\n",
    "fixed_self_hosted_costs = ec2_compute\n",
    "\n",
    "break_even_queries = fixed_self_hosted_costs / cost_per_query_bedrock\n",
    "break_even_users = break_even_queries / (queries_per_day * 30)\n",
    "\n",
    "print(f\"\\nSelf-hosted breaks even at:\")\n",
    "print(f\"  {break_even_queries:,.0f} queries/month\")\n",
    "print(f\"   = {break_even_users:,.0f} users @ {queries_per_day} queries/day\")\n",
    "print(f\"   = {break_even_users/users:.0f}x your current scale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988ea479-5bc8-4c0a-9c5b-e68b35525c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Throughput testing on the deployed fine-tuned model to measure tokens per second, time to first token (TTFT),\n",
    "\n",
    "from utils import test_model_throughput, visualize_throughput_results\n",
    "# Run the test\n",
    "\n",
    "throughput_results = test_model_throughput(deployment_arn)\n",
    "\n",
    "# Optionally create visualizations\n",
    "visualize_throughput_results(throughput_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13e1663-7569-49b2-b372-52ee81fd1c40",
   "metadata": {},
   "source": [
    "### Compare TTFT for Base Nova Micro Model to our SFT Model\n",
    "Benchmark the Time to First Token (TTFT) performance between the base Amazon Nova Micro model and our supervised fine-tuned (SFT) LoRA model to quantify latency and validate that fine-tuning maintains acceptable response time characteristics while delivering better accuracy for text-to-SQL generation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c799d3a1-8b26-43a6-a560-ce02cebeefdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Becasue the base model does not have knowledge of the SQL data we will ask a generic question to both models\n",
    "\n",
    "from utils import compare_models\n",
    "\n",
    "base_model_id = \"us.amazon.nova-micro-v1:0\"\n",
    "\n",
    "system_prompt = \"You are a helpful assistant.\"\n",
    "test_prompt = \"What are the performance specs of a bmw x5m and how does it compare with the porsche macan turbo\"\n",
    "\n",
    "results = compare_models(\n",
    "    custom_model_arn=deployment_arn,\n",
    "    base_model_id=base_model_id,\n",
    "    system=system_prompt,\n",
    "    prompt=test_prompt,\n",
    "    num_runs=10  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5227a9a-5486-45b5-9883-5efffa0bb44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the percentage in latency that the SFT model has in generating TTFT\n",
    "ttft_percentage_difference = ((381.49-356.28)/356.28)*100\n",
    "print(\"Our Custom model has a Time to first token differene of: \", ttft_percentage_difference,\"%\")\n",
    "otps_percentage_difference = ((184.56-253.57)/253.57)*100\n",
    "print(\"Our Custom model has a Output per second differene of: \", otps_percentage_difference,\"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427a29ee",
   "metadata": {},
   "source": [
    "---\n",
    "## Cleanup Resources\n",
    "\n",
    "**IMPORTANT:** Run this cell to delete all resources created in this notebook to avoid ongoing charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f69f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup - Delete all resources\n",
    "import shutil\n",
    "\n",
    "# Delete provisioned throughput (if created)\n",
    "try:\n",
    "    bedrock.delete_custom_model_deployment(customModelDeploymentIdentifier=deployment_arn)\n",
    "    print(\"Custom model deleted\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Delete custom model\n",
    "try:\n",
    "    bedrock.delete_custom_model(modelIdentifier=custom_model_arn)\n",
    "    print(\"Custom model deleted\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Delete IAM role\n",
    "try:\n",
    "    iam_client.delete_role_policy(RoleName='BedrockNovaCustomModelRole', PolicyName='BedrockNovaS3Access')\n",
    "    iam_client.delete_role(RoleName='BedrockNovaCustomModelRole')\n",
    "    print(\"IAM resources deleted\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Delete local data\n",
    "if os.path.exists('data'):\n",
    "    shutil.rmtree('data')\n",
    "    print(\"Local data deleted\")\n",
    "\n",
    "print(\"\\nCleanup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed6196b-f54f-4dca-8429-3af1adcb336a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
