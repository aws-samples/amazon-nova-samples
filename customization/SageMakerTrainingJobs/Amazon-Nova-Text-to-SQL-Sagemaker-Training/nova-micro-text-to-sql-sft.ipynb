{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4044ee2-645c-40d1-9d59-d768e28c681c",
   "metadata": {},
   "source": [
    "# Supervised Fine-Tuning (SFT) with Parameter Efficient Fine Tuning (PEFT LoRA) of Amazon Nova for Text-to-SQL Generation\n",
    "\n",
    "\n",
    "\n",
    "This notebook demonstrates Supervised Fine-Tuning (SFT) with Parameter-Efficient Fine-Tuning (PEFT) of Amazon Nova Micro for text-to-SQL generation using Amazon SageMaker Training Job. SFT is a technique that allows fine-tuning language models on specific tasks using labeled examples, while PEFT enables efficient fine-tuning by updating only a small subset of the model's parameters.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook illustrates the process of fine-tuning Amazon Nova Micro for text-to-SQL generation and demonstrates the complete workflow from data preparation to model evaluation. The approach combines proven techniques from multiple sources:\n",
    "\n",
    "- **Data Preparation**: SQL dataset converted to bedrock-conversation-2024 schema \n",
    "- **Training Method**: SFT using SageMaker training job LoRA approach using Nova-specific recipes and configurations\n",
    "- **Use Case**: Text-to-SQL generation with comprehensive evaluation and inference pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea10d748-6125-4899-b5b0-d83afe7578d0",
   "metadata": {},
   "source": [
    "## Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "11130253-ccc6-4b22-af9d-9c7d75b716e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T21:47:51.988932Z",
     "iopub.status.busy": "2025-11-19T21:47:51.988666Z",
     "iopub.status.idle": "2025-11-19T21:47:54.054352Z",
     "shell.execute_reply": "2025-11-19T21:47:54.053559Z",
     "shell.execute_reply.started": "2025-11-19T21:47:51.988912Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install sagemaker datasets pandas scikit-learn --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8b6105-ecec-4213-b56d-589238844dca",
   "metadata": {},
   "source": [
    "## Setup and Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1ce51663-0171-4d54-b16e-f85e3cadb692",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T21:07:50.871966Z",
     "iopub.status.busy": "2025-11-24T21:07:50.871690Z",
     "iopub.status.idle": "2025-11-24T21:07:51.204300Z",
     "shell.execute_reply": "2025-11-24T21:07:51.203728Z",
     "shell.execute_reply.started": "2025-11-24T21:07:50.871942Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::133856113780:role/service-role/AmazonSageMaker-ExecutionRole-20250805T200090\n",
      "sagemaker bucket: sagemaker-us-east-1-133856113780\n",
      "sagemaker bucket prefix: None\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import json\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import utils\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "sagemaker_session_bucket = None\n",
    "\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "bucket_name = sess.default_bucket()\n",
    "default_prefix = sess.default_bucket_prefix\n",
    "region = \"us-east-1\" # Currently US-EAST-1 is the only region where nova model customization is supported \n",
    "\n",
    "\n",
    "print(f'sagemaker role arn: {role}')\n",
    "print(f'sagemaker bucket: {bucket_name}')\n",
    "print(f'sagemaker bucket prefix: {default_prefix}')\n",
    "print(f'sagemaker session region: {region}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5d160878-db73-4fdd-9868-18bc2304be34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T21:07:51.304852Z",
     "iopub.status.busy": "2025-11-24T21:07:51.304593Z",
     "iopub.status.idle": "2025-11-24T21:07:51.307538Z",
     "shell.execute_reply": "2025-11-24T21:07:51.307000Z",
     "shell.execute_reply.started": "2025-11-24T21:07:51.304831Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# S3 prefix for training data\n",
    "training_input_path = f's3://{sess.default_bucket()}/datasets/nova-sql-context'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-prep",
   "metadata": {},
   "source": [
    "## 1. Data Preparation\n",
    "\n",
    "We'll use the [sql-create-context](https://huggingface.co/datasets/b-mc2/sql-create-context) dataset and format it according to the bedrock-conversation-2024 schema that Nova expects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103df6fc-5ad6-48c9-8e48-aad6303ee917",
   "metadata": {},
   "source": [
    "### Step 1a: Load and Explore the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "load-data",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T21:08:00.686011Z",
     "iopub.status.busy": "2025-11-24T21:08:00.685738Z",
     "iopub.status.idle": "2025-11-24T21:08:00.955940Z",
     "shell.execute_reply": "2025-11-24T21:08:00.955418Z",
     "shell.execute_reply.started": "2025-11-24T21:08:00.685991Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 78577\n",
      "Sample record:\n",
      "{'answer': 'SELECT Theme FROM farm_competition ORDER BY YEAR', 'question': 'What are the themes of farm competitions sorted by year in ascending order?', 'context': 'CREATE TABLE farm_competition (Theme VARCHAR, YEAR VARCHAR)'}\n"
     ]
    }
   ],
   "source": [
    "# Load the SQL dataset\n",
    "dataset = load_dataset('b-mc2/sql-create-context')\n",
    "print(f'Dataset size: {len(dataset[\"train\"])}')\n",
    "print('Sample record:')\n",
    "print(dataset['train'][19])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d6d265-01e6-4828-a3ef-39eb02506478",
   "metadata": {},
   "source": [
    "### Step 1b: Convert to Bedrock Conversation Format\n",
    "\n",
    "Nova expects data in the bedrock-conversation-2024 format:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"schemaVersion\": \"bedrock-conversation-2024\",\n",
    "  \"system\": [\n",
    "    {\n",
    "      \"text\": \"System prompt content\"\n",
    "    }\n",
    "  ],\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"text\": \"User question\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"text\": \"Assistant response\"\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "convert-format",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T21:08:02.489981Z",
     "iopub.status.busy": "2025-11-24T21:08:02.489720Z",
     "iopub.status.idle": "2025-11-24T21:08:02.494007Z",
     "shell.execute_reply": "2025-11-24T21:08:02.493520Z",
     "shell.execute_reply.started": "2025-11-24T21:08:02.489962Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample converted record:\n",
      "{\n",
      "  \"schemaVersion\": \"bedrock-conversation-2024\",\n",
      "  \"system\": [\n",
      "    {\n",
      "      \"text\": \"You are a powerful text-to-SQL model. Your job is to answer questions about a database. You can use the following table schema for context: CREATE TABLE head (age INTEGER)\"\n",
      "    }\n",
      "  ],\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": [\n",
      "        {\n",
      "          \"text\": \"Return the SQL query that answers the following question: How many heads of the departments are older than 56 ?\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": [\n",
      "        {\n",
      "          \"text\": \"SELECT COUNT(*) FROM head WHERE age > 56\"\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#Here we use a helper function to convert our data into the bedrock conversation format needed for finetuning our model\n",
    "from utils import create_bedrock_conversation\n",
    "\n",
    "\n",
    "sample_converted = create_bedrock_conversation(dataset['train'][0])\n",
    "print('Sample converted record:')\n",
    "print(json.dumps(sample_converted, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f824f15-365c-41b5-97a7-bc1581f70e9d",
   "metadata": {},
   "source": [
    "### Step 1c: Create Train/Test Split and Convert Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "process-data",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T21:08:10.322021Z",
     "iopub.status.busy": "2025-11-24T21:08:10.321753Z",
     "iopub.status.idle": "2025-11-24T21:08:10.386760Z",
     "shell.execute_reply": "2025-11-24T21:08:10.386236Z",
     "shell.execute_reply.started": "2025-11-24T21:08:10.322000Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 78577\n",
      "Using 2200 samples (to get ~200 training samples)\n",
      "Converted 2200 records\n",
      "Trimmed training data to 200 samples\n",
      "Final training samples: 200\n",
      "Final test samples: 220\n"
     ]
    }
   ],
   "source": [
    "MAX_TRAINING_SAMPLES = 200\n",
    "total_samples = min(len(dataset['train']), MAX_TRAINING_SAMPLES + 2000)\n",
    "\n",
    "print(f'Original dataset size: {len(dataset[\"train\"])}')\n",
    "print(f'Using {total_samples} samples (to get ~{MAX_TRAINING_SAMPLES} training samples)')\n",
    "\n",
    "# Convert limited dataset\n",
    "converted_data = []\n",
    "for i, record in enumerate(dataset['train']):\n",
    "    if i >= total_samples:\n",
    "        break\n",
    "    converted_data.append(create_bedrock_conversation(record))\n",
    "\n",
    "print(f'Converted {len(converted_data)} records')\n",
    "\n",
    "# Create train/test split\n",
    "train_data, test_data = train_test_split(converted_data, test_size=0.1, random_state=42)\n",
    "\n",
    "# Ensure training data doesn't exceed 20k limit\n",
    "if len(train_data) > MAX_TRAINING_SAMPLES:\n",
    "    train_data = train_data[:MAX_TRAINING_SAMPLES]\n",
    "    print(f'Trimmed training data to {MAX_TRAINING_SAMPLES} samples')\n",
    "\n",
    "print(f'Final training samples: {len(train_data)}')\n",
    "print(f'Final test samples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ecbf18-5830-4361-8978-b3399d95ec42",
   "metadata": {},
   "source": [
    "### Step 1d: Save Data in JSONL Format and upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "save-data",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T21:08:14.038039Z",
     "iopub.status.busy": "2025-11-24T21:08:14.037779Z",
     "iopub.status.idle": "2025-11-24T21:08:14.047297Z",
     "shell.execute_reply": "2025-11-24T21:08:14.046784Z",
     "shell.execute_reply.started": "2025-11-24T21:08:14.038020Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets saved successfully!\n",
      "Training file: data/train_dataset.jsonl (200 records)\n",
      "Test file: data/test_dataset.jsonl (220 records)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create data directory\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# Save training data\n",
    "with open('data/train_dataset.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for record in train_data:\n",
    "        f.write(json.dumps(record, separators=(',', ':')) + '\\n')\n",
    "\n",
    "# Save test data\n",
    "with open('data/test_dataset.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for record in test_data:\n",
    "        f.write(json.dumps(record, separators=(',', ':')) + '\\n')\n",
    "\n",
    "print('Datasets saved successfully!')\n",
    "print(f'Training file: data/train_dataset.jsonl ({len(train_data)} records)')\n",
    "print(f'Test file: data/test_dataset.jsonl ({len(test_data)} records)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "upload-s3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T21:08:16.906195Z",
     "iopub.status.busy": "2025-11-24T21:08:16.905917Z",
     "iopub.status.idle": "2025-11-24T21:08:17.203505Z",
     "shell.execute_reply": "2025-11-24T21:08:17.203012Z",
     "shell.execute_reply.started": "2025-11-24T21:08:16.906173Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data uploaded to: s3://sagemaker-us-east-1-133856113780/datasets/nova-sql-context/train_dataset.jsonl\n",
      "Test data uploaded to: s3://sagemaker-us-east-1-133856113780/datasets/nova-sql-context/test_dataset.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Upload datasets to S3\n",
    "train_s3_path = sagemaker.s3.S3Uploader.upload('data/train_dataset.jsonl', training_input_path)\n",
    "test_s3_path = sagemaker.s3.S3Uploader.upload('data/test_dataset.jsonl', training_input_path)\n",
    "\n",
    "print('Training data uploaded to:', train_s3_path)\n",
    "print('Test data uploaded to:', test_s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-setup",
   "metadata": {},
   "source": [
    "## Nova Micro Fine-Tuning Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "training-utils",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T21:08:21.397975Z",
     "iopub.status.busy": "2025-11-24T21:08:21.397712Z",
     "iopub.status.idle": "2025-11-24T21:08:21.486594Z",
     "shell.execute_reply": "2025-11-24T21:08:21.486112Z",
     "shell.execute_reply.started": "2025-11-24T21:08:21.397955Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b841d56ad8b464cac91aee6428509a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f97111ff5d5b4655a016609b1909bbc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/220 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 200 training samples\n",
      "Prepared 220 test samples\n",
      "\n",
      "Sample prepared record:\n",
      "{'system': [{'text': 'You are a powerful text-to-SQL model. Your job is to answer questions about a database. You can use the following table schema for context: CREATE TABLE editor (Name VARCHAR, Age VARCHAR)'}], 'messages': [{'content': [{'text': 'Return the SQL query that answers the following question: What is the name of the youngest editor?'}], 'role': 'user'}, {'content': [{'text': 'SELECT Name FROM editor ORDER BY Age LIMIT 1'}], 'role': 'assistant'}]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "from random import randint\n",
    "from utils import prepare_dataset_for_nova\n",
    "\n",
    "# Convert to datasets format\n",
    "train_df = pd.DataFrame(train_data)\n",
    "test_df = pd.DataFrame(test_data)\n",
    "\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "# Apply Nova preparation\n",
    "train_dataset_nova = dataset_dict['train'].map(\n",
    "    prepare_dataset_for_nova, \n",
    "    remove_columns=train_dataset.features\n",
    ")\n",
    "\n",
    "test_dataset_nova = dataset_dict['test'].map(\n",
    "    prepare_dataset_for_nova,\n",
    "    remove_columns=test_dataset.features\n",
    ")\n",
    "\n",
    "print(f'Prepared {len(train_dataset_nova)} training samples')\n",
    "print(f'Prepared {len(test_dataset_nova)} test samples')\n",
    "print('\\nSample prepared record:')\n",
    "print(train_dataset_nova[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nova-training-approach",
   "metadata": {},
   "source": [
    "# 2. Fine tuning the model\n",
    "\n",
    "In this step we are going to fine tune Nova Micro using a PyTorch estimator to run the supervised fine-tuning job with LoRa a Parameter-Efficient Fine-Tuning (PEFT) technique. we will also use a [Nova recipe](https://docs.aws.amazon.com/sagemaker/latest/dg/nova-model-recipes.html), which is a YAML configuration file that provides details to SageMaker AI on how to run your model customization job,it defines optimization settings, and includes any additional options required to fine-tune or train the model successfully. The code will be packaged to run inside a SageMaker training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d5e5e1c6-f090-46d9-a444-d3a2163a7d35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T21:08:36.788947Z",
     "iopub.status.busy": "2025-11-24T21:08:36.788674Z",
     "iopub.status.idle": "2025-11-24T21:08:36.795198Z",
     "shell.execute_reply": "2025-11-24T21:08:36.794660Z",
     "shell.execute_reply.started": "2025-11-24T21:08:36.788927Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['schemaVersion', 'system', 'messages'],\n",
       "    num_rows: 1\n",
       "})"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_row_dataset = train_dataset.select(range(1))  # Gets indices 0 through 0\n",
    "first_row_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "nova-config",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T21:09:13.715778Z",
     "iopub.status.busy": "2025-11-24T21:09:13.715520Z",
     "iopub.status.idle": "2025-11-24T21:09:13.719623Z",
     "shell.execute_reply": "2025-11-24T21:09:13.719074Z",
     "shell.execute_reply.started": "2025-11-24T21:09:13.715758Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model ID: nova-micro/prod\n",
      "Recipe: fine-tuning/nova/nova_micro_g5_g6_48x_gpu_lora_sft\n",
      "Instance type: ml.g5.48xlarge\n",
      "Instance count: 1\n",
      "Image URI: 708977205387.dkr.ecr.us-east-1.amazonaws.com/nova-fine-tune-repo:SM-TJ-SFT-latest\n"
     ]
    }
   ],
   "source": [
    "# Nova configuration\n",
    "model_id = \"nova-micro/prod\"\n",
    "recipe = \"fine-tuning/nova/nova_micro_g5_g6_48x_gpu_lora_sft\"\n",
    "instance_type = \"ml.g5.48xlarge\" \n",
    "instance_count = 1 \n",
    "\n",
    "# Nova-specific image URI\n",
    "image_uri = f\"708977205387.dkr.ecr.{sess.boto_region_name}.amazonaws.com/nova-fine-tune-repo:SM-TJ-SFT-latest\"\n",
    "\n",
    "print(f'Model ID: {model_id}')\n",
    "print(f'Recipe: {recipe}')\n",
    "print(f'Instance type: {instance_type}')\n",
    "print(f'Instance count: {instance_count}')\n",
    "print(f'Image URI: {image_uri}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cde5bca-eeba-46cb-a84b-e53fb524b924",
   "metadata": {},
   "source": [
    "## Step 2a: Create PyTorch estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "create-estimator",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T21:10:07.971963Z",
     "iopub.status.busy": "2025-11-24T21:10:07.971703Z",
     "iopub.status.idle": "2025-11-24T21:10:08.185878Z",
     "shell.execute_reply": "2025-11-24T21:10:08.185358Z",
     "shell.execute_reply.started": "2025-11-24T21:10:07.971944Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into '/tmp/launcher_j4lg7sdf'...\n",
      "INFO:sagemaker:Remote debugging, profiler and debugger hooks are disabled for Nova recipes.\n",
      "WARNING:sagemaker:Using instance_count argument to estimator to set number of nodes. Ignoring run -> replicas in recipe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training job name: train-nova-micro-sql-peft-sft\n",
      "Output path: s3://sagemaker-us-east-1-133856113780/train-nova-micro-sql-peft-sft\n",
      "PyTorch estimator created successfully!\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "# Define Training Job Name\n",
    "job_name = f\"train-{model_id.split('/')[0].replace('.', '-')}-sql-peft-sft\"\n",
    "\n",
    "# Define OutputDataConfig path\n",
    "if default_prefix:\n",
    "    output_path = f\"s3://{bucket_name}/{default_prefix}/{job_name}\"\n",
    "else:\n",
    "    output_path = f\"s3://{bucket_name}/{job_name}\"\n",
    "\n",
    "# Recipe overrides\n",
    "recipe_overrides = {\n",
    "    \"run\": {\n",
    "        \"replicas\": instance_count,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Create PyTorch estimator\n",
    "estimator = PyTorch(\n",
    "    output_path=output_path,\n",
    "    base_job_name=job_name,\n",
    "    role=role,\n",
    "    disable_output_compression=True,\n",
    "    disable_profiler=True,\n",
    "    debugger_hook_config=False,\n",
    "    instance_count=instance_count,\n",
    "    instance_type=instance_type,\n",
    "    training_recipe=recipe,\n",
    "    recipe_overrides=recipe_overrides,\n",
    "    max_run=432000,  \n",
    "    sagemaker_session=sess,\n",
    "    image_uri=image_uri\n",
    ")\n",
    "\n",
    "print(f'Training job name: {job_name}')\n",
    "print(f'Output path: {output_path}')\n",
    "print('PyTorch estimator created successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "configure-data-channels",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T21:10:14.155961Z",
     "iopub.status.busy": "2025-11-24T21:10:14.155702Z",
     "iopub.status.idle": "2025-11-24T21:10:14.159925Z",
     "shell.execute_reply": "2025-11-24T21:10:14.159397Z",
     "shell.execute_reply.started": "2025-11-24T21:10:14.155942Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data channels configured:\n",
      "Training data: s3://sagemaker-us-east-1-133856113780/datasets/nova-sql-context/train_dataset.jsonl\n",
      "Validation data: s3://sagemaker-us-east-1-133856113780/datasets/nova-sql-context/test_dataset.jsonl\n",
      "Data type: Converse (Nova-specific)\n"
     ]
    }
   ],
   "source": [
    "# Configure Data Channels\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "train_input = TrainingInput(\n",
    "    s3_data=train_s3_path,\n",
    "    distribution=\"FullyReplicated\",\n",
    "    s3_data_type=\"Converse\",  # Important: Nova uses \"Converse\" data type\n",
    ")\n",
    "\n",
    "val_input = TrainingInput(\n",
    "    s3_data=test_s3_path,\n",
    "    distribution=\"FullyReplicated\",\n",
    "    s3_data_type=\"Converse\",\n",
    ")\n",
    "\n",
    "print('Data channels configured:')\n",
    "print(f'Training data: {train_s3_path}')\n",
    "print(f'Validation data: {test_s3_path}')\n",
    "print('Data type: Converse (Nova-specific)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0981ffa-63ac-4374-8e4d-2c56be3f703d",
   "metadata": {},
   "source": [
    "## Step 2b: Begin training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "start-training",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T21:10:30.011081Z",
     "iopub.status.busy": "2025-11-24T21:10:30.010815Z",
     "iopub.status.idle": "2025-11-24T21:10:31.119017Z",
     "shell.execute_reply": "2025-11-24T21:10:31.118474Z",
     "shell.execute_reply.started": "2025-11-24T21:10:30.011061Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.telemetry.telemetry_logging:SageMaker Python SDK will collect telemetry to help us better understand our user's needs, diagnose issues, and deliver additional features.\n",
      "To opt out of telemetry, please disable via TelemetryOptOut parameter in SDK defaults config. For more information, refer to https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk.\n",
      "INFO:sagemaker:Creating training-job with name: train-nova-micro-sql-peft-sft-2025-11-24-21-10-30-120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Nova Micro fine-tuning job for Text-to-SQL...\n",
      "Training job started: train-nova-micro-sql-peft-sft-2025-11-24-21-10-30-120\n",
      "Monitor progress in SageMaker console\n"
     ]
    }
   ],
   "source": [
    "# Start the Nova training job\n",
    "print('Starting Nova Micro fine-tuning job for Text-to-SQL...')\n",
    "\n",
    "# Launch training job with train and validation inputs\n",
    "estimator.fit(inputs={\"train\": train_input, \"validation\": val_input}, wait=False)\n",
    "\n",
    "print(f'Training job started: {estimator.latest_training_job.name}')\n",
    "print('Monitor progress in SageMaker console')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "monitor-training",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T21:10:47.364426Z",
     "iopub.status.busy": "2025-11-24T21:10:47.364170Z",
     "iopub.status.idle": "2025-11-24T21:40:49.573737Z",
     "shell.execute_reply": "2025-11-24T21:40:49.573217Z",
     "shell.execute_reply.started": "2025-11-24T21:10:47.364407Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Job: train-nova-micro-sql-peft-sft-2025-11-24-21-10-30-120\n",
      "Status: Completed\n",
      "Elapsed time: 30 minutes\n",
      "Started: 2025-11-24 21:13:05\n",
      "Ended: 2025-11-24 21:40:15\n",
      "\n",
      "Training completed successfully!\n",
      "Model artifacts: s3://sagemaker-us-east-1-133856113780/train-nova-micro-sql-peft-sft/train-nova-micro-sql-peft-sft-2025-11-24-21-10-30-120/output/model\n"
     ]
    }
   ],
   "source": [
    "# Monitor training job progress - checks every minute until completion\n",
    "import time\n",
    "from IPython.display import HTML, display, clear_output\n",
    "\n",
    "training_job_name = estimator.latest_training_job.name\n",
    "region = sess.boto_region_name\n",
    "\n",
    "print(f'Training Job Name: {training_job_name}')\n",
    "print('Monitoring job status (updates every minute)...')\n",
    "\n",
    "# Monitor job status\n",
    "sagemaker_client = boto3.client('sagemaker', region_name=region)\n",
    "start_time = time.time()\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        response = sagemaker_client.describe_training_job(TrainingJobName=training_job_name)\n",
    "        status = response['TrainingJobStatus']\n",
    "        \n",
    "        # Calculate elapsed time\n",
    "        elapsed_minutes = int((time.time() - start_time) / 60)\n",
    "        \n",
    "        # Clear previous output and show current status\n",
    "        clear_output(wait=True)\n",
    "        print(f'Training Job: {training_job_name}')\n",
    "        print(f'Status: {status}')\n",
    "        print(f'Elapsed time: {elapsed_minutes} minutes')\n",
    "        \n",
    "        if 'TrainingStartTime' in response:\n",
    "            start_time_str = response['TrainingStartTime'].strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print(f'Started: {start_time_str}')\n",
    "        \n",
    "        if status in ['Completed', 'Failed', 'Stopped']:\n",
    "            if 'TrainingEndTime' in response:\n",
    "                end_time_str = response['TrainingEndTime'].strftime('%Y-%m-%d %H:%M:%S')\n",
    "                print(f'Ended: {end_time_str}')\n",
    "            \n",
    "            if status == 'Completed':\n",
    "                print('\\nTraining completed successfully!')\n",
    "                if 'ModelArtifacts' in response:\n",
    "                    model_uri = response['ModelArtifacts']['S3ModelArtifacts']\n",
    "                    print(f'Model artifacts: {model_uri}')\n",
    "            elif status == 'Failed':\n",
    "                print('\\nTraining failed!')\n",
    "                if 'FailureReason' in response:\n",
    "                    print(f'Reason: {response[\"FailureReason\"]}')\n",
    "            else:\n",
    "                print(f'\\nTraining stopped with status: {status}')\n",
    "            \n",
    "            break\n",
    "        \n",
    "        print('\\nTraining in progress... (checking again in 60 seconds)')\n",
    "        time.sleep(60)  # Wait 1 minute before next check\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print('\\nMonitoring stopped by user')\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f'\\nError checking job status: {str(e)}')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wait-completion",
   "metadata": {},
   "source": [
    "---\n",
    "## Wait Until the ^^ Training Job ^^ Completes Above! \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5740e4f8-dac0-49e7-b753-d1f5f0bc7967",
   "metadata": {},
   "source": [
    "## Retrieve the Manifest File Containing the Custom Model Artifact URI\n",
    "\n",
    "The model artifacts are stored in an Amazon-managed S3 escrow bucket. To deploy the model, we need to fetch the URI path from the manifest file.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b58ed3ea-b5b3-453e-b580-afb43ad632c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T22:36:54.904258Z",
     "iopub.status.busy": "2025-11-24T22:36:54.903983Z",
     "iopub.status.idle": "2025-11-24T22:36:55.036409Z",
     "shell.execute_reply": "2025-11-24T22:36:55.035874Z",
     "shell.execute_reply.started": "2025-11-24T22:36:54.904239Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading manifest from: s3://sagemaker-us-east-1-133856113780/train-nova-micro-sql-peft-sft/train-nova-micro-sql-peft-sft-2025-11-24-21-10-30-120/output/output/manifest.json\n",
      "Checkpoint S3 URI found:\n",
      "s3://customer-escrow-133856113780-smtj-23f4ff73/train-nova-micro-sql-peft-sft-2025-11-24-21-10-30-120/384\n",
      "Stored in variable: checkpoint_uri\n"
     ]
    }
   ],
   "source": [
    "# Initialize S3 client\n",
    "s3 = boto3.client('s3', region_name=region)\n",
    "\n",
    "# Construct the manifest path from the output_path\n",
    "manifest_key = f\"{output_path.replace(f's3://{bucket_name}/', '')}/{training_job_name}/output/output/manifest.json\"\n",
    "print(f\"Reading manifest from: s3://{bucket_name}/{manifest_key}\")\n",
    "\n",
    "try:\n",
    "    # Download and read the manifest file\n",
    "    response = s3.get_object(Bucket=bucket_name, Key=manifest_key)\n",
    "    manifest_content = response['Body'].read().decode('utf-8')\n",
    "    manifest_data = json.loads(manifest_content)\n",
    "    \n",
    "    # Extract the checkpoint S3 URI\n",
    "    checkpoint_s3_uri = manifest_data.get('checkpoint_s3_bucket')\n",
    "    \n",
    "    if checkpoint_s3_uri:\n",
    "        print(f\"Checkpoint S3 URI found:\")\n",
    "        print(f\"{checkpoint_s3_uri}\")\n",
    "        \n",
    "        # Store it in a variable for later use\n",
    "        checkpoint_uri = checkpoint_s3_uri\n",
    "        print(f\"Stored in variable: checkpoint_uri\")\n",
    "    else:\n",
    "        print(\"'checkpoint_s3_bucket' key not found in manifest.json\")\n",
    "        print(\"Manifest contents:\")\n",
    "        print(json.dumps(manifest_data, indent=2))\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error reading manifest: {str(e)}\")\n",
    "    print(f\"Troubleshooting:\")\n",
    "    print(f\"  - Verify the output_path variable is set correctly\")\n",
    "    print(f\"  - Check that manifest.json exists at: s3://{bucket_name}/{manifest_key}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd137251-c11a-4c72-9550-5a6f4c45e63c",
   "metadata": {},
   "source": [
    "## 3. Model Deployment\n",
    "\n",
    "Now that the model training has been completed, we'll deploy the model to Bedrock for inferenceing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1f91fe6c-9d02-4809-8a09-98a85b21f320",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T22:37:01.432570Z",
     "iopub.status.busy": "2025-11-24T22:37:01.432292Z",
     "iopub.status.idle": "2025-11-24T22:37:01.442840Z",
     "shell.execute_reply": "2025-11-24T22:37:01.442295Z",
     "shell.execute_reply.started": "2025-11-24T22:37:01.432549Z"
    }
   },
   "outputs": [],
   "source": [
    "#Bedrock Client initilizations and configs\n",
    "from botocore.config import Config\n",
    "from botocore.exceptions import ClientError\n",
    "my_config = Config(connect_timeout=60*3, read_timeout=60*3)\n",
    "bedrock = boto3.client('bedrock', region_name='us-east-1')\n",
    "bedrock_runtime = boto3.client(service_name='bedrock-runtime', config=my_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf0d5bc-b208-444f-b60f-d72d0df557f3",
   "metadata": {},
   "source": [
    "Create IAM Role to allow model deployment to Bedrock "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d6a3bce5-02e9-42b8-a775-e0eb8027ecd3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T22:37:03.256474Z",
     "iopub.status.busy": "2025-11-24T22:37:03.256201Z",
     "iopub.status.idle": "2025-11-24T22:37:03.454417Z",
     "shell.execute_reply": "2025-11-24T22:37:03.453896Z",
     "shell.execute_reply.started": "2025-11-24T22:37:03.256454Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role BedrockNovaImportRole already exists, retrieving existing role...\n",
      "Using existing role: arn:aws:iam::133856113780:role/BedrockNovaImportRole\n",
      "Policy BedrockNovaS3Access already exists, retrieving existing policy...\n",
      "Using existing policy: arn:aws:iam::133856113780:policy/BedrockNovaS3Access\n",
      "Successfully attached policy to role\n",
      "Final Role ARN: arn:aws:iam::133856113780:role/BedrockNovaImportRole\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import boto3\n",
    "import json\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Create the role to deploy model to bedrock \n",
    "iam = boto3.client('iam')\n",
    "sts = boto3.client('sts')\n",
    "account_id = sts.get_caller_identity()['Account']\n",
    "bucket = bucket_name\n",
    "\n",
    "role_name = 'BedrockNovaImportRole'\n",
    "policy_name = 'BedrockNovaS3Access'\n",
    "\n",
    "# Create role with error handling\n",
    "try:\n",
    "    role = iam.create_role(\n",
    "        RoleName=role_name,\n",
    "        AssumeRolePolicyDocument=json.dumps({\n",
    "            \"Version\": \"2012-10-17\",\n",
    "            \"Statement\": [{\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": {\"Service\": \"bedrock.amazonaws.com\"},\n",
    "                \"Action\": \"sts:AssumeRole\",\n",
    "                \"Condition\": {\n",
    "                    \"StringEquals\": {\"aws:SourceAccount\": account_id},\n",
    "                    \"ArnEquals\": {\"aws:SourceArn\": f\"arn:aws:bedrock:{region}:{account_id}:model-import-job/*\"}\n",
    "                }\n",
    "            }]\n",
    "        })\n",
    "    )\n",
    "    bedrock_role = role['Role']['Arn']\n",
    "    print(f\"Created new role: {bedrock_role}\")\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'EntityAlreadyExists':\n",
    "        print(f\"Role {role_name} already exists, retrieving existing role...\")\n",
    "        role = iam.get_role(RoleName=role_name)\n",
    "        bedrock_role = role['Role']['Arn']\n",
    "        print(f\"Using existing role: {bedrock_role}\")\n",
    "    else:\n",
    "        print(f\"Error creating role: {e}\")\n",
    "        raise\n",
    "\n",
    "# Create and attach S3 policy with error handling\n",
    "try:\n",
    "    policy = iam.create_policy(\n",
    "        PolicyName=policy_name,\n",
    "        PolicyDocument=json.dumps({\n",
    "            \"Version\": \"2012-10-17\",\n",
    "            \"Statement\": [{\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\"s3:GetObject\", \"s3:ListBucket\"],\n",
    "                \"Resource\": [f\"arn:aws:s3:::{bucket}\", f\"arn:aws:s3:::{bucket}/*\"]\n",
    "            }]\n",
    "        })\n",
    "    )\n",
    "    policy_arn = policy['Policy']['Arn']\n",
    "    print(f\"Created new policy: {policy_arn}\")\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'EntityAlreadyExists':\n",
    "        print(f\"Policy {policy_name} already exists, retrieving existing policy...\")\n",
    "        policy_arn = f\"arn:aws:iam::{account_id}:policy/{policy_name}\"\n",
    "        print(f\"Using existing policy: {policy_arn}\")\n",
    "    else:\n",
    "        print(f\"Error creating policy: {e}\")\n",
    "        raise\n",
    "\n",
    "# Attach policy to role with error handling\n",
    "try:\n",
    "    iam.attach_role_policy(\n",
    "        RoleName=role_name,\n",
    "        PolicyArn=policy_arn\n",
    "    )\n",
    "    print(f\"Successfully attached policy to role\")\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'LimitExceeded':\n",
    "        print(f\"Policy already attached to role\")\n",
    "    else:\n",
    "        print(f\"Error attaching policy: {e}\")\n",
    "        raise\n",
    "\n",
    "print(f\"Final Role ARN: {bedrock_role}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d5cc18-a68c-40b1-94bd-af6d1585d91f",
   "metadata": {},
   "source": [
    "## Step 3a: Create custom model in Bedrock\n",
    "\n",
    "Now we create a new custom model in Amazon Bedrock from our SageMaker Amazon Nova model stored in the Amazon-managed S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2b96d541-c3a2-466d-b713-4d2cae0bce94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T22:39:22.252988Z",
     "iopub.status.busy": "2025-11-24T22:39:22.252724Z",
     "iopub.status.idle": "2025-11-24T22:50:23.859218Z",
     "shell.execute_reply": "2025-11-24T22:50:23.858720Z",
     "shell.execute_reply.started": "2025-11-24T22:39:22.252968Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying Model\n",
      "s3://customer-escrow-133856113780-smtj-23f4ff73/train-nova-micro-sql-peft-sft-2025-11-24-21-10-30-120/384\n",
      "Model import initiated: arn:aws:bedrock:us-east-1:133856113780:custom-model/imported/rtild21brc0d\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Active\n",
      "âœ“ Model is ready for on-demand inference!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import uuid\n",
    "\n",
    "from utils import wait_for_model_active\n",
    "def create_custom_model(bedrock_client, model_name, s3_uri, role_arn):\n",
    "    \"\"\"\n",
    "    Deploy a PEFT/LoRA fine-tuned Nova model for on-demand inference.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client_request_token = str(uuid.uuid4())\n",
    "        \n",
    "        model_source_config = {\n",
    "            's3DataSource': {\n",
    "                's3Uri': s3_uri,  \n",
    "            }\n",
    "        }\n",
    "        \n",
    "        response = bedrock_client.create_custom_model(\n",
    "            modelName=model_name,\n",
    "            roleArn=role_arn,\n",
    "            modelSourceConfig=model_source_config,\n",
    "            clientRequestToken=client_request_token\n",
    "        )\n",
    "        \n",
    "        print(f\"Model import initiated: {response['modelArn']}\")\n",
    "        return response['modelArn']\n",
    "        \n",
    "    except ClientError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        raise\n",
    "\n",
    "print(\"Deploying Model\")\n",
    "print(checkpoint_uri)\n",
    "model_arn = create_custom_model(\n",
    "    bedrock,\n",
    "    model_name='nova-micro-peft-lora',\n",
    "    s3_uri=checkpoint_uri,\n",
    "    role_arn=bedrock_role\n",
    ")\n",
    "\n",
    "wait_for_model_active(bedrock, model_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365cd520-941e-4a66-abb2-8c5aebeeb6fe",
   "metadata": {},
   "source": [
    "## Step 3b: Deploy custom model for Amazon Bedrock on-demand inferencing\n",
    "Now that we have created our new custom model in Amazon Bedrock, we can begin deploying the model for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b115100-0df7-40dc-b3c7-f6d3a60faa0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T23:05:42.400259Z",
     "iopub.status.busy": "2025-11-24T23:05:42.400006Z",
     "iopub.status.idle": "2025-11-24T23:05:44.075950Z",
     "shell.execute_reply": "2025-11-24T23:05:44.075093Z",
     "shell.execute_reply.started": "2025-11-24T23:05:42.400238Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_arn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 40\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m check_deployment_status\n\u001b[0;32m---> 40\u001b[0m deployed_model_arn \u001b[38;5;241m=\u001b[39m create_model_deployment(\u001b[43mmodel_arn\u001b[49m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deployed_model_arn:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_arn' is not defined"
     ]
    }
   ],
   "source": [
    "def create_model_deployment(custom_model_arn):\n",
    "    \"\"\"\n",
    "    Create an on-demand inferencing deployment for the custom model\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    custom_model_arn : str\n",
    "        ARN of the custom model to deploy\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    deployment_arn : str\n",
    "        ARN of the created deployment\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Creating on-demand inferencing deployment for model: {custom_model_arn}\")\n",
    "        \n",
    "        # Generate a unique name for the deployment\n",
    "        deployment_name = f\"nova-sql-deployment-{time.strftime('%Y%m%d-%H%M%S')}\"\n",
    "        \n",
    "        # Create the deployment\n",
    "        response = bedrock.create_custom_model_deployment(\n",
    "            modelArn=custom_model_arn,\n",
    "            modelDeploymentName=deployment_name,\n",
    "            description=f\"on-demand inferencing deployment for model: {custom_model_arn}\",\n",
    "        )\n",
    "        \n",
    "        # Get the deployment ARN\n",
    "        deployment_arn = response.get('customModelDeploymentArn')\n",
    "        \n",
    "        print(f\"Deployment request submitted. Deployment ARN: {deployment_arn}\")\n",
    "        return deployment_arn\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating deployment: {e}\")\n",
    "        return None\n",
    "\n",
    "from utils import check_deployment_status\n",
    "\n",
    "deployed_model_arn = create_model_deployment(model_arn)\n",
    "if deployed_model_arn:\n",
    "    while True:\n",
    "        status = check_deployment_status(bedrock, deployed_model_arn)\n",
    "        print(f\"Model is in {status} phase\")\n",
    "        \n",
    "        if status == 'Active':\n",
    "            break\n",
    "        elif status == 'Failed':\n",
    "            raise Exception(f\"Deployment failed: {deployed_model_arn}\")\n",
    "        \n",
    "        time.sleep(15) #sleep for 15 seconds \n",
    "\n",
    "print(f\"Use the deployment Arn for inferencing: {deployed_model_arn}\")\n",
    "%store deployed_model_arn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluation",
   "metadata": {},
   "source": [
    "# 4. Model Evaluation and Testing\n",
    "\n",
    "Once training completes, we'll evaluate the model's text-to-SQL generation capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f93b7f1-6a24-43d9-b96d-06eafa8f1ef9",
   "metadata": {},
   "source": [
    "### Evaluation using an LLM as a judge\n",
    "\n",
    "Since we have access to the \"right\" answer, we can evaluate similarity between the SQL queries returned by the fine-tuned Llama model and the right answer. Evaluation can be a bit tricky, since there is no single metric that evaluates semantic and syntactic similarity between two SQL queries. One alternative is to use a more powerful LLM, like Claude 3 Sonnet, to measure the similarity between the two SQL queries (LLM as a judge)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "inference-function",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T23:05:53.772874Z",
     "iopub.status.busy": "2025-11-24T23:05:53.772616Z",
     "iopub.status.idle": "2025-11-24T23:05:53.790322Z",
     "shell.execute_reply": "2025-11-24T23:05:53.789590Z",
     "shell.execute_reply.started": "2025-11-24T23:05:53.772853Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Prepare 100 evaluation samples, prompt our fine-tuned model for the sql generation task then ask our judge model to give a score\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ( ask_nova_micro,ask_claude,\n\u001b[1;32m      3\u001b[0m     prepare_evaluation_samples,\n\u001b[1;32m      4\u001b[0m     test_sql_generation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     plot_ttft_comparison\n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 11\u001b[0m eval_samples \u001b[38;5;241m=\u001b[39m prepare_evaluation_samples(\u001b[43mtest_data\u001b[49m, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Show a sample\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSample evaluation record:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Prepare 100 evaluation samples, prompt our fine-tuned model for the sql generation task then ask our judge model to give a score\n",
    "from utils import ( ask_nova_micro,ask_claude,\n",
    "    prepare_evaluation_samples,\n",
    "    test_sql_generation,\n",
    "    get_score,\n",
    "    metrics_test,\n",
    "    run_cold_and_warm_benchmark,\n",
    "    plot_ttft_comparison\n",
    ")\n",
    "\n",
    "eval_samples = prepare_evaluation_samples(test_data, num_samples=100)\n",
    "# Show a sample\n",
    "print('\\nSample evaluation record:')\n",
    "print(json.dumps(eval_samples[0], indent=2))\n",
    "\n",
    "results = test_sql_generation(eval_samples, deployed_model_arn)\n",
    "\n",
    "scores = []\n",
    "print(\"Grading responses with LLM Judge model\")\n",
    "for result in results:\n",
    "    if result['status'] == 'success':\n",
    "        response = float(get_score(\n",
    "            result['system_prompt'],\n",
    "            result['query'],\n",
    "            result['expected_sql'],\n",
    "            result['generated_sql']\n",
    "        ))\n",
    "        scores.append(response)\n",
    "       \n",
    "\n",
    "print(\"Assigned scores: \", scores)\n",
    "print(\"The average score of the fine tuned model is: \", sum(scores)/float(len(scores)), \"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec4e07f-1b34-4c76-81ce-47f116ea8cfb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T17:35:15.139413Z",
     "iopub.status.busy": "2025-10-01T17:35:15.139155Z",
     "iopub.status.idle": "2025-10-01T17:35:15.144219Z",
     "shell.execute_reply": "2025-10-01T17:35:15.143610Z",
     "shell.execute_reply.started": "2025-10-01T17:35:15.139392Z"
    }
   },
   "source": [
    "## Operational Metrics for Nova Micro SFT\n",
    "Now lets test the latency of our Fine tuned Nova Micro LLM by measuring:\n",
    "\n",
    "* Time To First Token (TTFS) - Cold start time to first token for loading Lora adapters and invoking the model should be is 1 second\n",
    "* Overall Throughput per Second (OTPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44941a1-441f-469a-a896-4487087c1d66",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-13T21:00:50.730331Z",
     "iopub.status.idle": "2025-11-13T21:00:50.730774Z"
    }
   },
   "outputs": [],
   "source": [
    "metrics_results= metrics_test(\n",
    "    model_id= model_arn,\n",
    "    system = \"You are a powerful text-to-SQL model. Your job is to answer questions about a database. You can use the following table schema for context: CREATE TABLE table_name_6 (winner_and_score VARCHAR, week VARCHAR)\", \n",
    "    prompt=\"Return the SQL query that answers the following question: who is the winner and score for the week of august 9?\"\n",
    ")\n",
    "print(f\"TTFT: {metrics_results['ttft_ms']:.2f}ms\")\n",
    "print(f\"OTPS: {metrics_results['otps']:.2f} tokens/s\")\n",
    "print(f\"Total end-to-end latency: {metrics_results['total_time_ms']:.2f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f958d9da-5275-4813-847b-b2123618b23b",
   "metadata": {},
   "source": [
    "Now lets increase our test cases to get an average result for cold start time as well as warm start time to first token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5fe23d-71a6-4e73-9713-8e4fb542eefd",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-13T21:00:50.732629Z",
     "iopub.status.idle": "2025-11-13T21:00:50.732966Z"
    }
   },
   "outputs": [],
   "source": [
    "# For quick testing (3 cold starts, 10 warm calls, 2 min wait)\n",
    "\n",
    "results = run_cold_and_warm_benchmark(\n",
    "    model_id=deployed_model_arn,\n",
    "    system = \"You are a powerful text-to-SQL model. Your job is to answer questions about a database. You can use the following table schema for context: CREATE TABLE table_name_6 (winner_and_score VARCHAR, week VARCHAR)\", \n",
    "    prompt=\"Return the SQL query that answers the following question: who is the winner and score for the week of august 9?\",\n",
    "    num_cold_starts=5,\n",
    "    num_warm_calls=10,\n",
    "    cold_start_wait=600  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a925c86-5769-4951-b4b4-f3e53d496b25",
   "metadata": {},
   "source": [
    "## Plot the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3635c3d-badf-438b-996e-c3885e975233",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-13T21:00:50.734691Z",
     "iopub.status.idle": "2025-11-13T21:00:50.735074Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_ttft_comparison(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1819a2-c24f-4009-96ba-680eff19ad63",
   "metadata": {},
   "source": [
    "## Use case price comparison analysis \n",
    "\n",
    "Below we run an analysis of running a similar workload on a self-hosted ec2 instance as well as a Sagemaker real-time endpoint \n",
    "For this analysis we make the following assumptions \n",
    "\n",
    "* Users = 100\n",
    "* Queries per day = 10\n",
    "* Usage days 30 - 8(weekend) = 22\n",
    "* Total queries per month = users * queries per day * 22 = 22,000\n",
    "* Compute hours = 12\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65a10f7-2390-41b8-b9f6-64e08412f186",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-13T21:00:50.735811Z",
     "iopub.status.idle": "2025-11-13T21:00:50.736158Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Scenario\n",
    "users = 100\n",
    "queries_per_day = 10\n",
    "total_queries_per_month = users * queries_per_day * 22\n",
    "\n",
    "# Average tokens per query\n",
    "avg_input_tokens = 80\n",
    "avg_output_tokens = 60\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a890e75b-652d-4bdf-a386-b9c6ec0d8e04",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-13T21:00:50.736869Z",
     "iopub.status.idle": "2025-11-13T21:00:50.737214Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Bedrock On-Demand \n",
    "# ============================================================================\n",
    "\n",
    "input_cost = (total_queries_per_month * avg_input_tokens / 1000) * 0.000035\n",
    "output_cost = (total_queries_per_month * avg_output_tokens / 1000) * 0.00014\n",
    "bedrock_on_demand = input_cost + output_cost\n",
    "\n",
    "print(f\"\\n Bedrock On-Demand: ${bedrock_on_demand:.2f}/month\")\n",
    "print(f\"   Cost per query: ${bedrock_on_demand/total_queries_per_month:.6f}\")\n",
    "\n",
    "# Self-Hosted on EC2  g5.12xlarge\n",
    "# ============================================================================\n",
    "\n",
    "ec2_hourly = 5.672  \n",
    "ec2_compute = ec2_hourly * 12 * 22\n",
    "\n",
    "print(f\"\\n Self-Hosted (EC2 g5.12xlarge): ${ec2_compute:.2f}/month\")\n",
    "print(f\"   Cost per query: ${ec2_compute/total_queries_per_month:.4f}\")\n",
    "\n",
    "# SageMaker Endpoint\n",
    "# ============================================================================\n",
    "\n",
    "sagemaker_hourly = ec2_hourly+1.418  # (EC2 + SageMaker overhead)\n",
    "sagemaker_compute = sagemaker_hourly * 12 * 22\n",
    "sagemaker_total = sagemaker_compute + ec2_hourly\n",
    "\n",
    "print(f\"\\n SageMaker Endpoint: ${sagemaker_total:.2f}/month\")\n",
    "print(f\"   Compute: ${sagemaker_compute:.2f}\")\n",
    "print(f\"   Cost per query: ${sagemaker_total/total_queries_per_month:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# VISUALIZATION\n",
    "# ============================================================================\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Total Monthly Cost\n",
    "options = ['Bedrock\\nOn-Demand', 'Self-Hosted\\nEC2', 'SageMaker\\nEndpoint']\n",
    "costs = [bedrock_on_demand, ec2_compute, sagemaker_total]\n",
    "colors = ['#2ecc71', '#e74c3c', '#f39c12']\n",
    "\n",
    "bars = ax1.bar(options, costs, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "\n",
    "for bar, cost in zip(bars, costs):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'${cost:,.0f}',\n",
    "            ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "ax1.set_ylabel('Monthly Cost ($)', fontsize=13, fontweight='bold')\n",
    "ax1.set_title(f'Monthly Cost Comparison (Verified Pricing)\\n{users} users, {queries_per_day} queries/user/day',\n",
    "             fontsize=14, fontweight='bold')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Highlight winner\n",
    "winner_idx = np.argmin(costs)\n",
    "ax1.get_children()[winner_idx].set_edgecolor('green')\n",
    "ax1.get_children()[winner_idx].set_linewidth(4)\n",
    "\n",
    "# Plot 2: Cost per Query\n",
    "cost_per_query = [c / total_queries_per_month for c in costs]\n",
    "\n",
    "bars2 = ax2.barh(options, cost_per_query, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "\n",
    "for bar, cpq in zip(bars2, cost_per_query):\n",
    "    width = bar.get_width()\n",
    "    ax2.text(width, bar.get_y() + bar.get_height()/2.,\n",
    "            f' ${cpq:.5f}',\n",
    "            ha='left', va='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "ax2.set_xlabel('Cost Per Query ($)', fontsize=13, fontweight='bold')\n",
    "ax2.set_title('Cost Efficiency', fontsize=14, fontweight='bold')\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('cost_comparison_verified.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce643711-020e-4d4a-8a1d-7a1a45c4eb26",
   "metadata": {},
   "source": [
    "## Break Even Scale\n",
    "\n",
    "Now we can see that finetuning our model for on demand usage is significantly cheaper, but at what scale do the other options st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de822077-8e29-4f0f-9934-b9e513644ffd",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-13T21:00:50.737923Z",
     "iopub.status.idle": "2025-11-13T21:00:50.738271Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"BREAK-EVEN ANALYSIS\")\n",
    "\n",
    "cost_per_query_bedrock = bedrock_on_demand / total_queries_per_month\n",
    "fixed_self_hosted_costs = ec2_compute\n",
    "\n",
    "break_even_queries = fixed_self_hosted_costs / cost_per_query_bedrock\n",
    "break_even_users = break_even_queries / (queries_per_day * 30)\n",
    "\n",
    "print(f\"\\nSelf-hosted breaks even at:\")\n",
    "print(f\"  {break_even_queries:,.0f} queries/month\")\n",
    "print(f\"   = {break_even_users:,.0f} users @ {queries_per_day} queries/day\")\n",
    "print(f\"   = {break_even_users/users:.0f}x your current scale\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bff153-a491-497a-ac9b-691b8023e977",
   "metadata": {},
   "source": [
    "### Get the throughput for the custom model \n",
    "* To get the throughput we will get the average of tokens generated per second\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003b25b3-1774-4fcd-bf0b-2fa525c27878",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-13T21:00:50.739069Z",
     "iopub.status.idle": "2025-11-13T21:00:50.739415Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils import test_model_throughput, visualize_throughput_results\n",
    "\n",
    "# Run the test\n",
    "throughput_results = test_model_throughput(deployed_model_arn)\n",
    "\n",
    "#Visualize the results \n",
    "visualize_throughput_results(throughput_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7dac1e-034c-43da-903e-62957ca7e6f2",
   "metadata": {},
   "source": [
    "### Compare TTFT for Base Nova Micro Model to our SFT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ad9576-2087-4e1d-8dbc-6130a4f00b19",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-13T21:00:50.741063Z",
     "iopub.status.idle": "2025-11-13T21:00:50.741404Z"
    }
   },
   "outputs": [],
   "source": [
    "#Becasue the base model does not have knowledge of the SQL data we will ask a generic question to both models\n",
    "\n",
    "from utils import compare_models\n",
    "\n",
    "custom_model_arn = deployed_model_arn\n",
    "base_model_id = \"us.amazon.nova-micro-v1:0\"\n",
    "\n",
    "system_prompt = \"You are a helpful assistant.\"\n",
    "test_prompt = \"What are the performance specs of a bmw x5m and how does it compare with the porsche macan turbo\"\n",
    "\n",
    "results = compare_models(\n",
    "    custom_model_arn=custom_model_arn,\n",
    "    base_model_id=base_model_id,\n",
    "    system=system_prompt,\n",
    "    prompt=test_prompt,\n",
    "    num_runs=10  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d912e48-d315-42fe-a47f-7c3d82c663b9",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-13T21:00:50.741925Z",
     "iopub.status.idle": "2025-11-13T21:00:50.742269Z"
    }
   },
   "outputs": [],
   "source": [
    "# what is the percentage in latency that the SFT model has in generating TTFT\n",
    "ttft_percentage_difference = ((381.49-356.28)/356.28)*100\n",
    "print(\"Our Custom model has a Time to first token differene of: \", ttft_percentage_difference,\"%\")\n",
    "otps_percentage_difference = ((184.56-253.57)/253.57)*100\n",
    "print(\"Our Custom model has a Output per second differene of: \", otps_percentage_difference,\"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Cleanup Resources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cleanup-cell",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T19:45:00.604440Z",
     "iopub.status.busy": "2025-11-13T19:45:00.604154Z",
     "iopub.status.idle": "2025-11-13T19:45:01.541645Z",
     "shell.execute_reply": "2025-11-13T19:45:01.541120Z",
     "shell.execute_reply.started": "2025-11-13T19:45:00.604419Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Deployment deleted\n",
      "âœ“ Model deleted\n",
      "âœ“ IAM resources deleted\n",
      "âœ“ Local data deleted\n",
      "\n",
      "Cleanup complete!\n"
     ]
    }
   ],
   "source": [
    "# # Cleanup - Delete all resources\n",
    "# import boto3, shutil, os\n",
    "\n",
    "# bedrock = boto3.client('bedrock', region_name=region)\n",
    "# iam = boto3.client('iam')\n",
    "\n",
    "# # Delete deployment\n",
    "# try:\n",
    "#     bedrock.delete_custom_model_deployment(customModelDeploymentIdentifier=deployment_arn)\n",
    "#     print(\"Deployment deleted\")\n",
    "# except: pass\n",
    "\n",
    "# # Delete model\n",
    "# try:\n",
    "#     bedrock.delete_custom_model(modelIdentifier=model_arn)\n",
    "#     print(\"Model deleted\")\n",
    "# except: pass\n",
    "\n",
    "# # Delete IAM role\n",
    "# try:\n",
    "#     for p in iam.list_attached_role_policies(RoleName='BedrockNovaImportRole')['AttachedPolicies']:\n",
    "#         iam.detach_role_policy(RoleName='BedrockNovaImportRole', PolicyArn=p['PolicyArn'])\n",
    "#         iam.delete_policy(PolicyArn=p['PolicyArn'])\n",
    "#     iam.delete_role(RoleName='BedrockNovaImportRole')\n",
    "#     print(\"IAM resources deleted\")\n",
    "# except: pass\n",
    "\n",
    "# # Delete local data\n",
    "# if os.path.exists('data'): shutil.rmtree('data'); print(\"âœ“ Local data deleted\")\n",
    "\n",
    "# print(\"\\nCleanup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49482818-3223-444a-b50f-9d13d5250335",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
