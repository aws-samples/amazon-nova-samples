{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nova Evaluation Container Demo\n",
    "## Leveraging Nova Evaluation for Base and Custom Model Evaluations using SageMaker AI\n",
    "\n",
    "This notebook demonstrates the key features introduced in the Nova Evaluation Container:\n",
    "- **Metadata passthrough** for stratified analysis\n",
    "- **Log probabilities** for uncertainty-aware evaluation\n",
    "- **Multi-node evaluation** for scalability\n",
    "- **Custom metrics** with BYOM workflow\n",
    "- **Failure analysis** on low confidence predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "This notebook can run locally or in Amazon Sagemaker\n",
    "\n",
    "- **AWS Account** for stratified analysis\n",
    "- **IAM Permissions** Create Roles, Create Lambda Functions, Assume SageMaker Execution Role\n",
    "- **Sagemaker Training Instance** ml.g5.12xlarge (1 or more)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the SageMaker SDK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade sagemaker\n",
    "import sagemaker\n",
    "print(sagemaker.__version__) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required libraries for data manipulation (pandas, numpy), visualization (matplotlib, seaborn), AWS services (boto3), and SageMaker evaluation workflows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import boto3\n",
    "# SageMaker imports\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "print(\"Environment setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Dataset Preparation with Metadata Passthrough\n",
    "\n",
    "The **metadata passthrough** feature allows us to preserve custom fields end-to-end, enabling stratified analysis across different categories and difficulty levels without post-hoc joins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sample_dataset(file_path: str = 'modified_text_eval_dataset.jsonl') -> List[Dict]:\n",
    "    \"\"\"Load the sample dataset with metadata\"\"\"\n",
    "    dataset = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            dataset.append(json.loads(line))\n",
    "    return dataset\n",
    "\n",
    "def analyze_dataset_metadata(dataset: List[Dict]) -> pd.DataFrame:\n",
    "    \"\"\"Analyze metadata distribution in the dataset\"\"\"\n",
    "    metadata_list = []\n",
    "    for item in dataset:\n",
    "        metadata = json.loads(item.get('metadata', '{}'))\n",
    "        metadata_list.append(metadata)\n",
    "    \n",
    "    return pd.DataFrame(metadata_list)\n",
    "\n",
    "# Load and analyze the sample dataset\n",
    "dataset = load_sample_dataset()\n",
    "metadata_df = analyze_dataset_metadata(dataset)\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)} examples\")\n",
    "print(f\"\\nMetadata fields: {list(metadata_df.columns)}\")\n",
    "print(f\"\\nMetadata distribution:\")\n",
    "for col in metadata_df.columns:\n",
    "    print(f\"{col}: {metadata_df[col].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Coding Up the Custom Metrics with BYOM Workflow\n",
    "\n",
    "The **Bring Your Own Metrics (BYOM)** feature provides complete control over evaluation metrics. Here we implement custom metrics for our pet breed classification task. First lets download the github customization layer from https://github.com/aws/nova-custom-eval-sdk/releases and then run below command to publish a layer version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lambda function code for custom metrics (BYOM)\n",
    "lambda_code = '''\n",
    "import json\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from nova_custom_evaluation_sdk.processors.decorators import preprocess, postprocess\n",
    "from nova_custom_evaluation_sdk.lambda_handler import build_lambda_handler\n",
    "\n",
    "@preprocess\n",
    "def preprocessor(event: dict, context) -> dict:\n",
    "    data = event.get('data', {})\n",
    "    return {\n",
    "        \"statusCode\": 200,\n",
    "        \"body\": {\n",
    "            \"system\": data.get(\"system\"),\n",
    "            \"prompt\": data.get(\"prompt\", \"\"),\n",
    "            \"gold\": data.get(\"gold\", \"\")\n",
    "        }\n",
    "    }\n",
    "\n",
    "@postprocess\n",
    "def postprocessor(event: dict, context) -> dict:\n",
    "    data = event.get('data', {})\n",
    "    inference_output = data.get('inference_output', '')\n",
    "    gold = data.get('gold', '')\n",
    "    \n",
    "    metrics = []\n",
    "    \n",
    "    # 1. Schema Validation\n",
    "    schema_valid = validate_schema(inference_output)\n",
    "    metrics.append({\"metric\": \"schema_validation\", \"value\": 1.0 if schema_valid else 0.0})\n",
    "    \n",
    "    # 2. Classification Metrics\n",
    "    class_metrics = calculate_class_metrics(inference_output, gold)\n",
    "    metrics.extend(class_metrics)\n",
    "    \n",
    "    return {\"statusCode\": 200, \"body\": metrics}\n",
    "\n",
    "def validate_schema(output: str) -> bool:\n",
    "    try:\n",
    "        parsed = json.loads(output)\n",
    "        return \"class\" in parsed\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def calculate_class_metrics(inference_output: str, gold: str) -> list:\n",
    "    pred_class = extract_class(inference_output)\n",
    "    true_class = extract_class(gold)\n",
    "    \n",
    "    if not pred_class or not true_class:\n",
    "        return [{\"metric\": \"class_accuracy\", \"value\": 0.0}]\n",
    "    \n",
    "    accuracy = 1.0 if pred_class.lower() == true_class.lower() else 0.0\n",
    "    return [{\"metric\": \"class_accuracy\", \"value\": accuracy}]\n",
    "\n",
    "def extract_class(text: str) -> str:\n",
    "    try:\n",
    "        parsed = json.loads(text)\n",
    "        return str(parsed.get(\"class\", \"\")).strip()\n",
    "    except:\n",
    "        return text.strip()\n",
    "\n",
    "lambda_handler = build_lambda_handler(\n",
    "    preprocessor=preprocessor,\n",
    "    postprocessor=postprocessor\n",
    ")\n",
    "'''\n",
    "\n",
    "print(\"Custom Lambda function code for BYOM workflow:\")\n",
    "print(\"- Schema validation metric\")\n",
    "print(\"- Classification accuracy metric\")\n",
    "print(\"- Preprocessing and postprocessing hooks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets use this code and create a lambda function with the added layer as shown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Setting Up the Custom Metrics Lambda Function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.a: Publish Lambda Layer\n",
    "\n",
    "Publish the Nova custom evaluation layer to AWS Lambda using boto3 and the provided zip file. The layer contains SDK dependencies required for custom metric computation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "\n",
    "layer_name = \"nova-custom-eval-layer\"\n",
    "zip_file_path = 'nova-custom-evaluation-layer.zip'\n",
    "\n",
    "# Get region with fallback\n",
    "region = os.environ.get('AWS_DEFAULT_REGION') or boto3.Session().region_name or 'us-east-1'\n",
    "lambda_client = boto3.client('lambda', region_name=region)\n",
    "\n",
    "if os.path.exists(zip_file_path):\n",
    "    try:\n",
    "        with open(zip_file_path, 'rb') as f:\n",
    "            response = lambda_client.publish_layer_version(\n",
    "                LayerName=layer_name,\n",
    "                Content={'ZipFile': f.read()},\n",
    "                CompatibleRuntimes=['python3.9', 'python3.10', 'python3.11', 'python3.12']\n",
    "            )\n",
    "        layer_arn = response['LayerVersionArn']\n",
    "        print(f\"Published layer: {layer_arn}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        raise\n",
    "else:\n",
    "    # Check for existing layer\n",
    "    try:\n",
    "        response = lambda_client.list_layer_versions(LayerName=layer_name, MaxItems=1)\n",
    "        layer_arn = response['LayerVersions'][0]['LayerVersionArn']\n",
    "        print(f\"Using existing layer: {layer_arn}\")\n",
    "    except:\n",
    "        raise Exception(f\"Layer zip not found. Download from https://github.com/aws/nova-custom-eval-sdk/releases\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.b: Create an IAM role that has permisions to create Lambda Function\n",
    "Create an IAM execution role with basic Lambda permissions to enable function execution and CloudWatch logging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import os\n",
    "import boto3\n",
    "\n",
    "# Get region with fallback\n",
    "region = os.environ.get('AWS_DEFAULT_REGION') or boto3.Session().region_name or 'us-east-1'\n",
    "\n",
    "iam_client = boto3.client(\"iam\", region_name=region)\n",
    "lambda_client = boto3.client(\"lambda\", region_name=region)\n",
    "sts_client = boto3.client(\"sts\", region_name=region)\n",
    "\n",
    "role_name = \"nova-custom-eval-lambda-role\"\n",
    "\n",
    "try:\n",
    "    account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "    lambda_client.list_functions(MaxItems=1)\n",
    "    \n",
    "    try:\n",
    "        role_response = iam_client.get_role(RoleName=role_name)\n",
    "        role_arn = role_response[\"Role\"][\"Arn\"]\n",
    "        print(f\"Using existing role: {role_arn}\")\n",
    "    except iam_client.exceptions.NoSuchEntityException:\n",
    "        trust_policy = {\n",
    "            \"Version\": \"2012-10-17\",\n",
    "            \"Statement\": [{\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": {\"Service\": \"lambda.amazonaws.com\"},\n",
    "                \"Action\": \"sts:AssumeRole\"\n",
    "            }]\n",
    "        }\n",
    "        \n",
    "        role_response = iam_client.create_role(\n",
    "            RoleName=role_name,\n",
    "            AssumeRolePolicyDocument=json.dumps(trust_policy),\n",
    "            Description=\"Execution role for Nova custom evaluation Lambda\"\n",
    "        )\n",
    "        \n",
    "        iam_client.attach_role_policy(\n",
    "            RoleName=role_name,\n",
    "            PolicyArn=\"arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole\"\n",
    "        )\n",
    "        \n",
    "        role_arn = role_response[\"Role\"][\"Arn\"]\n",
    "        print(f\"Created role: {role_arn}\")\n",
    "        time.sleep(10)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3c: Create Lambda Function with Layer\n",
    "\n",
    "Deploy the Lambda function with custom metrics code and attach the evaluation layer for runtime dependencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import tempfile\n",
    "import zipfile\n",
    "import os\n",
    "import time\n",
    "\n",
    "function_name = \"nova-custom-eval\"\n",
    "region = os.environ.get('AWS_DEFAULT_REGION') or boto3.Session().region_name or 'us-east-1'\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "    lambda_file = os.path.join(tmp_dir, \"lambda_function.py\")\n",
    "    with open(lambda_file, \"w\") as f:\n",
    "        f.write(lambda_code)\n",
    "    \n",
    "    zip_path = os.path.join(tmp_dir, \"function.zip\")\n",
    "    with zipfile.ZipFile(zip_path, \"w\") as z:\n",
    "        z.write(lambda_file, arcname=\"lambda_function.py\")\n",
    "    \n",
    "    with open(zip_path, \"rb\") as f:\n",
    "        zip_content = f.read()\n",
    "    \n",
    "    lambda_client = boto3.client(\"lambda\", region_name=region)\n",
    "    \n",
    "    try:\n",
    "        lambda_client.get_function(FunctionName=function_name)\n",
    "        print(f\"Updating function {function_name}...\")\n",
    "        response = lambda_client.update_function_code(\n",
    "            FunctionName=function_name,\n",
    "            ZipFile=zip_content\n",
    "        )\n",
    "    except lambda_client.exceptions.ResourceNotFoundException:\n",
    "        print(f\"Creating function {function_name}...\")\n",
    "        response = lambda_client.create_function(\n",
    "            FunctionName=function_name,\n",
    "            Runtime=\"python3.9\",\n",
    "            Role=role_arn,\n",
    "            Handler=\"lambda_function.lambda_handler\",\n",
    "            Code={\"ZipFile\": zip_content},\n",
    "            Timeout=30,\n",
    "            MemorySize=256\n",
    "        )\n",
    "    \n",
    "    # Wait for function to be ready\n",
    "    while True:\n",
    "        config = lambda_client.get_function_configuration(FunctionName=function_name)\n",
    "        if config[\"LastUpdateStatus\"] == \"Successful\":\n",
    "            break\n",
    "        time.sleep(2)\n",
    "    \n",
    "    # Now update layer\n",
    "    response = lambda_client.update_function_configuration(\n",
    "        FunctionName=function_name,\n",
    "        Layers=[layer_arn]\n",
    "    )\n",
    "    \n",
    "    function_arn = response[\"FunctionArn\"]\n",
    "    print(f\"Lambda Function ARN: {function_arn}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3d: Update Recipe Configuration with Lambda ARN\n",
    "\n",
    "\n",
    "Configure the evaluation recipe with model parameters, inference settings (including top_logprobs for confidence analysis), and the Lambda function ARN for custom metrics previously published."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "recipe_config = {\n",
    "    \"run\": {\n",
    "        \"name\": \"support-ticket-classification\",\n",
    "        \"model_type\": \"amazon.nova-lite-v1:0:300k\",\n",
    "        \"model_name_or_path\": \"nova-lite/prod\",\n",
    "        \"replicas\": 1\n",
    "    },\n",
    "    \"evaluation\": {\n",
    "        \"task\": \"gen_qa\",\n",
    "        \"strategy\": \"gen_qa\",\n",
    "        \"metric\": \"all\"\n",
    "    },\n",
    "    \"inference\": {\n",
    "        \"max_new_tokens\": 2048,\n",
    "        \"temperature\": 0,\n",
    "        \"top_logprobs\": 10\n",
    "    },\n",
    "    \"processor\": {\n",
    "        \"lambda_arn\": function_arn,\n",
    "        \"preprocessing\": {\"enabled\": False},\n",
    "        \"postprocessing\": {\"enabled\": True},\n",
    "        \"aggregation\": \"average\"\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(\"lambda_recipe.yaml\", \"w\") as f:\n",
    "    yaml.dump(recipe_config, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "print(f\"Recipe saved with Lambda ARN: {function_arn}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Launch SageMaker Evaluation Job\n",
    "\n",
    "Configure and launch the Nova evaluation job with custom metrics and log probabilities enabled.\n",
    "Configure the SageMaker training job with compute resources, S3 paths, and the evaluation recipe, then submit for execution. \n",
    "\n",
    "*Change the following fields to match your values (input_s3_uri, output_s3_uri, role)*\n",
    "\n",
    "*Don't forget to upload your dataset to S3*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['AWS_DEFAULT_REGION'] = 'us-east-1'\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# Use your actual SageMaker execution role ARN\n",
    "role = \"enter-sagemaker-execution-role-ARN\"\n",
    "\n",
    "input_s3_uri = \"s3://enter-bucket-name/input/text_eval_dataset.jsonl\"\n",
    "output_s3_uri = \"s3://enter-budket-name/output/\"\n",
    "\n",
    "estimator = PyTorch(\n",
    "    output_path=output_s3_uri,\n",
    "    base_job_name=\"support-ticket-classification\",\n",
    "    role=role,\n",
    "    instance_type=\"ml.g5.12xlarge\",\n",
    "    training_recipe=\"lambda_recipe.yaml\",\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    image_uri=\"708977205387.dkr.ecr.us-east-1.amazonaws.com/nova-evaluation-repo:SM-TJ-Eval-latest\"\n",
    ")\n",
    "\n",
    "print(f\"Using role: {role}\")\n",
    "estimator.fit(inputs={\"train\": TrainingInput(s3_data=input_s3_uri)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ^^ This will generally take somewhere between 20-40 mins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Review Results after Evaluation Job is compeleted\n",
    "\n",
    "Load the evaluation results from the S3 output path, including detailed metrics, predictions, and log probabilities for analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_parquet_file(file_path):\n",
    "    \"\"\"\n",
    "    Load a parquet file into a pandas DataFrame\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the parquet file\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: The loaded DataFrame\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the parquet file\n",
    "        df = pd.read_parquet(file_path)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading parquet file: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download, extract, and load evaluation results from S3, including detailed metrics, predictions, and log probabilities for analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import tarfile\n",
    "import glob\n",
    "\n",
    "def download_and_extract_output(output_s3_uri, job_name, output_dir='./evaluation_results'):\n",
    "    \"\"\"\n",
    "    Download output.tar.gz from S3 and extract it to a local directory.\n",
    "    Returns the path to the parquet results file.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    if output_s3_uri.startswith('s3://'):\n",
    "        output_s3_uri = output_s3_uri[5:]\n",
    "    \n",
    "    if output_s3_uri.endswith('/'):\n",
    "        output_s3_uri = output_s3_uri[:-1]\n",
    "    \n",
    "    parts = output_s3_uri.split('/')\n",
    "    bucket = parts[0]\n",
    "    prefix = '/'.join(parts[1:])\n",
    "    \n",
    "    s3_key = f\"{prefix}/{job_name}/output/output.tar.gz\"\n",
    "    local_tar_path = os.path.join(output_dir, \"output.tar.gz\")\n",
    "    \n",
    "    print(f\"Downloading from s3://{bucket}/{s3_key} to {local_tar_path}\")\n",
    "    \n",
    "    s3_client = boto3.client('s3')\n",
    "    \n",
    "    try:\n",
    "        s3_client.download_file(bucket, s3_key, local_tar_path)\n",
    "        print(\"Download completed successfully\")\n",
    "        \n",
    "        print(f\"Extracting to {output_dir}\")\n",
    "        with tarfile.open(local_tar_path, \"r:gz\") as tar:\n",
    "            tar.extractall(path=output_dir)\n",
    "        print(\"Extraction completed successfully\")\n",
    "        \n",
    "        os.remove(local_tar_path)\n",
    "        print(f\"Removed {local_tar_path}\")\n",
    "        \n",
    "        # Find the parquet file\n",
    "        parquet_pattern = os.path.join(output_dir, \"**\", \"*.parquet\")\n",
    "        parquet_files = glob.glob(parquet_pattern, recursive=True)\n",
    "        \n",
    "        if parquet_files:\n",
    "            results_path = parquet_files[0]\n",
    "            print(f\"\\nResults parquet file: {results_path}\")\n",
    "            return results_path\n",
    "        else:\n",
    "            print(\"Warning: No parquet file found\")\n",
    "            return None\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Download and get results path\n",
    "results_data_path = download_and_extract_output(output_s3_uri, estimator._current_job_name)\n",
    "\n",
    "# Load the results\n",
    "if results_data_path:\n",
    "    results_data = pd.read_parquet(results_data_path)\n",
    "    print(f\"Loaded {len(results_data)} rows from results\")\n",
    "else:\n",
    "    print(\"Failed to load results\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eval results visualization\n",
    "\n",
    "After your evaluation job completes successfully, you can access and analyze the results using the information in this section. Based on the output_s3_path (such as s3://output_path/) defined in the recipe, the output structure is the following:\n",
    "\n",
    "\n",
    "```\n",
    "job_name/\n",
    "├── eval-results/\n",
    "│    └── results_[timestamp].json\n",
    "│    └── inference_output.jsonl (only present for gen_qa)\n",
    "│    └── details/\n",
    "│        └── model/\n",
    "│            └── execution-date-time/\n",
    "│                └──details_task_name_#_datetime.parquet\n",
    "└── tensorboard-results/\n",
    "    └── eval/\n",
    "        └── events.out.tfevents.[timestamp]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Failure Analysis with Log Probabilities\n",
    "\n",
    "After evaluation completes, analyze low-confidence predictions using log probabilities and metadata for stratified failure analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "def calculate_confidence_score(pred_logits_str) -> float:\n",
    "    \"\"\"Calculate confidence score from log probabilities\"\"\"\n",
    "    if not pred_logits_str or pred_logits_str == '[]':\n",
    "        return 0.0\n",
    "    \n",
    "    try:\n",
    "        logits = ast.literal_eval(pred_logits_str)\n",
    "        if not logits or len(logits) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        token_probs = []\n",
    "        for token_dict in logits[0]:\n",
    "            if isinstance(token_dict, dict) and token_dict:\n",
    "                max_logprob = max(token_dict.values())\n",
    "                token_probs.append(np.exp(max_logprob))\n",
    "        \n",
    "        return float(np.mean(token_probs)) if token_probs else 0.0\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def analyze_low_confidence_failures(results_df: pd.DataFrame, confidence_threshold: float = 0.7, quality_threshold: float = 0.3) -> dict:\n",
    "    \"\"\"Perform comprehensive failure analysis\"\"\"\n",
    "    \n",
    "    results_df['confidence'] = results_df['pred_logits'].apply(calculate_confidence_score)\n",
    "    results_df['quality_score'] = results_df['metrics'].apply(\n",
    "        lambda x: ast.literal_eval(x).get('f1', 0) if x else 0\n",
    "    )\n",
    "    results_df['is_correct'] = results_df['quality_score'] >= quality_threshold\n",
    "    \n",
    "    # Analyze low confidence predictions\n",
    "    low_conf_df = results_df[results_df['confidence'] < confidence_threshold]\n",
    "    \n",
    "    # Also identify high confidence but low quality (overconfident errors)\n",
    "    overconfident_df = results_df[(results_df['confidence'] >= confidence_threshold) & \n",
    "                                   (results_df['quality_score'] < quality_threshold)]\n",
    "    \n",
    "    analysis_results = {\n",
    "        'summary': {\n",
    "            'total_predictions': len(results_df),\n",
    "            'low_confidence_count': len(low_conf_df),\n",
    "            'low_confidence_rate': len(low_conf_df) / len(results_df) if len(results_df) > 0 else 0,\n",
    "            'overconfident_errors': len(overconfident_df),\n",
    "            'overconfident_rate': len(overconfident_df) / len(results_df) if len(results_df) > 0 else 0,\n",
    "            'avg_confidence': results_df['confidence'].mean(),\n",
    "            'avg_quality': results_df['quality_score'].mean(),\n",
    "            'overall_accuracy': results_df['is_correct'].mean()\n",
    "        },\n",
    "        'low_confidence_examples': [],\n",
    "        'overconfident_examples': []\n",
    "    }\n",
    "    \n",
    "    # Low confidence examples\n",
    "    if len(low_conf_df) > 0:\n",
    "        for idx, row in low_conf_df.iterrows():\n",
    "            try:\n",
    "                specifics = ast.literal_eval(row['specifics'])\n",
    "                metadata = json.loads(specifics.get('metadata', '{}'))\n",
    "                \n",
    "                analysis_results['low_confidence_examples'].append({\n",
    "                    'example': row['example'][:100] + '...' if len(row['example']) > 100 else row['example'],\n",
    "                    'confidence': row['confidence'],\n",
    "                    'quality_score': row['quality_score'],\n",
    "                    'category': metadata.get('category', 'unknown'),\n",
    "                    'difficulty': metadata.get('difficulty', 'unknown'),\n",
    "                    'domain': metadata.get('domain', 'unknown')\n",
    "                })\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # Overconfident errors\n",
    "    if len(overconfident_df) > 0:\n",
    "        for idx, row in overconfident_df.iterrows():\n",
    "            try:\n",
    "                specifics = ast.literal_eval(row['specifics'])\n",
    "                metadata = json.loads(specifics.get('metadata', '{}'))\n",
    "                \n",
    "                analysis_results['overconfident_examples'].append({\n",
    "                    'example': row['example'][:100] + '...' if len(row['example']) > 100 else row['example'],\n",
    "                    'confidence': row['confidence'],\n",
    "                    'quality_score': row['quality_score'],\n",
    "                    'category': metadata.get('category', 'unknown'),\n",
    "                    'difficulty': metadata.get('difficulty', 'unknown'),\n",
    "                    'domain': metadata.get('domain', 'unknown')\n",
    "                })\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    return analysis_results\n",
    "\n",
    "# Use already loaded data\n",
    "results_df = pd.DataFrame(results_data)\n",
    "analysis_results = analyze_low_confidence_failures(results_df, confidence_threshold=0.7, quality_threshold=0.3)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FAILURE ANALYSIS RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTotal predictions: {analysis_results['summary']['total_predictions']}\")\n",
    "print(f\"Average confidence: {analysis_results['summary']['avg_confidence']:.3f}\")\n",
    "print(f\"Average F1 quality: {analysis_results['summary']['avg_quality']:.3f}\")\n",
    "print(f\"Overall accuracy (F1>0.3): {analysis_results['summary']['overall_accuracy']:.1%}\")\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(\"LOW CONFIDENCE PREDICTIONS (confidence < 0.7)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Count: {analysis_results['summary']['low_confidence_count']} ({analysis_results['summary']['low_confidence_rate']:.1%})\")\n",
    "for i, example in enumerate(analysis_results['low_confidence_examples'][:5], 1):\n",
    "    print(f\"\\n{i}. Confidence: {example['confidence']:.3f} | F1: {example['quality_score']:.3f}\")\n",
    "    print(f\"   {example['difficulty']} | {example['domain']}\")\n",
    "    print(f\"   {example['example']}\")\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(\"OVERCONFIDENT ERRORS (confidence >= 0.7 but F1 < 0.3)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Count: {analysis_results['summary']['overconfident_errors']} ({analysis_results['summary']['overconfident_rate']:.1%})\")\n",
    "for i, example in enumerate(analysis_results['overconfident_examples'][:5], 1):\n",
    "    print(f\"\\n{i}. Confidence: {example['confidence']:.3f} | F1: {example['quality_score']:.3f}\")\n",
    "    print(f\"   {example['difficulty']} | {example['domain']}\")\n",
    "    print(f\"   {example['example']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions for Result Parsing\n",
    "\n",
    "The following utility functions parse evaluation outputs, extract class predictions from JSON responses, and handle log probability data for confidence analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Parse log probability data and extract confidence scores for predicted classes. These functions handle token-level probability extraction from the model's output, supporting both single-token and multi-token class predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "def extract_gold(text):\n",
    "    \"\"\"\n",
    "    More robust extraction handling various formats and bytes\n",
    "    \"\"\"\n",
    "    is_bytes = isinstance(text, bytes)\n",
    "    \n",
    "    if is_bytes:\n",
    "        patterns = [\n",
    "            rb'\"class\"\\s*:\\s*\"([^\"]+)\"',\n",
    "            rb'\"class\"\\s*:\\s*([^,}\\s]+)',\n",
    "            rb\"'class'\\s*:\\s*'([^']+)'\",\n",
    "            rb\"'class'\\s*:\\s*([^,}\\s]+)\",\n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, text)\n",
    "            if match:\n",
    "                value = match.group(1).decode('utf-8', errors='ignore').strip()\n",
    "                value = re.sub(r'[.,;]+$', '', value)\n",
    "                return value\n",
    "    else:\n",
    "        patterns = [\n",
    "            r'\"class\"\\s*:\\s*\"([^\"]+)\"',\n",
    "            r'\"class\"\\s*:\\s*([^,}\\s]+)',\n",
    "            r\"'class'\\s*:\\s*'([^']+)'\",\n",
    "            r\"'class'\\s*:\\s*([^,}\\s]+)\",\n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, text)\n",
    "            if match:\n",
    "                value = match.group(1).strip()\n",
    "                value = re.sub(r'[.,;]+$', '', value)\n",
    "                return value\n",
    "    \n",
    "    return None\n",
    "\n",
    "def parse_preds(text):\n",
    "    \"\"\"\n",
    "    Parse a malformed JSON string into a dictionary\n",
    "    \"\"\"\n",
    "    if isinstance(text, bytes):\n",
    "        text = text.decode('utf-8', errors='ignore')\n",
    "    \n",
    "    text = text.strip()\n",
    "    if text.startswith('[') and text.endswith(']'):\n",
    "        text = text[1:-1].strip()\n",
    "    \n",
    "    if (text.startswith(\"'\") and text.endswith(\"'\")) or \\\n",
    "       (text.startswith('\"') and text.endswith('\"')):\n",
    "        text = text[1:-1]\n",
    "    \n",
    "    text = text.replace('\\\\n', '\\n')\n",
    "    text = text.replace('\\\\\"', '\"')\n",
    "    \n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except json.JSONDecodeError as e:\n",
    "        result = {}\n",
    "        \n",
    "        class_match = re.search(r'\"class\"\\s*:\\s*\"([^\"]+)\"', text)\n",
    "        if class_match:\n",
    "            result['class'] = class_match.group(1)\n",
    "        \n",
    "        thought_match = re.search(r'\"thought\"\\s*:\\s*\"([^\"]+)\"', text)\n",
    "        if thought_match:\n",
    "            result['thought'] = thought_match.group(1)\n",
    "        \n",
    "        desc_match = re.search(r'\"description\"\\s*:\\s*\"([^\"]+)\"', text)\n",
    "        if desc_match:\n",
    "            result['description'] = desc_match.group(1)\n",
    "        \n",
    "        return result if result else None\n",
    "\n",
    "# Success confirmation\n",
    "print(\"✓ Helper functions defined successfully:\")\n",
    "print(\"  - extract_gold(): Extracts class from gold standard\")\n",
    "print(\"  - parse_preds(): Parses prediction text to dict\")\n",
    "print(\"\\nFunctions are ready to use for data processing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Execute the complete analysis pipeline on evaluation results. This function iterates through all predictions, extracts classes and metadata, computes confidence scores, and generates summary statistics including accuracy breakdowns by metadata categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import ast\n",
    "import json\n",
    "\n",
    "def parse_log_probs(data_str):\n",
    "    \"\"\"Parse the log probability data string into a list of token dictionaries\"\"\"\n",
    "    pattern = r\"\\{([^}]+)\\}\"\n",
    "    matches = re.findall(pattern, data_str)\n",
    "    \n",
    "    parsed_sequence = []\n",
    "    for match in matches:\n",
    "        pairs = re.findall(r\"'([^']+)':\\s*([-\\d.e]+)\", match)\n",
    "        token_dict = {token: float(log_prob) for token, log_prob in pairs}\n",
    "        parsed_sequence.append(token_dict)\n",
    "    \n",
    "    return parsed_sequence\n",
    "\n",
    "def find_target_tokens(target_class):\n",
    "    \"\"\"Split target class into component tokens\"\"\"\n",
    "    tokens = []\n",
    "    \n",
    "    if '_' in target_class:\n",
    "        parts = target_class.split('_')\n",
    "        for i, part in enumerate(parts):\n",
    "            if i > 0:\n",
    "                tokens.append('_')\n",
    "            subtokens = re.findall(r'[A-Z]?[a-z]+|[A-Z]+(?=[A-Z][a-z]|\\b)', part)\n",
    "            tokens.extend(subtokens)\n",
    "    else:\n",
    "        tokens = re.findall(r'[A-Z]?[a-z]+|[A-Z]+(?=[A-Z][a-z]|\\b)', target_class)\n",
    "    \n",
    "    return [target_class] + tokens\n",
    "\n",
    "def extract_class_confidence(data_str, target_class):\n",
    "    \"\"\"Extract confidence score for any target class prediction\"\"\"\n",
    "    parsed_sequence = parse_log_probs(data_str)\n",
    "    possible_tokens = find_target_tokens(target_class)\n",
    "    \n",
    "    print(f\"Searching for target class: '{target_class}'\")\n",
    "    print(f\"Possible token sequences: {possible_tokens}\\n\")\n",
    "    \n",
    "    found_tokens = []\n",
    "    \n",
    "    for step_idx, token_dict in enumerate(parsed_sequence):\n",
    "        for token in possible_tokens:\n",
    "            if token in token_dict:\n",
    "                log_prob = token_dict[token]\n",
    "                prob = math.exp(log_prob)\n",
    "                found_tokens.append({\n",
    "                    'step': step_idx,\n",
    "                    'token': token,\n",
    "                    'log_prob': log_prob,\n",
    "                    'probability': prob,\n",
    "                    'confidence_pct': prob * 100\n",
    "                })\n",
    "    \n",
    "    if not found_tokens:\n",
    "        print(f\"❌ Target class '{target_class}' not found in sequence\")\n",
    "        return None\n",
    "    \n",
    "    if len(found_tokens) == 1:\n",
    "        result = found_tokens[0]\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"✓ FOUND: '{target_class}' as single token\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"Step: {result['step']}\")\n",
    "        print(f\"Log Probability: {result['log_prob']:.10f}\")\n",
    "        print(f\"Confidence: {result['confidence_pct']:.6f}%\")\n",
    "        print(\"=\" * 70)\n",
    "        return result\n",
    "    else:\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"✓ FOUND: '{target_class}' as token sequence\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        overall_prob = 1.0\n",
    "        for token_info in found_tokens:\n",
    "            print(f\"  Step {token_info['step']:2d} | '{token_info['token']:12s}' | \"\n",
    "                  f\"Confidence: {token_info['confidence_pct']:8.4f}%\")\n",
    "            overall_prob *= token_info['probability']\n",
    "        \n",
    "        overall_log_prob = math.log(overall_prob)\n",
    "        print(\"-\" * 70)\n",
    "        print(f\"Overall Models Sequence Confidence: {overall_prob*100:.6f}%\")\n",
    "        print(f\"Overall Log Probability: {overall_log_prob:.10f}\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        return {\n",
    "            'tokens': found_tokens,\n",
    "            'overall_probability': overall_prob,\n",
    "            'overall_log_prob': overall_log_prob,\n",
    "            'confidence_pct': overall_prob * 100\n",
    "        }\n",
    "\n",
    "def parse_to_dict(text):\n",
    "    \"\"\"Parse a malformed JSON string into a dictionary\"\"\"\n",
    "    if isinstance(text, bytes):\n",
    "        text = text.decode('utf-8', errors='ignore')\n",
    "    \n",
    "    original_text = text\n",
    "    text = text.strip()\n",
    "    \n",
    "    if text.startswith('[') and text.endswith(']'):\n",
    "        text = text[1:-1].strip()\n",
    "    \n",
    "    if (text.startswith(\"'\") and text.endswith(\"'\")) or \\\n",
    "       (text.startswith('\"') and text.endswith('\"')):\n",
    "        text = text[1:-1]\n",
    "    \n",
    "    try:\n",
    "        result = ast.literal_eval(original_text)\n",
    "        if isinstance(result, dict):\n",
    "            for key, value in result.items():\n",
    "                if isinstance(value, str) and (value.strip().startswith('{') or value.strip().startswith('[')):\n",
    "                    try:\n",
    "                        result[key] = json.loads(value)\n",
    "                    except:\n",
    "                        pass\n",
    "            return result\n",
    "    except (ValueError, SyntaxError):\n",
    "        pass\n",
    "    \n",
    "    text = text.replace('\\\\n', '\\n')\n",
    "    text = text.replace('\\\\\"', '\"')\n",
    "    text = text.replace(\"\\\\'\", \"'\")\n",
    "    \n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "    \n",
    "    result = {}\n",
    "    patterns = [\n",
    "        r'\"(\\w+)\"\\s*:\\s*\"([^\"]*)\"',\n",
    "        r\"'(\\w+)'\\s*:\\s*'([^']*)'\",\n",
    "        r'\"(\\w+)\"\\s*:\\s*({[^}]+})',\n",
    "        r\"'(\\w+)'\\s*:\\s*({[^}]+})\",\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        matches = re.findall(pattern, text, re.DOTALL)\n",
    "        for key, value in matches:\n",
    "            if value.strip().startswith('{'):\n",
    "                try:\n",
    "                    result[key] = json.loads(value.replace(\"'\", '\"'))\n",
    "                except:\n",
    "                    result[key] = value\n",
    "            else:\n",
    "                result[key] = value\n",
    "    \n",
    "    return result if result else None\n",
    "\n",
    "# Success confirmation\n",
    "print(\"✓ Log probability and parsing functions defined successfully:\")\n",
    "print(\"  - parse_log_probs(): Parses log probability strings\")\n",
    "print(\"  - find_target_tokens(): Tokenizes target class names\")\n",
    "print(\"  - extract_class_confidence(): Extracts confidence for predictions\")\n",
    "print(\"  - parse_to_dict(): Parses malformed JSON/dict strings\")\n",
    "print(\"\\nAll functions are ready for confidence analysis and data processing.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Run comprehensive classification analysis across the entire evaluation dataset. This function processes predictions, gold labels, metadata, and confidence scores to generate per-sample results with stratified performance metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import re\n",
    "import json\n",
    "\n",
    "def extract_class_value(text):\n",
    "    \"\"\"Extract class value from various text formats\"\"\"\n",
    "    # Handle string representation of list: \"['...']\"\n",
    "    if isinstance(text, str) and text.startswith('['):\n",
    "        try:\n",
    "            parsed_list = ast.literal_eval(text)\n",
    "            if isinstance(parsed_list, list) and len(parsed_list) > 0:\n",
    "                text = parsed_list[0]\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Handle numpy array\n",
    "    if isinstance(text, np.ndarray) and len(text) > 0:\n",
    "        text = text[0]\n",
    "    \n",
    "    # Convert to string\n",
    "    text = str(text)\n",
    "    \n",
    "    # Try JSON format: {\"class\": \"Document_Request\"}\n",
    "    try:\n",
    "        parsed = json.loads(text)\n",
    "        if isinstance(parsed, dict) and 'class' in parsed:\n",
    "            return parsed['class']\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Try markdown format: **Issue Type:** Document Request\n",
    "    patterns = [\n",
    "        r'\\*\\*Issue Type:\\*\\*\\s*([^\\n]+)',\n",
    "        r'Issue Type:\\s*([^\\n]+)',\n",
    "        r'\"class\"\\s*:\\s*\"([^\"]+)\"',\n",
    "        r\"'class'\\s*:\\s*'([^']+)'\",\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            value = match.group(1).strip()\n",
    "            # Clean up common suffixes\n",
    "            value = re.sub(r'\\s*/\\s*.*$', '', value)  # Remove \"/ Department\" part\n",
    "            return value\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Test it\n",
    "print(\"Testing extraction:\")\n",
    "pred_sample = results_data['predictions'][0]\n",
    "gold_sample = results_data['gold'][0]\n",
    "\n",
    "print(f\"Prediction extracted: {extract_class_value(pred_sample)}\")\n",
    "print(f\"Gold extracted: {extract_class_value(gold_sample)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Preview the analysis results showing predictions, gold labels, correctness, confidence scores, and metadata for the first few samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Multi-Node Scaling (optional)\n",
    "\n",
    "Optionally you can scale evaluation to larger datasets using **multi-node evaluation** by simply increasing the replica count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-node configuration - just change the replicas parameter\n",
    "multinode_config = {\n",
    "    \"run\": {\n",
    "        \"name\": \"support-ticket-classification-multinode\",\n",
    "        \"model_name_or_path\": \"nova-lite-v1:0\",\n",
    "        \"replicas\": 4  # Scale to 4 nodes automatically you must have quota allocation for this\n",
    "    },\n",
    "    \"evaluation\": {\n",
    "        \"task\": \"gen_qa\",\n",
    "        \"strategy\": \"gen_qa\",\n",
    "        \"metric\": \"all\"\n",
    "    },\n",
    "    \"inference\": {\n",
    "        \"max_new_tokens\": 2048,\n",
    "        \"temperature\": 0,\n",
    "        \"top_logprobs\": 10\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Multi-node scaling configuration:\")\n",
    "print(f\"Replicas: {multinode_config['run']['replicas']}\")\n",
    "print(\"\\nBenefits:\")\n",
    "print(\"- Automatic workload distribution\")\n",
    "print(\"- Preserved metadata-based analysis\")\n",
    "print(\"- Deterministic result aggregation\")\n",
    "print(\"- Scale from thousands to millions of examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the first few rows of the analysis results to verify the data structure and preview classification outcomes with confidence scores and metadata fields.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Visualization and Insights\n",
    "\n",
    "Visualize the evaluation results and failure analysis to identify improvement opportunities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Check if confidence already exists, if not calculate it\n",
    "if 'confidence' not in results_df.columns:\n",
    "    def calculate_confidence_score(pred_logits_str):\n",
    "        if not pred_logits_str or pred_logits_str == '[]':\n",
    "            return 0.0\n",
    "        try:\n",
    "            logits = ast.literal_eval(pred_logits_str)\n",
    "            if not logits or len(logits) == 0:\n",
    "                return 0.0\n",
    "            token_probs = [np.exp(max(td.values())) for td in logits[0] if isinstance(td, dict) and td]\n",
    "            return float(np.mean(token_probs)) if token_probs else 0.0\n",
    "        except:\n",
    "            return 0.0\n",
    "    \n",
    "    results_df['confidence'] = results_df['pred_logits'].apply(calculate_confidence_score)\n",
    "\n",
    "# Extract class values\n",
    "def extract_class_value(text):\n",
    "    if isinstance(text, str) and text.startswith('['):\n",
    "        try:\n",
    "            text = ast.literal_eval(text)[0]\n",
    "        except:\n",
    "            pass\n",
    "    if isinstance(text, np.ndarray) and len(text) > 0:\n",
    "        text = text[0]\n",
    "    text = str(text)\n",
    "    try:\n",
    "        parsed = json.loads(text)\n",
    "        if isinstance(parsed, dict) and 'class' in parsed:\n",
    "            return parsed['class']\n",
    "    except:\n",
    "        pass\n",
    "    match = re.search(r'\\*\\*Issue Type:\\*\\*\\s*([^\\n]+)', text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return re.sub(r'\\s*/\\s*.*$', '', match.group(1).strip())\n",
    "    return None\n",
    "\n",
    "# Extract metadata\n",
    "def extract_metadata(row):\n",
    "    try:\n",
    "        specifics = ast.literal_eval(row['specifics'])\n",
    "        metadata = json.loads(specifics.get('metadata', '{}'))\n",
    "        return pd.Series({\n",
    "            'meta_category': metadata.get('category', 'unknown'),\n",
    "            'meta_difficulty': metadata.get('difficulty', 'unknown'),\n",
    "            'meta_domain': metadata.get('domain', 'unknown')\n",
    "        })\n",
    "    except:\n",
    "        return pd.Series({'meta_category': 'unknown', 'meta_difficulty': 'unknown', 'meta_domain': 'unknown'})\n",
    "\n",
    "# Add missing columns\n",
    "results_df['pred_class'] = results_df['predictions'].apply(extract_class_value)\n",
    "results_df['gold_class'] = results_df['gold'].apply(extract_class_value)\n",
    "results_df['correct'] = results_df['pred_class'] == results_df['gold_class']\n",
    "results_df[['meta_category', 'meta_difficulty', 'meta_domain']] = results_df.apply(extract_metadata, axis=1)\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "axes[0, 0].hist(results_df['confidence'], bins=20, alpha=0.7, edgecolor='black')\n",
    "axes[0, 0].axvline(0.7, color='red', linestyle='--', label='Low confidence threshold')\n",
    "axes[0, 0].set_title('Confidence Score Distribution')\n",
    "axes[0, 0].set_xlabel('Confidence Score')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "confidence_bins = pd.cut(results_df['confidence'], bins=5)\n",
    "accuracy_by_conf = results_df.groupby(confidence_bins, observed=True)['correct'].mean()\n",
    "accuracy_by_conf.plot(kind='bar', ax=axes[0, 1], rot=45)\n",
    "axes[0, 1].set_title('Accuracy by Confidence Bin')\n",
    "axes[0, 1].set_ylabel('Accuracy')\n",
    "\n",
    "domain_perf = results_df.groupby('meta_domain')['correct'].mean()\n",
    "domain_perf.plot(kind='bar', ax=axes[1, 0], color='skyblue')\n",
    "axes[1, 0].set_title('Accuracy by Domain')\n",
    "axes[1, 0].set_ylabel('Accuracy')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "difficulty_perf = results_df.groupby('meta_difficulty')['correct'].mean()\n",
    "difficulty_perf.plot(kind='bar', ax=axes[1, 1], color='lightcoral')\n",
    "axes[1, 1].set_title('Accuracy by Difficulty')\n",
    "axes[1, 1].set_ylabel('Accuracy')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated key capabilities of the Nova Evaluation Container:\n",
    "\n",
    "### **Metadata Passthrough**\n",
    "- Preserves custom fields end-to-end for stratified analysis\n",
    "- Enables performance analysis by category, difficulty, domain\n",
    "- No post-hoc joins required\n",
    "\n",
    "### **Log Probabilities** \n",
    "- Token-level uncertainty information for calibration studies\n",
    "- Confidence-based routing and quality gates\n",
    "- Hallucination detection capabilities\n",
    "\n",
    "### **Custom Metrics (BYOM)**\n",
    "- Complete control over evaluation pipeline\n",
    "- Schema validation and domain-specific metrics\n",
    "- Pre/post-processing hooks\n",
    "\n",
    "### **Multi-Node Scaling**\n",
    "- Simple replica configuration for distributed evaluation\n",
    "- Maintains deterministic results and metadata analysis\n",
    "- Scales from thousands to millions of examples\n",
    "\n",
    "\n",
    "### Next Steps\n",
    "1. Upload your own dataset to S3 and configure the recipe YAML\n",
    "2. Deploy the custom Lambda function for BYOM workflow\n",
    "3. Launch the SageMaker evaluation job\n",
    "4. Analyze results using the failure analysis framework\n",
    "5. Scale to larger datasets using multi-node configuration\n",
    "\n",
    "For more information, see the [Nova Evaluation Container documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/nova-hp-evaluate.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
