{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Nova 2 Lite Custom Model Deployment\n",
    "\n",
    "After a custom model is created with a model customization job or import a SageMaker AI-trained custom Amazon Nova model, the next step is to up on-demand inference for the model. With on-demand inference, you only pay for what you use and you don't need to set up provisioned compute resources.\n",
    "\n",
    "To set up on-demand inference for a custom model, you deploy it with a custom model deployment. After you deploy your custom model, you use the deployment's Amazon Resource Name (ARN) as the modelId parameter when you submit prompts and generate responses with model inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Getting Started\n",
    "\n",
    "You can customize Amazon Nova models with Amazon Bedrock, or SageMaker AI, or SageMaker Hyperpod. Once this model is customized, it must then be hosted for inference.\n",
    "\n",
    "A cost effective solution is Bedrock OnDemand Inference (ODI).  ODI allows a custom model to be hosted and made accessbile quickly via Bedrock.\n",
    "\n",
    "On-demand (OD) inference allows you to run inference on your custom Amazon Nova models without maintaining provisioned throughput endpoints. This helps you optimize costs and scale efficiently. With On-demand inference, you are charged based on usage, measured in tokens, both in and out.\n",
    "\n",
    "An alternate choices is Provision Throughput on Bedrock, but this solution tends to be more expensive and should only be chosen after careful analysis\n",
    "\n",
    "\n",
    "This notebook will demonstrate deploying a custom model.\n",
    "\n",
    "[Customizing Amazon Nova Models](https://docs.aws.amazon.com/nova/latest/userguide/customization.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Nova Customization](./images/NovaCustomization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important** <br>To complete this notebook, the SFT notebook must be completed first. In that workbook, a model is trained and relevent artifacts defined.  The model artifact information are carried over for use in this notebook.  Specific items from the SFT notebook, used in this notebook, are called out below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prerequisites and Dependencies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies\n",
    "Several python packages will need to be installed in order to execute this notebook.  Please review the packages in requirements.txt. \n",
    "\n",
    "botocore, boto3, sagemaker are required for the training jobs, while the other packages are used to help visualize results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r ./requirements.txt --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisite: SFT Notebook\n",
    "The SFT notebook walks through training a custom model. As a result of that training, output model artifacts are created.  We will use those output artifacts to host the model in Bedrock. \n",
    "\n",
    "**Important** <br>To complete this notebook, the SFT notebook must be completed before this notebook. This notebook will use the artifacts created in the SFT notebook and are called out below.\n",
    "\n",
    "**--------------- STOP ---------------** <br><br>To complete this notebook, the SFT notebook must be completed first. This notebook will use the artifacts created in the SFT notebook and are called out below.\n",
    "<br><br>\n",
    "\n",
    "Either restore or set these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These values are obtained as result of executing the SFT notebook\n",
    "sm_training_job_name = \"\"\n",
    "checkpoint_s3_bucket = \"\"\n",
    "\n",
    "%store -r sm_training_job_name \n",
    "%store -r checkpoint_s3_bucket \n",
    "\n",
    "print(sm_training_job_name)\n",
    "print(checkpoint_s3_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, we will not use sm_training_job_name in the notebook directly, but a good practice to document this so as to refer back to this job for all details as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credentials, Sessions, Roles, and more!\n",
    "\n",
    "This section sets up the necessary AWS credentials and SageMaker session to run the notebook. You'll need proper IAM permissions to use SageMaker.\n",
    "\n",
    "\n",
    "If you are going to use Sagemaker in a local environment, you will need access to an IAM Role with the required permissions for Sagemaker. Learn more about it here [AWS Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html).\n",
    "\n",
    "For more details on other Nova pre-requisites needed check out [AWS Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/nova-model-general-prerequisites.html)\n",
    "\n",
    "The code initializes a SageMaker session, sets up the IAM role, and configures the S3 bucket for storing training data artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sagemaker_session = sagemaker.Session(boto_session=boto3.Session(region_name='us-east-1'))\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "    \n",
    "except ValueError:\n",
    "    iam = boto3.client(\"iam\")\n",
    "    role = iam.get_role(RoleName=\"sagemaker_execution_role\")[\"Role\"][\"Arn\"]\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker session region: {sagemaker_session.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deployment / Inference\n",
    "\n",
    "Now that the model has been trained, it is time to deploy that model.  The current options include deployment through Bedrock:\n",
    "- [On-Demand Inference (ODI)](https://docs.aws.amazon.com/nova/latest/userguide/custom-fine-tune-odi.html)\n",
    "- [Provisions Throughout (PT)](https://docs.aws.amazon.com/bedrock/latest/userguide/prov-throughput.html)\n",
    "\n",
    "\n",
    "Create the fine-tuned model in Bedrock and deploy to perform inference:\n",
    "- Create model in Bedrock\n",
    "- Deploy model in Bedrock\n",
    "- Evaluate performance\n",
    "\n",
    "After training and evaluating our model, we want to make it available for inference. Amazon Bedrock provides a serverless endpoint for model deployment, allowing us to serve the model without managing infrastructure.\n",
    "\n",
    "The Bedrock Custom Model feature of Amazon Bedrock lets us import our fine-tuned model and access it through the same API as other foundation models. This provides:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Deployment - On-Demand Inference on Bedrock\n",
    "\n",
    "Create the fine-tuned model in Bedrock and deploy to perform inference:\n",
    "- Create model in Bedrock\n",
    "- Deploy model in Bedrock\n",
    "- Evaluate performance\n",
    "\n",
    "After training and evaluating our model, we want to make it available for inference. Amazon Bedrock provides a serverless endpoint for model deployment, allowing us to serve the model without managing infrastructure.\n",
    "\n",
    "The Bedrock Custom Model feature of Amazon Bedrock lets us import our fine-tuned model and access it through the same API as other foundation models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bedrock offers 2 clients to interact. The \"bedrock\" client is for the control plane, used for managing models and customization jobs.  The \"bedrock-runtime\" client is for the data plane.  We will use the \"bedrock\" client for deploying our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Bedrock client\n",
    "bedrock = boto3.client(\"bedrock\", region_name=sagemaker_session.boto_region_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Define a unique name for imported model\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S-%f\")[:-3]\n",
    "imported_model_name = f\"imported-model-name-{timestamp}\"\n",
    "\n",
    "print(f\"imported_model_name: \\n{imported_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Create a Custom Model in Bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimated time to complete:  10 minutes\n",
    "\n",
    "request_params = {\n",
    "    \"modelName\": imported_model_name,\n",
    "    \"modelSourceConfig\": {\"s3DataSource\": {\"s3Uri\": checkpoint_s3_bucket}},\n",
    "    \"roleArn\": role,\n",
    "    \"clientRequestToken\": \"NovaRecipeSageMaker\",\n",
    "}\n",
    "\n",
    "# Create the model import job\n",
    "response = bedrock.create_custom_model(**request_params)\n",
    "\n",
    "model_arn = response[\"modelArn\"]\n",
    "\n",
    "# Output the model ARN\n",
    "print(f\"Model import job created with ARN: \\n{model_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2. Monitoring the Create Custom Model status\n",
    "\n",
    "After initiating the model import, we need to monitor its progress. The status goes through several states:\n",
    "\n",
    "* CREATING: Model is being imported\n",
    "* ACTIVE: Import successful\n",
    "* FAILED: Import encountered errors\n",
    "\n",
    "This cell polls the Bedrock API every 60 seconds to check the status of the model import, continuing until it reaches a terminal state (ACTIVE or FAILED). Once the import completes successfully, we'll have the model ARN which we can use for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimated time to complete:  10 minutes\n",
    "import time\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Check CMI job status\n",
    "while True:\n",
    "    response = bedrock.list_custom_models(sortBy='CreationTime',sortOrder='Descending')\n",
    "    model_summaries = response[\"modelSummaries\"]\n",
    "    status = \"\"\n",
    "    for model in model_summaries:\n",
    "        if model[\"modelName\"] == imported_model_name:\n",
    "            status = model[\"modelStatus\"].upper()\n",
    "            model_arn = model[\"modelArn\"]\n",
    "            print(f'{model[\"modelStatus\"].upper()} {model[\"modelArn\"]} ...')\n",
    "            if status in [\"ACTIVE\", \"FAILED\"]:\n",
    "                break\n",
    "    if status in [\"ACTIVE\", \"FAILED\"]:\n",
    "        break\n",
    "    clear_output(wait=True)\n",
    "    time.sleep(10)\n",
    "    \n",
    "print(f\"model_arn: \\n{model_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3 Deploy the Custom Model in Bedrock using OnDemand Inference (ODI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_custom_model_deployment(bedrock_client, custom_model_arn, imported_model_name, tags = None):\n",
    "    \"\"\"Create a custom model deployment\n",
    "    Args:\n",
    "        bedrock_client: A boto3 Amazon Bedrock client for making API calls\n",
    "\n",
    "    Returns:\n",
    "        str: The ARN of the new custom model deployment\n",
    "\n",
    "    Raises:\n",
    "        Exception: If there is an error creating the deployment\n",
    "    \"\"\"\n",
    "\n",
    "    if tags is None or len(tags) == 0:\n",
    "        tags=[\n",
    "                {'key': 'Environment', 'value': 'Production'},\n",
    "                {'key': 'Team', 'value': 'Amazon'},\n",
    "                {'key': 'Project', 'value': 'SFT-PEFT'}\n",
    "            ]\n",
    "    \n",
    "    try:\n",
    "        response = bedrock_client.create_custom_model_deployment(\n",
    "            modelDeploymentName=f\"{imported_model_name}-deployment\",\n",
    "            modelArn=custom_model_arn,\n",
    "            description=\"Deployment description\",\n",
    "            tags=tags\n",
    "        )\n",
    "\n",
    "        deployment_arn = response['customModelDeploymentArn']\n",
    "        print(f\"Deployment created: {deployment_arn}\")\n",
    "        return deployment_arn\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating deployment: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimated time to complete:  2 minutes\n",
    "\n",
    "deployment_arn = create_custom_model_deployment(bedrock, model_arn, imported_model_name)\n",
    "\n",
    "print(f\"deployment_arn: \\n{deployment_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.4 Monitor the Custom Model Deployment in Bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimated time to complete:  2 minutes\n",
    "import time\n",
    "\n",
    "# Check CMD job status\n",
    "while True:\n",
    "    response = bedrock.list_custom_model_deployments()\n",
    "    model_deployment_summaries = response[\"modelDeploymentSummaries\"]\n",
    "    status = \"\"\n",
    "    for deployment in model_deployment_summaries:\n",
    "        if deployment[\"modelArn\"] == model_arn:\n",
    "            status = deployment[\"status\"].upper()\n",
    "            customModelDeploymentName = deployment[\"customModelDeploymentName\"]\n",
    "            customModelDeploymentArn = deployment[\"customModelDeploymentArn\"]\n",
    "            print(f'{deployment[\"status\"].upper()} {deployment[\"modelArn\"]} ...')\n",
    "            if status in [\"ACTIVE\", \"FAILED\"]:\n",
    "                break\n",
    "    if status in [\"ACTIVE\", \"FAILED\"]:\n",
    "        break\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    time.sleep(10)\n",
    "\n",
    "print(f\"customModelDeploymentName: \\n{customModelDeploymentName}\\n\")\n",
    "print(f\"customModelDeploymentArn: \\n{customModelDeploymentArn}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.5 Testing the Deployed Model\n",
    "\n",
    "Now that our model is deployed to Amazon Bedrock, we can invoke it for inference. We'll set up the necessary clients and functions to interact with our model through the Bedrock Runtime API.\n",
    "\n",
    "Inference Setup Components:\n",
    "* Bedrock Runtime Client: AWS SDK client for making inference calls\n",
    "* Helper Function: To handle retry logic and properly format requests\n",
    "\n",
    "Applies the proper chat template to user messages\n",
    "* Handles retry logic for robustness\n",
    "* Sets appropriate generation parameters like temperature and top-p\n",
    "\n",
    "This setup allows us to easily test how well our training worked by sending queries to the model and evaluating its responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "# Initialize Bedrock Runtime client\n",
    "boto3_session = boto3.Session()\n",
    "\n",
    "client = boto3_session.client(\n",
    "    service_name=\"bedrock-runtime\",\n",
    "    region_name=sagemaker_session.boto_region_name,\n",
    "    config=Config(\n",
    "        connect_timeout=300,  # 5 minutes\n",
    "        read_timeout=300,  # 5 minutes\n",
    "        retries={\"max_attempts\": 3},\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), 'utils'))\n",
    "\n",
    "from utils import generate_2_0\n",
    "\n",
    "\n",
    "system_prompt = f\"\"\"\n",
    "You are a helpful AI assistant that can answer questions and provide information.\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": [{\"text\": \"What is the weather in Rome, Italy?\"}]},\n",
    "]\n",
    "\n",
    "response = generate_2_0(\n",
    "    client=client,\n",
    "    model_id=customModelDeploymentArn,\n",
    "    system_prompt=system_prompt,\n",
    "    messages=messages,\n",
    "    temperature=0.1,\n",
    "    top_p=0.9,\n",
    ")\n",
    "\n",
    "response[\"output\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.6 Bedrock Console Testing\n",
    "As the model is now deployed to Bedrock, it will be available in the Chat / Text Playground.\n",
    "\n",
    "Access the model by going to the Bedrock console, and selecting the Chat / Text Playground."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Bedrock Chat / Text Playground](./images/playground.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, click Select model.  In the Select model dialog, choose Custom models - and this will show the list of custom models available on Bedrock.  Select your custom model and then click Apply.\n",
    "\n",
    "![Select model](./images/select-model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you will be back in the Chat / Text playground and able to send prompts to the custom model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Deployment - Provisioned Throughput (PT) on Bedrock\n",
    "This notebook will not cover PT on Bedrock at this time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
