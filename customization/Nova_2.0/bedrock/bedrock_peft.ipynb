{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94836170",
   "metadata": {},
   "source": [
    "# PEFT Supervised Fine-Tuning (SFT) Amazon Nova 2 Lite using Amazon Bedrock\n",
    "\n",
    "Training a large language model typically has two major stages: pre-training and post-training. During pre-training, the model is exposed to trillions of tokens of raw text and optimized purely for next-token prediction. This makes it an extremely capable pattern completer over the distribution of web and curated text. It absorbs syntax, semantics, facts, and broad reasoning patterns. But it is unaligned with human intent, meaning it does not inherently understand instructions, user goals, or context-appropriate behavior. It simply continues text in whatever style best fits its training distribution. As a result, a pre-trained model tends to autocomplete rather than follow directions, is inconsistent about formatting or tool use, and can mirror undesirable biases or unsafe content present in the data. In short, pre-training builds general competence, not usefulness for tasks.\n",
    "\n",
    "Post-training turns that competent pattern completer into a useful assistant. Teams typically run multiple rounds of Supervised Fine-Tuning (SFT) to teach the model to follow instructions, adhere to schemas and policies, call tools, and produce reliable, scoped outputs by imitating high-quality demonstrations. This adds a first layer of alignment where the model learns to respond to prompts as tasks, not just text to continue. They then apply Reinforcement Fine-Tuning (RFT) to push behavior further using measurable feedback (e.g., verifiers or an LLM-as-a-judge), optimizing nuanced trade-offs like accuracy vs. brevity, safety vs. coverage, or multi-step reasoning under constraints. In practice, teams alternate SFT and RFT in cycles, progressively shaping the pre-trained model into a reliable, policy-aligned system that performs complex tasks with consistency.\n",
    "\n",
    "Supervised fine-tuning is the classic approach of training the LLM on a dataset of human-labeled input-output pairs for the task of interest. In other words, you provide examples of prompts (or questions, instructions, etc.) along with the correct or desired responses, and continue training the model on these. The model's weights are adjusted to minimize a supervised loss (typically cross-entropy between its predictions and the target output tokens). This is essentially the same kind of training used in most supervised machine learning tasks, now applied to LLM to specialize it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2825928e",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "Amazon Nova 2.0 introduces enhanced fine-tuning capabilities on Amazon Bedrock, including supervised fine-tuning with reasoning content and reinforcement fine-tuning with reward-based optimization.\n",
    "\n",
    "### Supervised fine-tuning on Amazon Nova 2.0\n",
    "\n",
    "Amazon Nova 2.0 supervised fine-tuning uses the same Converse API format as Amazon Nova 1.0 with optional reasoning content fields, allowing you to train models that show their thinking process before generating final answers.\n",
    "\n",
    "*Key features*\n",
    "* Support for text, image and video inputs in user content blocks\n",
    "* Optional reasoning content in assistant responses to capture intermediate thinking steps\n",
    "* Homogeneous dataset requirements (choose text-only, text+image, or text+video)\n",
    "* Support for PNG, JPEG and GIF images\n",
    "* Support for MOV, MKV and MP4 videos\n",
    "* Configurable reasoning modes for training optimization\n",
    "\n",
    "### Reinforcement fine-tuning (RFT) on Amazon Nova 2.0\n",
    "Reinforcement fine-tuning optimizes Amazon Nova models using measurable feedback signals rather than exact correct answers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da1b607",
   "metadata": {},
   "source": [
    "This notebook will walk through Supervised fine-tuning on Amazon Nova 2.0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebf9546",
   "metadata": {},
   "source": [
    "## 2. Prerequisites and Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15a2924",
   "metadata": {},
   "source": [
    "### Prerequisite: Data Prep Notebook\n",
    "The Data Prep notebook walks through preparing and transformming a public dataset into a format and scheme acceptable for SMTJ.  The Data Prep notebook creates training and validation datasets used for training a model, as well as a test dataset for evaluation.\n",
    "\n",
    "**--------------- STOP ---------------** <br><br>To complete this notebook, the Data Prep notebook must be completed first. In that workbook, training, validation, and eval datasets are created.  These datasets are carried over for use in this notebook.  Specific items from the Data Prep notebook, used in this notebook, are called out below.\n",
    "<br><br>\n",
    "\n",
    "Either restore or set these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbecf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This value is obtained as result of executing the data prep notebook\n",
    "train_dataset_s3_path = \"\"\n",
    "%store -r train_dataset_s3_path \n",
    "\n",
    "print(train_dataset_s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c72805",
   "metadata": {},
   "source": [
    "#### *Note* \n",
    "We will be using the Bedrock UI to train a model.  We will not be using the `train_dataset_s3_path` in the notebook directly as code.  Instead, from the Bedrock UI, this S3 URI will be navigated to and then selected during setup (yes, the S3 URI can be copied and pasted into the required fields).  \n",
    "\n",
    "Make sure the S3 bucket is accessible to Bedrock.  Or, copy the file `dataset.json` to an S3 bucket that is accessible to Bedrock.\n",
    "\n",
    "For this notebook example, the datset.json file was copied from the SageMaker bucket to a new bucket location.  This was done to keep the SageMaker bucket accessible to only SageMaker.<br><br>\n",
    "`s3://notebook-resource-nova/customization/bedrock/training-data/dataset.jsonl`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df9667d",
   "metadata": {},
   "source": [
    "### Prequisite: Service Role\n",
    "For the notebook, we will create and use a service account called \n",
    "\n",
    "`NovaCustomizationRole`.\n",
    "\n",
    "<br>This role is create by following this guidance.\n",
    "\n",
    "[Create a service role for model customization](https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-iam-role.html)\n",
    "\n",
    " \n",
    "This role can be create now. Or, later in this notebook, there is the opportunity to create this role during the customization job set-up. During the customization job set-up, in the Serice access section, the permissions of this role can be seen by clicking on \"View permission details\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5495ba",
   "metadata": {},
   "source": [
    "## 3. Data Prep - Review\n",
    "In the data prep workbook, we created our training, validation, and test datasets.  We will use the train and validation datasets for training.\n",
    "\n",
    "Remember, prepare high-quality prompt-response pairs for training. Data should be:\n",
    "- Consistent in format\n",
    "- Representative of desired behavior\n",
    "- Deduplicated and cleaned\n",
    "\n",
    "\n",
    "For reference, here is the schema that represents a single record in thre training data.  Amazon Nova 2.0 SFT data uses the same Converse API format as Amazon Nova 1.0, with the addition of optional reasoning content fields (optional fields to be show in the \"Exploring More\" section below.)\n",
    "\n",
    "```\n",
    "{\n",
    "  \"schemaVersion\": \"bedrock-conversation-2024\",\n",
    "  \"system\": [{\"text\": \"You are a digital assistant with a friendly personality\"}],\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [{ \"text\": \"What is the capital of Mars?\"}]\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": [{\"text\": \"Mars does not have a capital. Perhaps it will one day.\"}]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24c1828",
   "metadata": {},
   "source": [
    "## 4. Model Supervised Fine Tuning (SFT) and Customization\n",
    "You can customize Amazon Nova models through Bedrock.\n",
    "\n",
    "There are 3 options for customization:\n",
    "* Supervised fine-tuning job\n",
    "* Distillation job\n",
    "* Reinforcement fine tuning job\n",
    "\n",
    "### Create Training Job\n",
    "\n",
    "To start, navigate to the Bedrock console.  In the left-hand menu under Tune, select Custom models.\n",
    "\n",
    "For this notebook, choose Supervised fine-tuning job.\n",
    "\n",
    "![Nova Customization](./images/b-select-job.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7e9eaf",
   "metadata": {},
   "source": [
    "Upon selecting Supervised fine-tuning job, you will be greeted with the Create Fine-tuning job page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ae4685",
   "metadata": {},
   "source": [
    "### Job configuration\n",
    "Set the job name as:\n",
    "<br>peft-fine-tuning-without-reasoning-67-job\n",
    "\n",
    "This name must be unique in the list of model customization jobs.\n",
    "\n",
    "This job name has \"without-reasonining\" in the name. The reason to have naming is to point out that the data for training job could have additional reasoning data. See the \"Exploring More\" section at the end.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5276f9",
   "metadata": {},
   "source": [
    "![Job configuration](./images/b-job-config.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2bcfbb",
   "metadata": {},
   "source": [
    "### Model details\n",
    "Model details is a 2 step process. \n",
    "1.  Select the source model from which you would like to customize.  This could be a base model, or, this could be a previously trained model, allowing for iterative training.\n",
    "2.  Set the Fine-tune model name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8488a9c",
   "metadata": {},
   "source": [
    "#### Select Source model\n",
    "Choose \"Select Model\".  In the \"Categories\" left pane, in the \"Serverless model providers\" section, select \"Amazon\".  This will update the available models.  \n",
    "\n",
    "Select \"Nova 2 Lite\".\n",
    "\n",
    "As a side note, the section \"Custom & managed endpoints\" is where one would select a previously trained model in order to perform iterative training.\n",
    "\n",
    "![Job configuration](./images/b-select-model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3d5180",
   "metadata": {},
   "source": [
    "#### Fine-tuned model name\n",
    "Set the Fine-tuned model name: <br>peft-fine-tuning-without-reasoning-67-model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a480570",
   "metadata": {},
   "source": [
    "Here are the Model details completed\n",
    "\n",
    "![Model details](./images/b-model-details.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35eeb065",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Use default setting for this notebook.\n",
    "\n",
    "![Hyperparameters](./images/b-hyperparameters.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a57db18",
   "metadata": {},
   "source": [
    "### Input data\n",
    "Select the location where the training data was stored.  This training data was prepared in the data prep notebook.  And in the cells above, it was asked that the dataset.jsonl file be place in a location accessible to Bedrock.\n",
    "\n",
    "Set Input data:\n",
    "<br>s3://notebook-resource-nova/customization/bedrock/training-data/dataset.jsonl\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d9ecdb",
   "metadata": {},
   "source": [
    "### Output data\n",
    "Select the location where the completed training artifacts will be placed.  Again, make sure this location is accessible to Bedrock\n",
    "\n",
    "Set Output data:\n",
    "<br>s3://notebook-resource-nova/customization/bedrock/outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8cc3c3",
   "metadata": {},
   "source": [
    "Here are the completed Input data and Output data\n",
    "\n",
    "![Input data and Output data](./images/b-input-output.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0abef6",
   "metadata": {},
   "source": [
    "### Service Access\n",
    "In this notebook demonstration, we are going to use an existing service role.\n",
    "\n",
    "NovaCustomizationRole\n",
    "\n",
    "This role was created using the following documentation guidelines.\n",
    "\n",
    "[Create a service role for model customization](https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-iam-role.html)\n",
    "\n",
    "<br>Also, click on \"View permission details\" to see the permission policy and trust relationship necessary for the role."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e18065",
   "metadata": {},
   "source": [
    "Here is the completed Service access\n",
    "\n",
    "![Service Access](./images/b-service-access.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a8f8f2",
   "metadata": {},
   "source": [
    "### Create the job\n",
    "With all the above steps complete, click \"Create Job\".  Jobs will take time to run and there is not a clear mechanism to use in order to know when a job will start or complete.\n",
    "\n",
    "Jobs will validate the data to ensure the proper format is used.\n",
    "\n",
    "Here is the screen of what will be seen upon \"Create Job\".\n",
    "\n",
    "![Job Execution](./images/b-job-execution.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c231b710",
   "metadata": {},
   "source": [
    "Jobs will then run until completion (failure or succcess).  To observe the current status of the job, in Bedrock console, select Custom models (under the Tune left-hand menu), and then select the respective job in progress.\n",
    "\n",
    "![Service Access](./images/b-in-progress.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacc4b5f",
   "metadata": {},
   "source": [
    "********** ********** ********** ********** **********\n",
    "In our scenario, training of this model took approximately 1 hour and 45 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caca253d",
   "metadata": {},
   "source": [
    "## Deployment and Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b7f271",
   "metadata": {},
   "source": [
    "Woot! Our model has been trained.  But the model has not been prepared for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa6abfb",
   "metadata": {},
   "source": [
    "### Inference\n",
    "To set up the model for inference, navigate to Custom models.  In the Models section, click on \"peft-fine-tuning-without-reasoning-67-model.\n",
    "\n",
    "We are then presented Model details, Job overview, Model overview, and Training metric.\n",
    "\n",
    "Click \"Setup inference\".  \n",
    "Choose \"Deploy for on-demand\".\n",
    "\n",
    "![Setup inference](./images/b-setup-inference.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef79a88",
   "metadata": {},
   "source": [
    "There are 2 deployment options\n",
    "* On-demand Inference\n",
    "* Provisioned Throughput\n",
    "\n",
    "On-demand inference for Bedrock Nova models offers pay-per-use pricing with no capacity planning required, making it suitable for variable or unpredictable workloads.\n",
    "\n",
    "Provisioned Throughput for Bedrock Nova models provides guaranteed, consistent performance by reserving dedicated capacity upfront, making it ideal for predictable, high-volume workloads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9bf696",
   "metadata": {},
   "source": [
    "### Deploy for on-demand\n",
    "Select \"Deploy for on-demand\", enter a unique deployement name and short description.  Ensure the selected model is our recently trained model.\n",
    "\n",
    "For deployment name, use:\n",
    "peft-fine-tuning-without-reasoning-67-deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c86b72",
   "metadata": {},
   "source": [
    "Here is our completed screen.\n",
    "\n",
    "![Deployment](./images/b-deploy-od.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17013a83",
   "metadata": {},
   "source": [
    "It will take about 1 minute for deployment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cd36d2",
   "metadata": {},
   "source": [
    "## Playground - Experiment with the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bf7571",
   "metadata": {},
   "source": [
    "Once the model is deployed correctly, let's experiment and test the model.  \n",
    "\n",
    "To verify the model deployed correctly, navigate to \"Custom model on-demand\" and in the \"Custom model deployments\" section, click our Deployment name.\n",
    "\n",
    "![Deployment name](./images/b-custom-model-on-demand.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a5b78e",
   "metadata": {},
   "source": [
    "Click the \"Test in playground\" button\n",
    "\n",
    "![Test in playground](./images/b-test-in-playground.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d388c6",
   "metadata": {},
   "source": [
    "We are in the Text playground. \n",
    "\n",
    "Try a simple prompt:  \"hi\"\n",
    "\n",
    "![Playground](./images/b-playground.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86474b6e",
   "metadata": {},
   "source": [
    "Voila!  We have a response from our custom model.  Try more prompts.  \n",
    "\n",
    "Also, try to switch the mode from \"Single prompt\" to \"Chat\".  You might have to re-select the custom model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1884ca",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This notebook shows that with minimal steps, we can train a custom model and then deploy that model for inference, using the Bedrock UI alone.  How exciting and efficient.\n",
    "\n",
    "Experiment more with custom models.  Give our RFT (Reinforcement Fine-Tuning) notebooks a try as well.\n",
    "\n",
    "Hope you were able to learn how easy it is to train, deploy, and inference a custom Nova 2 Lite model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3669369c",
   "metadata": {},
   "source": [
    "## Exploring More"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2143b0",
   "metadata": {},
   "source": [
    "### Reasoning Data for Training\n",
    "Amazon Nova 2.0 SFT data uses the same Converse API format as Amazon Nova 1.0, with the addition of optional reasoning content fields.\n",
    "\n",
    "Reasoning content (also called chain-of-thought) captures the model's intermediate thinking steps before generating a final answer. In the assistant turn, we use the reasoningContent field to include these reasoning traces.\n",
    "\n",
    "Please see the documentation [Supervised fine-tuning on Amazon Nova 2.0](https://docs.aws.amazon.com/bedrock/latest/userguide/nova-2-sft-data-prep.html) for more details.\n",
    "\n",
    "An example of a data that uses reasoning:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"schemaVersion\": \"bedrock-conversation-2024\",\n",
    "  \"system\": [\n",
    "    {\n",
    "      \"text\": \"You are a digital assistant with a friendly personality\"\n",
    "    }\n",
    "  ],\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"text\": \"What country is right next to Australia?\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"reasoningContent\": {\n",
    "            \"reasoningText\": {\n",
    "              \"text\": \"I need to use my world knowledge of geography to answer this question\"\n",
    "            }\n",
    "          }\n",
    "        },\n",
    "        {\n",
    "          \"text\": \"The closest country to Australia is New Zealand, located to the southeast across the Tasman Sea.\"\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "Currently, only reasoningText is supported within reasoningContent. Multimodal reasoning content is not yet available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02381cc",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
