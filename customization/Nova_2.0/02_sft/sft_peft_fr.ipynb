{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Fine-Tuning (SFT) Amazon Nova 2 Lite using Amazon SageMaker Training Jobs\n",
    "\n",
    "First, it helps to understand the general flow of training LLMs. Training a large language model typically has two major stages: pre-training and post-training. During pre-training, the model is exposed to trillions of tokens of raw text and optimized purely for next-token prediction. This makes it an extremely capable pattern completer over the distribution of web and curated text. It absorbs syntax, semantics, facts, and broad reasoning patterns. But it is unaligned with human intent, meaning it does not inherently understand instructions, user goals, or context-appropriate behavior. It simply continues text in whatever style best fits its training distribution. As a result, a pre-trained model tends to autocomplete rather than follow directions, is inconsistent about formatting or tool use, and can mirror undesirable biases or unsafe content present in the data. In short, pre-training builds general competence, not usefulness for tasks.\n",
    "\n",
    "Post-training turns that competent pattern completer into a useful assistant. Teams typically run multiple rounds of Supervised Fine-Tuning (SFT) to teach the model to follow instructions, adhere to schemas and policies, call tools, and produce reliable, scoped outputs by imitating high-quality demonstrations. This adds a first layer of alignment where the model learns to respond to prompts as tasks, not just text to continue. They then apply Reinforcement Fine-Tuning (RFT) to push behavior further using measurable feedback (e.g., verifiers or an LLM-as-a-judge), optimizing nuanced trade-offs like accuracy vs. brevity, safety vs. coverage, or multi-step reasoning under constraints. In practice, teams alternate SFT and RFT in cycles, progressively shaping the pre-trained model into a reliable, policy-aligned system that performs complex tasks with consistency.\n",
    "\n",
    "Supervised fine-tuning is the classic approach of training the LLM on a dataset of human-labeled input-output pairs for the task of interest. In other words, you provide examples of prompts (or questions, instructions, etc.) along with the correct or desired responses, and continue training the model on these. The model's weights are adjusted to minimize a supervised loss (typically cross-entropy between its predictions and the target output tokens). This is essentially the same kind of training used in most supervised machine learning tasks, now applied to LLM to specialize it.\n",
    "\n",
    "### Model Customization\n",
    "You can customize Amazon Nova models through base recipes using and Amazon SageMaker Training Jobs (SMTJ). These recipes support Supervised Fine-Tuning (SFT), with both Full-Rank and Low-Rank Adaptation (LoRA) options.  \n",
    "\n",
    "End-to-end customization workflow involves stages like \n",
    "- model training\n",
    "- model evaluation\n",
    "- deployment for inference. \n",
    "\n",
    "Further, SMTJ allow Nova models to customized using iterative training.  Iterative training is the process of repeatedly fine-tuning a model through multiple training cycles across different training methods — train, evaluate, analyze errors, adjust data/objectives/hyperparameters — with each round starting from the previous checkpoint.\n",
    "\n",
    "This model customization approach on SageMaker AI provides greater flexibility and control to fine-tune its supported Amazon Nova models, optimize hyperparameters with precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Getting Started\n",
    "\n",
    "This notebook demonstrates Full Rank Supervised Fine-Tuning (SFT), Supervised Fine-Tuning (SFT) with Parameter-Efficient Fine-Tuning (PEFT), and iterative training of Amazon Nova using Amazon SageMaker Training Jobs. \n",
    "\n",
    "Supervised Fine-Tuning (SFT) trains an LLM on labeled input-output pairs to modify behavior for specific tasks. Training is essentially a continuous loop of steps:\n",
    "\n",
    "- Data preparation\n",
    "- Model customization / training\n",
    "- Model evaluation\n",
    "- Deployment and inference\n",
    "- Analysis Feedback\n",
    "\n",
    "This notebook will focus on Model customization / training using SageMaker Training Jobs (SMTJ), a feature supported in SageMaker AI.\n",
    "\n",
    "As Nova models are 1st party models, access to model weights is not permitted.  However, using a SMTJ coupled with a \"recipe\" accomplishes the customization training goal.\n",
    "\n",
    "A recipe is simply a configuration file.\n",
    "\n",
    "Once a SMTJ completes, the job results are place into S3, and a `manifest.json` file and a `step_wise_training_metrics.csv` are created.  The manifest file identifies the escrow location of the trained model. An escrow account holds and manages assets (model weights) ensuring security."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prerequisites and Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies\n",
    "Several python packages will need to be installed in order to execute this notebook.  Please review the packages in requirements.txt. \n",
    "\n",
    "botocore, boto3, sagemaker are required for the training jobs, while the other packages are used to help visualize results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r ./requirements.txt --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisite: Data Prep Notebook\n",
    "The Data Prep notebook walks through preparing and transforming a public dataset into a format and scheme acceptable for SMTJ.  The Data Prep notebook creates training and validation datasets used for training a model, as well as a test dataset for evaluation.\n",
    "\n",
    "**--------------- STOP ---------------** <br><br>To complete this notebook, the Data Prep notebook must be completed first. In that workbook, training, validation, and eval datasets are created.  These datasets are carried over for use in this notebook.  Specific items from the Data Prep notebook, used in this notebook, are called out below.\n",
    "<br><br>\n",
    "\n",
    "Either restore or set these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This value is obtained as result of executing the data prep notebook\n",
    "train_dataset_s3_path = \"\"\n",
    "%store -r train_dataset_s3_path \n",
    "\n",
    "print(train_dataset_s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credentials, Sessions, Roles, and more!\n",
    "\n",
    "This section sets up the necessary AWS credentials and SageMaker session to run the notebook. You'll need proper IAM permissions to use SageMaker.\n",
    "\n",
    "\n",
    "If you are going to use Sagemaker in a local environment, you will need access to an IAM Role with the required permissions for Sagemaker. Learn more about it here [AWS Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html).\n",
    "\n",
    "For more details on other Nova pre-requisites needed check out [AWS Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/nova-model-general-prerequisites.html)\n",
    "\n",
    "The code initializes a SageMaker session, sets up the IAM role, and configures the S3 bucket for storing training data artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "\n",
    "sagemaker_session = sagemaker.Session(boto_session=boto3.Session(region_name='us-east-1'))\n",
    "\n",
    "sagemaker_session_bucket = None\n",
    "\n",
    "if sagemaker_session_bucket is None and sagemaker_session is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "    \n",
    "except ValueError:\n",
    "    iam = boto3.client(\"iam\")\n",
    "    role = iam.get_role(RoleName=\"sagemaker_execution_role\")[\"Role\"][\"Arn\"]\n",
    "\n",
    "bucket_name = sagemaker_session.default_bucket()\n",
    "default_prefix = sagemaker_session.default_bucket_prefix\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sagemaker_session.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sagemaker_session.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capture S3 bucket prefix for later use.  After a SageMaker Training Job completes, this value will be used to identify where the SMTJ outputs are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if default_prefix:\n",
    "    output_path_prefix = f\"s3://{bucket_name}/{default_prefix}\"\n",
    "\n",
    "else:\n",
    "    output_path_prefix= f\"s3://{bucket_name}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Prep - Review\n",
    "In the data prep workbook, we created our training, validation, and test datasets.  We will use the train and validation datasets for training.\n",
    "\n",
    "Remember, prepare high-quality prompt-response pairs for training. Data should be:\n",
    "- Consistent in format\n",
    "- Representative of desired behavior\n",
    "- Deduplicated and cleaned\n",
    "\n",
    "\n",
    "For reference, here is the schema that represents a single record in thre training data.\n",
    "\n",
    "```\n",
    "{\n",
    "  \"schemaVersion\": \"bedrock-conversation-2024\",\n",
    "  \"system\": [{\"text\": \"You are a digital assistant with a friendly personality\"}],\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [{ \"text\": \"What is the capital of Mars?\"}]\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": [{\"text\": \"Mars does not have a capital. Perhaps it will one day.\"}]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Supervised Fine Tuning (SFT) and Customization\n",
    "You can customize Amazon Nova models through recipes and train them on SageMaker AI. Recipes support techniques such as supervised fine-tuning (SFT), with both full-rank and low-rank adaptation (LoRA) options.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model fine-tuning\n",
    "\n",
    "To customize a model, the SageMaker Training Job (SMTJ) uses a PyTorch estimator to run the customization.  The estimator defines properties of the training job, such as training job name, instance type, instance count, output location for job results, training recipe, and more.\n",
    "\n",
    "More details of the estimator can be found here: [PyTorch Estimator](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html)\n",
    "\n",
    "So we must define\n",
    "- the training recipe\n",
    "- properties for the estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recipes\n",
    "A Nova recipe is a YAML configuration file that provides details to SageMaker AI on how to run your model customization job. It provides the base model name, sets training hyperparameters, defines optimization settings, and includes any additional options required to fine-tune or train the model successfully.\n",
    "\n",
    "These recipe files serve as the blueprint for your model customization jobs, allowing you to specify training parameters, hyperparameters, and other critical settings that determine how your model learns from your data. To adjust the hyperparameters, follow the guidelines in [Selecting hyperparameters](https://docs.aws.amazon.com//nova/latest/userguide/customize-fine-tune-hyperparameters.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuring the Model and Recipe\n",
    "\n",
    "This specifies which model to fine-tune and the recipe to use. The recipe includes \"LoRA\" indicating parameter-efficient fine-tuning, and \"sft\" indicating supervised fine-tuning.\n",
    "\n",
    "The recipes can be found at:\n",
    "- [Amazon Nova recipes](https://docs.aws.amazon.com/sagemaker/latest/dg/nova-model-recipes.html), or \n",
    "- [GitHub - sagemaker-hyperpod-recipes](https://github.com/aws/sagemaker-hyperpod-recipes). Navigate to `recipes_collection -> recipes -> fine-tuning/nova -> nova_2_0/nova_lite`  to find all Nova 2 Lite fine tuning recipes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take a look at 3 SFT recipe examples - PEFT, Full Rank, and Iterative Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PEFT SFT Recipe\n",
    "This recipe configures PEFT / LoRA SFT training.\n",
    "\n",
    "Note, the value for `peft_recipe` is found in the \"Amazon Nova recipes\" link above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_recipe = \"fine-tuning/nova/nova_2_0/nova_lite/SFT/nova_lite_2_0_p5_gpu_lora_sft\"\n",
    "peft_recipe_job_name = \"train-peft-sft-nova-lite-2-recipe-job\" # example\n",
    "peft_sm_job_name = \"train-peft-sft-nova-lite-2\"\n",
    "\n",
    "peft_recipe_overrides = {\n",
    "    \"run\": {\n",
    "        \"name\": peft_recipe_job_name,\n",
    "        \"data_s3_path\": \"\",  # For SMTJ, this is \"\"\n",
    "        \"output_s3_path\": f\"{output_path_prefix}/{peft_sm_job_name}\"\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Rank Recipe\n",
    "This recipe configures Full Rank SFT training.\n",
    "\n",
    "Note, the value for `full_rank_recipe` is found in the \"Amazon Nova recipes\" link above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_rank_recipe = \"fine-tuning/nova/nova_2_0/nova_lite/SFT/nova_lite_2_0_p5_gpu_sft\"\n",
    "full_rank_recipe_job_name = \"train-fr-sft-nova-lite-2-recipe-job\" # example\n",
    "full_rank_sm_job_name = \"train-fr-sft-nova-lite-2\"\n",
    "\n",
    "full_rank_recipe_overrides = {\n",
    "    \"run\": {\n",
    "        \"name\": full_rank_recipe_job_name,\n",
    "        \"data_s3_path\": \"\",  # For SMTJ, this is \"\"\n",
    "        \"output_s3_path\": f\"{output_path_prefix}/{full_rank_sm_job_name}\"\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative Training Recipe\n",
    "Iterative training is the process of repeatedly fine-tuning a model through multiple training cycles across different training methods — train, evaluate, analyze errors, adjust data/objectives/hyperparameters — with each round starting from the previous checkpoint. This approach allows you to systematically target model failure modes, incorporate curated examples addressing specific weaknesses, and adapt to changing requirements over time.\n",
    "\n",
    "#### How it works\n",
    "After each training job completes, a manifest file is generated in the output location specified by the output_path parameter in your training configuration.\n",
    "\n",
    "To access your checkpoint:\n",
    "\n",
    "1. Navigate to your specified output_path in S3\n",
    "2. Download and extract the output.tar.gz file\n",
    "3. Open the manifest.json file inside\n",
    "4. Locate the checkpoint_s3_bucket parameter, which contains the S3 URI of your trained model.  \n",
    "\n",
    "**-------------- Important --------------**<br><br>\n",
    "The checkpoint_s3_bucket URI found in the manifest.json file is the source of truth for location of the trained model checkpoint.\n",
    "<br><br>\n",
    "\n",
    "Example manifest.json structure:\n",
    "```\n",
    "{\n",
    "  \"checkpoint_s3_bucket\": \"s3://customer-escrow-<account-number>-smtj-<unique-identifier>/<job-name>/stepID\",\n",
    "  ...\n",
    "}```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back now to the recipe, this recipe configures Iterative training.\n",
    "\n",
    "Note, the value for `iterative_peft_recipe` is found in the \"Amazon Nova recipes\" link above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterative_peft_recipe = \"fine-tuning/nova/nova_2_0/nova_lite/SFT/nova_lite_2_0_p5_gpu_lora_sft\"\n",
    "iterative_peft_recipe_job_name = \"train-iterative-peft-sft-nova-lite-2-recipe-job\" # example\n",
    "iterative_peft_sm_job_name = \"train-iterative-peft-sft-nova-lite-2\"\n",
    "\n",
    "iterative_peft_model_name_or_path = \"<place checkpoint_s3_bucket value from manifest.json>\" # example\n",
    "\n",
    "iterative_peft_recipe_overrides = {\n",
    "    \"run\": {\n",
    "        \"name\": iterative_peft_recipe_job_name,\n",
    "        \"model_name_or_path\": iterative_peft_model_name_or_path,\n",
    "        \"data_s3_path\": \"\",  # For SMTJ, this is \"\"\n",
    "        \"output_s3_path\": f\"{output_path_prefix}/{iterative_peft_sm_job_name}\"\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select training technique to use\n",
    "This is just a helper that allows chosing of whichever recipe desired.  Change the value of `technique` to one of the keywords indicated in the comment.\n",
    "\n",
    "This will allow running the notebook efficiently.  Change the key, run the cells.  Change the keyword to another technique, run the cells.  Easy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# technique values: \"PEFT\" OR \"FR\" OR \"IPEFT\"\n",
    "technique = \"PEFT\" \n",
    "\n",
    "\n",
    "sm_training_job_name = \"\"\n",
    "training_recipe = \"\"\n",
    "recipe_overrides = {}\n",
    "\n",
    "\n",
    "if technique == \"PEFT\":\n",
    "    # PEFT Training\n",
    "    print(\"PEFT Training\")\n",
    "    training_recipe = peft_recipe\n",
    "    recipe_overrides = peft_recipe_overrides\n",
    "    sm_training_job_name = peft_sm_job_name\n",
    "elif technique == \"FR\":\n",
    "    # Full Rank Training\n",
    "    print(\"Full Rank Training\")\n",
    "    training_recipe = full_rank_recipe\n",
    "    recipe_overrides = full_rank_recipe_overrides\n",
    "    sm_training_job_name = full_rank_sm_job_name\n",
    "elif technique == \"IPEFT\":\n",
    "    # Iterative PEFT Training\n",
    "    print(\"Iterative PEFT Training\")\n",
    "    training_recipe = iterative_peft_recipe\n",
    "    recipe_overrides = iterative_peft_recipe_overrides\n",
    "    sm_training_job_name = iterative_peft_sm_job_name\n",
    "else:\n",
    "    print(\"*** Issue - training undefined ***\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instance Type and Count\n",
    "\n",
    "P5 instances are optimized for deep learning workloads, providing high-performance GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = \"ml.p5.48xlarge\"\n",
    "instance_count = 4\n",
    "\n",
    "print(f\"instance_type: \\n{instance_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select Container Image URI\n",
    "\n",
    "This specifies the pre-built container for SFT fine-tuning of Nova 2 Lite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri = \"708977205387.dkr.ecr.us-east-1.amazonaws.com/nova-fine-tune-repo:SM-TJ-SFT-V2-latest\"\n",
    "\n",
    "print(f\"image_uri: \\n{image_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output path\n",
    "This path will be used to write model results of the model training.  In this location will be found an output.tar.gz file, containing the `manifest.json` and `step_wise_training_metrics.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "print(f\"recipe_overrides: \\n{json.dumps(recipe_overrides, indent=4)}\\n\")\n",
    "\n",
    "output_s3_path = recipe_overrides[\"run\"][\"output_s3_path\"]\n",
    "\n",
    "print(f\"output_path:\\n{output_s3_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure PyTorch estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(\n",
    "    output_path=output_s3_path,\n",
    "    base_job_name=sm_training_job_name,\n",
    "    role=role,\n",
    "    disable_profiler=True,\n",
    "    debugger_hook_config=False,\n",
    "    instance_count=instance_count,\n",
    "    instance_type=instance_type,\n",
    "    training_recipe=training_recipe,\n",
    "    recipe_overrides=recipe_overrides,\n",
    "    max_run=432000,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    image_uri=image_uri,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure the Data Channels\n",
    "\n",
    "Configure the Data Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "train_input = TrainingInput(\n",
    "    s3_data=train_dataset_s3_path,\n",
    "    distribution=\"FullyReplicated\",\n",
    "    s3_data_type=\"Converse\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start the Training Job\n",
    "This starts the training job with the configured estimator and datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting the train job with our uploaded dataset as input\n",
    "estimator.fit(inputs={\"train\": train_input}, wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_job_name = estimator.latest_training_job.name\n",
    "print(f'Training Job Name:  \\n{training_job_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, Markdown, Image\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region={}#/jobs/{}\">Training Job</a> After About 5 Minutes</b>'.format(\"us-east-1\", training_job_name)))\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/cloudwatch/home?region={}#logStream:group=/aws/sagemaker/TrainingJobs;prefix={};streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After About 5 Minutes</b>'.format(\"us-east-1\", training_job_name)))\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://s3.console.aws.amazon.com/s3/buckets/{}/{}/?region={}&tab=overview\">S3 Output Data</a> After The Training Job Has Completed</b>'.format(bucket_name, training_job_name, \"us-east-1\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## _Wait Until the ^^ Training Job ^^ Completes Above!( 20-40 mins)_\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Artifacts\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Reading the Output Content after training job completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the S3 model artifact Output s3 uri\n",
    "model_s3_uri = estimator.model_data\n",
    "\n",
    "output_s3_uri = \"/\".join(model_s3_uri.split(\"/\")[:-1])+\"/output.tar.gz\"\n",
    "\n",
    "print(f\"model_s3_uri: \\n{model_s3_uri}\\n\")\n",
    "print(f\"output_s3_uri: \\n{output_s3_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2. Downloading and Extracting the Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_folder = \"tmp\"\n",
    "\n",
    "# create a unique folder name based on the output s3 uri\n",
    "folder = output_s3_uri.rsplit('/', 2)[0] \n",
    "folder = folder.rsplit('/', 1)[-1]\n",
    "\n",
    "print(f\"folder: \\n{folder}\\n\")\n",
    "\n",
    "!mkdir -p ./$tmp_folder/$folder/train_output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp $output_s3_uri ./$tmp_folder/$folder/train_output/output.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xvzf ./$tmp_folder/$folder/train_output/output.tar.gz -C ./$tmp_folder/$folder/train_output/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Open Manifest and get the model checkpoint\n",
    "This is one of the most important steps to find the current model checkpoint. UI's may get confusing, but the `checkpoint_s3_bucket` is the source of truth for the location of the model checkpoint artifacts.  Again, this location is within an AWS managed account, and not accessible to users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "checkpoint_s3_bucket = json.load(open(f'./{tmp_folder}/{folder}/train_output/manifest.json'))['checkpoint_s3_bucket']\n",
    "\n",
    "# This value will be used for custom model Deployment as well as for model Evaluation (see eval)\n",
    "print(f\"checkpoint_s3_bucket: \\n{checkpoint_s3_bucket}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4. Plotting the Train/Loss Curve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the CSV files\n",
    "train_df = pd.read_csv(f'./{tmp_folder}/{folder}/train_output/step_wise_training_metrics.csv')\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_df['step_number'], train_df['training_loss'], label='Training Loss', color='blue')\n",
    "\n",
    "plt.xlabel('Step Number')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training vs Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Deployment / Inference\n",
    "Woot! All done!  Congratulations on training a custom model.\n",
    "\n",
    "Next, please see the deployment notebook in the deployment folder.\n",
    "\n",
    "<br>**---------- BEFORE YOU GO!! ----------**<br><br>\n",
    "The below values are needed for the customization notebook.\n",
    "\n",
    "To use that notebook, these values will need to be recorded and used:\n",
    "- SageMaker sm_training_job_name\n",
    "- checkpoint_s3_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nsm_training_job_name:\\n{sm_training_job_name}\\n\")\n",
    "print(f\"checkpoint_s3_bucket: \\n{checkpoint_s3_bucket}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store sm_training_job_name\n",
    "%store checkpoint_s3_bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Exploring More for SFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Nova 2 Lite Iterative Training\n",
    "Organizations often face challenges when implementing single-shot fine-tuning approaches for their generative AI models. The single-shot fine-tuning method involves selecting training data, configuring hyperparameters, and hoping the results meet expectations without the ability to make incremental adjustments. Single-shot fine-tuning frequently leads to suboptimal results and requires starting the entire process from scratch when improvements are needed.\n",
    "\n",
    "Iterative fine-tuning provides several advantages over single-shot approaches that make it valuable for production environments. Risk mitigation becomes possible through incremental improvements, so you can test and validate changes before committing to larger modifications. With this approach, you can make data-driven optimization based on real performance feedback rather than theoretical assumptions about what might work. The methodology also helps developers to apply different training techniques sequentially to refine model behavior. Most importantly, iterative fine-tuning accommodates evolving business requirements driven by continuous live data traffic. As user patterns change over time and new use cases emerge that weren’t present in initial training, you can leverage this fresh data to refine your model’s performance without starting from scratch.\n",
    "\n",
    "Iterative Training is available for Nova 2 Lite using either Bedrock or SageMaker Training Jobs.\n",
    "\n",
    "[SageMaker AI - Iterative Training](https://docs.aws.amazon.com/sagemaker/latest/dg/nova-hp-iterative-training.html)\n",
    "\n",
    "[Iterative fine-tuning on Amazon Bedrock](https://aws.amazon.com/blogs/machine-learning/iterative-fine-tuning-on-amazon-bedrock-for-strategic-model-improvement/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
