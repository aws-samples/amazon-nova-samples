{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58d70ffe-5f6c-4162-9714-7a77250d2aac",
   "metadata": {},
   "source": [
    "##  Offline model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c7aa3d-6e05-4346-bf7f-5f4ecfd095f8",
   "metadata": {},
   "source": [
    "## Installing Dependencies\n",
    "\n",
    "The first cell installs the required Python packages for this notebook. For more details on other pre-requisites needed check out [AWS Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/nova-model-general-prerequisites.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0be9be-f3e1-4300-b3b0-866d08f9d87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r ./requirements.txt --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc64053-c16d-45fa-9b63-6aab790c8490",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345c2f54-e73d-4069-a002-a72adb552c59",
   "metadata": {},
   "source": [
    "## Getting model output variables from fine tuned notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3da94b-f124-4b3a-92e3-9c5056820b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "sagemaker_session_bucket = None\n",
    "\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client(\"iam\")\n",
    "    role = iam.get_role(RoleName=\"sagemaker_execution_role\")[\"Role\"][\"Arn\"]\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "bucket_name = sess.default_bucket()\n",
    "default_prefix = sess.default_bucket_prefix\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d509baeb-fce2-4611-ae84-217ce63e69bf",
   "metadata": {},
   "source": [
    "The purpose of the evaluation process is to assess trained-model performance against benchmarks or custom dataset. The evaluation process typically involves steps to create evaluation recipe pointing to the trained model, specify evaluation datasets and metrics, submit a separate training job for the evaluation, and evaluate against standard benchmarks or custom data. The evaluation process will output performance metrics stored in your Amazon S3 bucket.\n",
    "\n",
    "Create minimal recipe for `gen_qa` evaluation. With `gen_qa` evaluation, we bring our own dataset for evaluation, and measure the following metrics:\n",
    "\n",
    "* rouge1\n",
    "* rouge2\n",
    "* rougeL\n",
    "* exact_match\n",
    "* quasi_exact_match\n",
    "* f1_score\n",
    "* f1_score_quasi\n",
    "* bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e80fb5-fb8e-4a12-a782-0b3078f9f0e9",
   "metadata": {},
   "source": [
    "#### Specify instance type and image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edfdcdf-ba6b-449d-b111-4f036606c97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = \"ml.p5.48xlarge\" # Override the instance type if you want to get a different container version\n",
    "instance_count = 1\n",
    "\n",
    "instance_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47adf94e-8350-4ada-a88e-6be84a731c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri = f\"708977205387.dkr.ecr.us-east-1.amazonaws.com/nova-evaluation-repo:SM-TJ-Eval-Beta-latest\"\n",
    "\n",
    "image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccf4599-da98-4dd7-b899-d74974a1ba39",
   "metadata": {},
   "source": [
    "#### Configure the model and recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99c65fb-5fbf-4134-b8b9-abfae8000f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"nova-lite-2/prod\"\n",
    "recipe = \"evaluation/nova/nova_2_0/nova_lite/nova_lite_2_0_p5_48xl_gpu_bring_your_own_dataset_eval\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f417626-fc19-4c09-bb10-3f5c83c0bee9",
   "metadata": {},
   "source": [
    "#### Create a PyTorch estimator and run the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc1aa82-9525-45ae-9eda-87c13e6070ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "# define Training Job Name\n",
    "job_name = f\"train-{model_id.split('/')[0].replace('.', '-')}-peft-sft-eval\"\n",
    "\n",
    "# define OutputDataConfig path\n",
    "if default_prefix:\n",
    "    output_path = f\"s3://{bucket_name}/{default_prefix}/{job_name}\"\n",
    "else:\n",
    "    output_path = f\"s3://{bucket_name}/{job_name}\"\n",
    "\n",
    "recipe_overrides ={\n",
    "    \"run\": {\n",
    "        \"name\": job_name,\n",
    "        \"model_name_or_path\": model_id,\n",
    "        \"data_s3_path\": \"\",  # For SMTJ, this is \"\"\n",
    "        \"output_s3_path\": f\"{output_path}/{job_name}\"\n",
    "    },\n",
    "    \"processor\": {\n",
    "        \"lambda_arn\": \"\"\n",
    "    }\n",
    "}\n",
    "eval_estimator = PyTorch(\n",
    "    output_path=output_path,\n",
    "    base_job_name=job_name,\n",
    "    role=role,\n",
    "    disable_profiler=True,\n",
    "    debugger_hook_config=False,\n",
    "    instance_count=instance_count,\n",
    "    instance_type=instance_type,\n",
    "    training_recipe=recipe,\n",
    "    sagemaker_session=sess,\n",
    "    image_uri=image_uri\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086f9653-abaf-469b-b69c-46f501ad0a83",
   "metadata": {},
   "source": [
    "#### Configure data channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9377ace-85ae-48b8-8e74-eb67fc9c996b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "eval_input = TrainingInput(\n",
    "    s3_data=test_dataset_s3_path,\n",
    "    distribution=\"FullyReplicated\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d4e146-68d8-4c54-8f25-efcf685eb799",
   "metadata": {},
   "source": [
    "### Starting the Evaluation Job (SM Training jobs)\n",
    "This starts the training job with the configured estimator and datasets. Note that it uses the test dataset for validation during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c489e9e4-8c20-44c8-b917-4bcece08a057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting the train job with our uploaded datasets as input\n",
    "eval_estimator.fit(inputs={\"train\": eval_input}, wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659227d1-6421-4e12-a6b7-646f52395014",
   "metadata": {},
   "source": [
    "## ^^ _This will take 20-30 mins in evaluation_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c843abc1-7d69-4bda-92d0-aba7eb524d78",
   "metadata": {},
   "source": [
    "### Viewing the Evaluation Artifacts \n",
    "\n",
    "Downloading the artifact from Evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45998158-82cc-4aaf-bb24-c02b89b87e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = eval_estimator.model_data\n",
    "output = '/'.join(output.split(\"/\")[:-1]) +\"/output.tar.gz\"\n",
    "! aws s3 cp $output ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fdffc6-0258-4c56-943d-4a58b1b850e9",
   "metadata": {},
   "source": [
    "### Visualize results\n",
    "\n",
    "The notebook defines a function to visualize the evaluation metrics in a bar chart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eba3893-bcec-4a2c-a21b-b038c77385d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "tarfile.open('output.tar.gz', 'r:gz').extractall('output_folder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915860b1-01e7-440a-9423-dfc869743e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = \"output_folder/\" + recipe_job_name +\"/eval_results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69b8d81-2df2-4d98-bd1a-5010029dcc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "def plot_metrics(results):\n",
    "    # Extract metrics and their standard errors\n",
    "    metrics = {}\n",
    "    for key, value in results.items():\n",
    "        if not key.endswith(\"_stderr\"):\n",
    "            metrics[key] = {\"value\": value, \"stderr\": results.get(f\"{key}_stderr\", 0)}\n",
    "\n",
    "    # Sort metrics by value for better visualization\n",
    "    sorted_metrics = dict(\n",
    "        sorted(metrics.items(), key=lambda x: x[1][\"value\"], reverse=True)\n",
    "    )\n",
    "\n",
    "    # Prepare data for plotting\n",
    "    labels = list(sorted_metrics.keys())\n",
    "    values = [sorted_metrics[label][\"value\"] for label in labels]\n",
    "    errors = [sorted_metrics[label][\"stderr\"] for label in labels]\n",
    "\n",
    "    # Normalize BLEU score to be on the same scale as other metrics (0-1)\n",
    "    bleu_index = labels.index(\"bleu\") if \"bleu\" in labels else -1\n",
    "    if bleu_index >= 0:\n",
    "        values[bleu_index] /= 100\n",
    "        errors[bleu_index] /= 100\n",
    "\n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # Create bar chart\n",
    "    x = np.arange(len(labels))\n",
    "    bars = ax.bar(\n",
    "        x,\n",
    "        values,\n",
    "        yerr=errors,\n",
    "        align=\"center\",\n",
    "        alpha=0.7,\n",
    "        capsize=5,\n",
    "        color=\"skyblue\",\n",
    "        ecolor=\"black\",\n",
    "    )\n",
    "\n",
    "    # Add labels and title\n",
    "    ax.set_ylabel(\"Score\")\n",
    "    ax.set_title(\"Evaluation Metrics\")\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels, rotation=45, ha=\"right\")\n",
    "    ax.set_ylim(0, 1.0)\n",
    "\n",
    "    # Add value labels on top of bars\n",
    "    for i, bar in enumerate(bars):\n",
    "        height = bar.get_height()\n",
    "        # Convert BLEU back to its original scale for display\n",
    "        display_value = values[i] * 100 if labels[i] == \"bleu\" else values[i]\n",
    "        ax.text(\n",
    "            bar.get_x() + bar.get_width() / 2.0,\n",
    "            height + 0.01,\n",
    "            f\"{display_value:.2f}\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "        )\n",
    "\n",
    "    # Add a note about BLEU\n",
    "    if bleu_index >= 0:\n",
    "        ax.text(\n",
    "            0.5,\n",
    "            -0.15,\n",
    "            \"Note: BLEU score shown as percentage (original: {:.2f})\".format(\n",
    "                values[bleu_index] * 100\n",
    "            ),\n",
    "            transform=ax.transAxes,\n",
    "            ha=\"center\",\n",
    "            fontsize=9,\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b995a54b-cce7-4c8a-9548-ad1114e7875b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "def find_json_files(path):\n",
    "    return glob.glob(os.path.join(path, \"*.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c0c352-b6b7-410a-97db-9522fd68d66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results_path = find_json_files(results_path)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05293cf0-5375-47c6-a08a-6c6b4c08310d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(evaluation_results_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "fig = plot_metrics(data[\"results\"][\"all\"])\n",
    "\n",
    "output_file = os.path.join(\"./\", 'evaluation_metrics.png')\n",
    "fig.savefig(output_file, bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
