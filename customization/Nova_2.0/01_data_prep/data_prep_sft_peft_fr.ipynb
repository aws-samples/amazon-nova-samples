{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Nova Lite 2.0 Data Preparation (for Model Customization)\n",
    "\n",
    "Preparing high-quality, properly formatted data is a critical first step in the fine-tuning process for large language models. Whether you're using supervised fine-tuning (SFT) or Direct Preference Optimization (DPO), with either full-rank or low-rank adaptation (LoRA) approaches, your data must adhere to specific format requirements to ensure successful model training. This section outlines the necessary data formats, validation methods, and best practices to help you prepare your datasets effectively for fine-tuning Amazon Nova models.\n",
    "\n",
    "Data preparation is the critical process of collecting, cleaning, formatting, and organizing datasets for machine learning model training. It involves transforming raw data into a structured format that models can effectively learn from, ensuring data quality, consistency, and relevance to the target task.\n",
    "\n",
    "In this notebook, the tasks below will take a raw data set and transform it into a training, validation, and eval datasets compatible to Nova 2.0 models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Getting Started\n",
    "For this notebook, we are going to build training, validation, and test datasets to be used with SageMaker Training Jobs (SMTJ) in order to fine tune a Nova Lite 2.0 model, and then evaluate that model.\n",
    "\n",
    "This notebook will use the public [glaiveai/glaive-function-calling-v2](https://huggingface.co/datasets/glaiveai/glaive-function-calling-v2) dataset.  \n",
    "\n",
    "This dataset is an open-source dataset and model suite focused on enabling and improving function calling capabilities for large language models (LLMs). \n",
    "\n",
    "We will use this dataset to train our model, but we must transform this dataset into a schema consumable by SMTJ and Nova."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prerequisites and Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies\n",
    "Several python packages will need to be installed in order to execute this notebook.  Please review the packages in requirements.txt. \n",
    "\n",
    "boto3, sagemaker are required for the training jobs, while the other packages are used to help visualize results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r ./requirements.txt --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credentials, Sessions, Roles, and more!\n",
    "\n",
    "This section sets up the necessary AWS credentials and SageMaker session to run the notebook. You'll need proper IAM permissions to use SageMaker.\n",
    "\n",
    "\n",
    "If you are going to use Sagemaker in a local environment, you will need access to an IAM Role with the required permissions for Sagemaker. Learn more about it here [AWS Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html).\n",
    "\n",
    "For more details on other Nova pre-requisites needed check out [AWS Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/nova-model-general-prerequisites.html)\n",
    "\n",
    "The code initializes a SageMaker session, sets up the IAM role, and configures the S3 bucket for storing training data artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "# we use the SageMaker session as we need to place the training and eval data into a bucket location where SageMaker has access.\n",
    "\n",
    "session_sagemaker = sagemaker.Session(boto_session=boto3.Session(region_name='us-east-1'))\n",
    "\n",
    "sagemaker_session_bucket = None\n",
    "\n",
    "if sagemaker_session_bucket is None and session_sagemaker is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = session_sagemaker.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client(\"iam\")\n",
    "    role = iam.get_role(RoleName=\"sagemaker_execution_role\")[\"Role\"][\"Arn\"]\n",
    "\n",
    "\n",
    "bucket_name = session_sagemaker.default_bucket()\n",
    "default_prefix = session_sagemaker.default_bucket_prefix\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {session_sagemaker.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {session_sagemaker.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation\n",
    "Prepare high-quality prompt-response pairs for training. Data should be:\n",
    "- Consistent in format\n",
    "- Representative of desired behavior\n",
    "- Deduplicated and cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Validation Datasets\n",
    "Our first task will be to prepare the datasets into a required schema to train using SMTJ. \n",
    "\n",
    "That schema is describe below, with a system property and messages property. Both property take arrays.  The messages property use the familar \"user\" -> \"assistant\" turns paradigm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For custom training, each data record must be wrangled into a specific format. Then, all records that make up the training dataset are written to a JSONL file.  Here is the schema that represents a single record.  We call this the `Converse` format.\n",
    "\n",
    "```\n",
    "{\n",
    "    \"system\": [{\"text\": Content of the System prompt}],\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [ { \"text\": Content of the user prompt }]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"reasoningContent\": {\n",
    "                        \"reasoningText\": { \"text\": <reasoning text -> Nova Pro COT AND 2.0 reasoning> }\n",
    "                    }\n",
    "                },\n",
    "                { \"text\": Content of the answer }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "\"reasoningContent\" is optional content.  It is content that explain why the assistant response is the golden response. By adding reasoningContent, models will train better than those without reasoning content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test (Eval) Dataset\n",
    "The schema for an test (evaluation) record is different when using the SageMaker evaluation container.  For SageMaker training jobs that use the evaluation container, the JSON schema looks like this:\n",
    "\n",
    "For reference, here is the schema that represents a single record in the test data.  \n",
    "\n",
    "```\n",
    "{\n",
    "    \"system\": \"\",\n",
    "    \"query\": \"\",\n",
    "    \"response\": \"\"\n",
    "}\n",
    "```\n",
    "\n",
    "Each of these single records are all then aggregated into a single jsonl file, which must be named as `gen_qa.jsonl`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Datasets\n",
    "The data preparation activity within this notebook will\n",
    "- take a data set as input\n",
    "- wrangle the data and transform each record into the above schema\n",
    "- aggreate all of these records into a single JSONL file\n",
    "- write this JSONL file to S3 to be consumed by the customization process as found in the respective technique folder  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Data Loading\n",
    "\n",
    "This code loads the first 10,000 examples from the glaive-function-calling-v2 dataset from Hugging Face. Show a few rows of this data set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"glaiveai/glaive-function-calling-v2\", split=\"train[:1000]\")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the dataset to a pandas DataFrame makes it easier to work with and manipulate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.preprocessing import glaive_to_standard_format\n",
    "\n",
    "processed_dataset = glaive_to_standard_format(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(processed_dataset)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Train/Val/Test Split\n",
    "\n",
    "The dataset is split into training (72%), validation (18%), and test (10%) sets to properly evaluate the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "temp, val = train_test_split(df, test_size=0.1, random_state=42)\n",
    "train, test = train_test_split(temp, test_size=0.01, random_state=42)\n",
    "\n",
    "print(\"Number of train elements: \", len(train))\n",
    "print(\"Number of test elements: \", len(val))\n",
    "print(\"Number of val elements: \", len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3: Data Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall, our objective is to transform each record of the training data into the schema required for SMTJ and the Amazon Nova Lite 2.0 model.\n",
    "\n",
    "Once, the schema is accomplished for each record, then all transformed records will be written into a single JSONL file. \n",
    "\n",
    "As a reminder, here is the target schema for the training and validation data:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"system\": [{ \"text\": Content of the System prompt }],\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{ \"text\": Content of the user prompt }]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [{ \"text\": Content of the answer }]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help achieve this transformation goal, the notebook defines utility functions to clean the dataset content by removing prefixes and handling special cases:\n",
    "\n",
    "```python\n",
    "def clean_prefix(content):\n",
    "    # Removes prefixes like \"USER:\", \"ASSISTANT:\", etc.\n",
    "    ...\n",
    "\n",
    "def clean_message_list(message_list):\n",
    "    # Cleans message lists from None values and converts to proper format\n",
    "    ...\n",
    "\n",
    "def clean_numbered_conversation(message_list):\n",
    "    # Cleans message lists from None values and converts to proper format\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions transform the dataset into the format required by Nova models, handling tool calls and formatting:\n",
    "\n",
    "```python\n",
    "\n",
    "def transform_tool_format(tool):\n",
    "    # Transforms tool format to Nova's expected format\n",
    "    ...\n",
    "\n",
    "def prepare_dataset(sample):\n",
    "    # Prepares dataset in the required format for Nova models\n",
    "    ...\n",
    "\n",
    "def prepare_dataset_test(sample):\n",
    "    # Formats validation dataset for evaluation\n",
    "    ...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4: Wrangle data set into Converse Format\n",
    "Create Train, Validation, and Test Datasets\n",
    "\n",
    "- Train data is data that the model will train upon\n",
    "- Eval data is data that used to measure metrics of the trained model\n",
    "- Test data is data that the model has never seen nor been trained on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from random import randint\n",
    "from utils.data_prep_utils import (\n",
    "    prepare_dataset,\n",
    "    clean_message_list,\n",
    "    prepare_dataset_test,\n",
    "    make_eval_compatible\n",
    ")\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train)\n",
    "val_dataset = Dataset.from_pandas(val)\n",
    "intermediate_test_dataset = Dataset.from_pandas(test)\n",
    "\n",
    "\n",
    "dataset = DatasetDict(\n",
    "    {\"train\": train_dataset, \"test\": intermediate_test_dataset, \"val\": val_dataset}\n",
    ")\n",
    "\n",
    "train_dataset = dataset[\"train\"].map(\n",
    "    prepare_dataset, remove_columns=train_dataset.features\n",
    ")\n",
    "\n",
    "train_dataset = train_dataset.to_pandas()\n",
    "\n",
    "train_dataset[\"messages\"] = train_dataset[\"messages\"].apply(clean_message_list)\n",
    "\n",
    "print(train_dataset.iloc[randint(0, len(train_dataset))].to_json())\n",
    "\n",
    "val_dataset = dataset[\"val\"].map(prepare_dataset, remove_columns=val_dataset.features)\n",
    "\n",
    "val_dataset = val_dataset.to_pandas()\n",
    "\n",
    "val_dataset[\"messages\"] = val_dataset[\"messages\"].apply(clean_message_list)\n",
    "\n",
    "print(val_dataset.iloc[randint(0, len(val_dataset))].to_json())\n",
    "\n",
    "intermediate_test_dataset = dataset[\"test\"].map(\n",
    "    prepare_dataset_test, remove_columns=intermediate_test_dataset.features\n",
    ")\n",
    "\n",
    "\n",
    "# Transform test_dataset into Evaluation compatible\n",
    "# test_dataset looks like this:\n",
    "\n",
    "# {\"messages\":[{\"query\":\"1\",\"response\":\"\",\"system\":\"\"}]}\n",
    "# {\"messages\":[{\"query\":\"2\",\"response\":\"\",\"system\":\"\"}]}\n",
    "# {\"messages\":[{\"query\":\"3\",\"response\":\"\",\"system\":\"\"}]}\n",
    "\n",
    "# and we transform it into this for Evaluation\n",
    "\n",
    "# {\"query\":\"1\",\"response\":\"\",\"system\":\"\"}\n",
    "# {\"query\":\"2\",\"response\":\"\",\"system\":\"\"}\n",
    "# {\"query\":\"3\",\"response\":\"\",\"system\":\"\"}\n",
    "\n",
    "test_data = make_eval_compatible(intermediate_test_dataset)\n",
    "test_dataset = Dataset.from_list(make_eval_compatible(intermediate_test_dataset))\n",
    "\n",
    "print(test_dataset[randint(0, len(test_dataset))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5: Upload all 3 curated datasets (train, val, test) to Amazon S3\n",
    "\n",
    "The processed datasets are saved locally and then uploaded to Amazon S3 for use in SageMaker training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save files locally for review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Save datasets to s3\n",
    "folder = \"tmp\"\n",
    "os.makedirs(f\"./{folder}/train\", exist_ok=True)\n",
    "os.makedirs(f\"./{folder}/val\", exist_ok=True)\n",
    "os.makedirs(f\"./{folder}/test\", exist_ok=True)\n",
    "\n",
    "train_dataset.to_json(f\"./{folder}/train/dataset.jsonl\", orient=\"records\", lines=True)\n",
    "val_dataset.to_json(f\"./{folder}/val/dataset.jsonl\", orient=\"records\", lines=True)\n",
    "test_dataset.to_json(f\"./{folder}/test/gen_qa.jsonl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload Datasets to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the S3 bucket paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# save train_dataset to s3 using our SageMaker session\n",
    "if default_prefix:\n",
    "    input_path = f\"{default_prefix}/datasets/nova-sft-peft\"\n",
    "else:\n",
    "    input_path = f\"datasets/nova-sft-peft\"\n",
    "\n",
    "train_dataset_s3_path = f\"s3://{bucket_name}/{input_path}/train/dataset.jsonl\"\n",
    "val_dataset_s3_path = f\"s3://{bucket_name}/{input_path}/val/dataset.jsonl\"\n",
    "test_dataset_s3_path = f\"s3://{bucket_name}/{input_path}/test/gen_qa.jsonl\"\n",
    "\n",
    "print(f\"Training data uploaded to:\")\n",
    "print(train_dataset_s3_path)\n",
    "\n",
    "print(f\"\\nValidation data uploaded to:\")\n",
    "print(val_dataset_s3_path)\n",
    "\n",
    "print(f\"\\nTest data uploaded to:\")\n",
    "print(test_dataset_s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Push to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "s3_client.upload_file(\n",
    "    f\"./{folder}/train/dataset.jsonl\", bucket_name, f\"{input_path}/train/dataset.jsonl\"\n",
    ")\n",
    "\n",
    "s3_client.upload_file(\n",
    "    f\"./{folder}/val/dataset.jsonl\", bucket_name, f\"{input_path}/val/dataset.jsonl\"\n",
    ")\n",
    "\n",
    "s3_client.upload_file(\n",
    "    f\"./{folder}/test/gen_qa.jsonl\", bucket_name, f\"{input_path}/test/gen_qa.jsonl\"\n",
    ")\n",
    "\n",
    "# comment the line of code below if would like to see data files locally.\n",
    "# shutil.rmtree(f\"./{folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training and Customization\n",
    "\n",
    "Excellent!  All done with the Date Preparation!\n",
    "\n",
    "Now, it is time to train the model.  Please go to the SFT notebook in order to take the next steps on this journey.\n",
    "\n",
    "**---------- BEFORE YOU GO!! ----------**<br><br>\n",
    "The below values are needed for the customization notebook.  Copy these values as they are needed in the SFT notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training data uploaded to:\")\n",
    "print(train_dataset_s3_path)\n",
    "\n",
    "print(f\"\\nValidation data uploaded to:\")\n",
    "print(val_dataset_s3_path)\n",
    "\n",
    "print(f\"\\nTest data uploaded to:\")\n",
    "print(test_dataset_s3_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'train_dataset_s3_path' (str)\n",
      "Stored 'val_dataset_s3_path' (str)\n",
      "Stored 'test_dataset_s3_path' (str)\n"
     ]
    }
   ],
   "source": [
    "%store train_dataset_s3_path\n",
    "%store val_dataset_s3_path\n",
    "%store test_dataset_s3_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Deployment / Inference\n",
    "\n",
    "Deployment and Inference are found in a different notebook.  Please see the Deployment notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
