{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Nova 2 Lite Fine Tuned Model Evaluation using Amazon SageMaker Training Jobs\n",
    "\n",
    "The purpose of the evaluation process is to assess trained-model performance against benchmarks or custom dataset. The evaluation process typically involves steps to create evaluation recipe pointing to the trained model, specify evaluation datasets and metrics, submit a separate job for the evaluation, and evaluate against standard benchmarks or custom data. The evaluation process will output performance metrics stored in your Amazon S3 bucket.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Getting Started\n",
    "Amazon Nova provides four different types of evaluation recipes. A recipe is a yaml file that configures a SageMaker job to perform the specified tasks.  Recipes are the only mechanism to configure the respective SageMaker job.\n",
    "\n",
    "All recipes are available in the Amazon SageMaker HyperPod recipes GitHub repository. Though located in the HyperPod repository, recipes can be used with SageMaker Training Jobs (SMTJ) as well as SageMaker HyperPod. \n",
    "\n",
    "For Amazon Nova, SageMaker recipes include:\n",
    "- General text benchmark recipes\n",
    "- General multi-modal benchmark recipes\n",
    "- Bring your own dataset benchmark recipes\n",
    "- Nova LLM as a Judge benchmark recipes\n",
    "\n",
    "\n",
    "Recipes are organized by model, technique, and GPU. A full list of recipes can be found in the Nova recipe reference material.\n",
    "[Amazon Nova recipes](https://docs.aws.amazon.com/sagemaker/latest/dg/nova-model-recipes.html)\n",
    "\n",
    "The GitHub repository of all recipes:\n",
    "\n",
    "[SageMaker HyperPod recipes GitHub](https://github.com/aws/sagemaker-hyperpod-recipes?tab=readme-ov-file#fine-tuning) **\n",
    "\n",
    "** For Nova specific recipes, see the folder `recipes_collection -> recipes -> evaluation/nova`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dependencies and Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies\n",
    "Several python packages will need to be installed in order to execute this notebook.  Please review the packages in requirements.txt. \n",
    "\n",
    "botocore, boto3, sagemaker are required for the training jobs, while the other packages are used to help visualize results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r ./requirements.txt --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisite: Data Prep Notebook\n",
    "The Data Prep notebook walks through preparing and transformming a public data into a format and scheme acceptable for SMTJ.  The Data Prep notebook creates training and validation datasets used for training a model, as well as a test dataset for evaluation.\n",
    "\n",
    "This note book will use the `test` dataset as describe below.\n",
    "\n",
    "### Prerequisite: SFT Notebook\n",
    "The SFT notebook walks through training a custom model. As a result of that training, output model artifacts are created.  We will use those output artificate to eval the model using SMTJ. \n",
    "\n",
    "**--------------- STOP ---------------** <br><br>To complete this notebook, both the Data Prep notebook and SFT notebook must be completed first. Those notebooks create the necessary datasets, as well as model artifacts.  Specific items that are carried over from those notebooks, to this notebook, are called out below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This value is obtained as result of executing the data prep notebook\n",
    "test_dataset_s3_path = \"\"\n",
    "%store -r test_dataset_s3_path \n",
    "\n",
    "print(test_dataset_s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credentials, Sessions, Roles, and more!\n",
    "\n",
    "This section sets up the necessary AWS credentials and SageMaker session to run the notebook. You'll need proper IAM permissions to use SageMaker.\n",
    "\n",
    "\n",
    "If you are going to use Sagemaker in a local environment, you will need access to an IAM Role with the required permissions for Sagemaker. Learn more about it here [AWS Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html).\n",
    "\n",
    "For more details on other Nova pre-requisites needed check out [AWS Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/nova-model-general-prerequisites.html)\n",
    "\n",
    "The code initializes a SageMaker session, sets up the IAM role, and configures the S3 bucket for storing training data artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sagemaker_session = sagemaker.Session(boto_session=boto3.Session(region_name='us-east-1'))\n",
    "\n",
    "sagemaker_session_bucket = None\n",
    "\n",
    "if sagemaker_session_bucket is None and sagemaker_session is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "\n",
    "except ValueError:\n",
    "    iam = boto3.client(\"iam\")\n",
    "    role = iam.get_role(RoleName=\"sagemaker_execution_role\")[\"Role\"][\"Arn\"]\n",
    "\n",
    "\n",
    "bucket_name = sagemaker_session.default_bucket()\n",
    "default_prefix = sagemaker_session.default_bucket_prefix\n",
    "\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sagemaker_session.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sagemaker_session.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capture S3 bucket prefix for later use.  After a SageMaker Training Job completes, this value will be used to identify where the SMTJ outputs are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if default_prefix:\n",
    "    output_path_prefix = f\"s3://{bucket_name}/{default_prefix}\"\n",
    "\n",
    "else:\n",
    "    output_path_prefix= f\"s3://{bucket_name}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Prep - Review\n",
    "In the data prep workbook, we created our training, validation, and test datasets.  We will use the `test` dataset created for evaluation.  If the data prep notebook has not been completed, do so now so that these datasets and s3 locations are available to complete this notebook.\n",
    "\n",
    "The test data is found in the file `gen_qa.jsonl` and for the eval SMTJ this is the required name.\n",
    "\n",
    "Remember, prepare high-quality prompt-response pairs for training. Data should be:\n",
    "- Consistent in format\n",
    "- Representative of desired behavior\n",
    "- Deduplicated and cleaned\n",
    "\n",
    "\n",
    "For reference, here is the schema that represents a single record in the test data.  \n",
    "\n",
    "```\n",
    "{\n",
    "    \"system\": \"\",\n",
    "    \"query\": \"\",\n",
    "    \"response\": \"\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation\n",
    "Bedfore starting, a SageMaker AI-trained Amazon Nova model is needed, for which you want to evaluate its performance.\n",
    "\n",
    "To evaluate a model, the SageMaker Training Job (SMTJ) uses a PyTorch estimator to run the customization.  The estimator defines properties of the training job, such as training job name, instance type, instance count, output location for job results, training recipe, and more.\n",
    "\n",
    "More details of the estimator can be found here: [PyTorch Estimator](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html)\n",
    "\n",
    "So we must define\n",
    "- the training recipe\n",
    "- properties for the estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recipes\n",
    "A Nova recipe is a YAML configuration file that provides details to SageMaker AI on how to run your model evaluation job. It provides the model name or checkpoint, sets training hyperparameters, defines optimization settings, and includes any additional options required to eval the model successfully.\n",
    "\n",
    "These recipe files serve as the blueprint for your model customization jobs, allowing you to specify training parameters, hyperparameters, and other critical settings that determine how your model learns from your data.\n",
    "\n",
    "Evals offer specific tasks that can be accomplished:\n",
    "- General text benchmark recipes\n",
    "- General multi-modal benchmark recipes\n",
    "- Bring your own dataset benchmark recipes (BYOD)\n",
    "- Nova LLM as a Judge benchmark recipes (BYOM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When defining a recipe, there are key components to modify for your uses case.  See this link for details - [Evaluation specific configurations](https://docs.aws.amazon.com/sagemaker/latest/dg/nova-model-evaluation.html#nova-model-evaluation-config)\n",
    "\n",
    "The recipes can be found at:\n",
    "- [Amazon Nova recipes](https://docs.aws.amazon.com/sagemaker/latest/dg/nova-model-recipes.html), or \n",
    "- [GitHub - sagemaker-hyperpod-recipes](https://github.com/aws/sagemaker-hyperpod-recipes). Navigate to `recipes_collection -> recipes -> evaluation -> nova -> nova_2_0/nova_lite`  to find all Nova 2 Lite evaluation recipes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at some text eval recipe examples: General Text Benchmarks, Bring Your Own Dataset, and Bring Your Own Metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recipe Organization in Notebook \n",
    "The organization of this notebook is to illustrate multiple recipes (for example, BYOD or BYOM). \n",
    "\n",
    "Please review the link above, \"GitHub - sagemaker-hyperpod-recipes\", to learn of more about the properties of a recipe. \n",
    "\n",
    "Recipes are not executed sequentially.\n",
    "\n",
    "In order to execute a distinct recipe, a flag in python code is set futher below in the notebook.  Then, when the notebook is executed, the selected recipe is configured for the SMTJ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: General Text Benchmarks\n",
    "These recipes enable you to evaluate the fundamental capabilities of Amazon Nova models across a comprehensive suite of text-only benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtb_recipe = \"evaluation/nova/nova_2_0/nova_lite/nova_lite_2_0_p5_48xl_gpu_general_text_benchmark_eval\"\n",
    "gtb_recipe_job_name = \"eval-gtb-nova-lite-2-recipe-job\" # example\n",
    "gtb_sm_job_name = \"eval-gtb-nova-lite-2\"\n",
    "\n",
    "# this can also be a checkpoint URI of a trained model\n",
    "gtb_peft_model_name_or_path = \"nova-lite-2/prod\" # example\n",
    "\n",
    "\n",
    "gtb_recipe_overrides = {\n",
    "    \"run\": {\n",
    "        \"name\": gtb_recipe_job_name,\n",
    "        \"model_name_or_path\": gtb_peft_model_name_or_path,\n",
    "        \"data_s3_path\": \"\",  # For SMTJ, this is \"\"\n",
    "        \"output_s3_path\": f\"{output_path_prefix}/{gtb_sm_job_name}\"\n",
    "    },\n",
    "    \"processor\": {\n",
    "        \"lambda_arn\": \"\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Bring Your Own Dataset Recipe (BYOD)\n",
    "This recipe enable you to bring your own dataset for benchmarking and compare model outputs to reference answers using different types of metrics (BYOM, explained further below).\n",
    "\n",
    "Note, as our goal is to eval against a trained checkpoint, the model_name_or_path identifies that checkpoint.\n",
    "\n",
    "When a model is trained using SMTJ, the output from that job includes a manifest.json.  In that manifest.json is a property, \"checkpoint_s3_bucket\".  This property holds the value that will be used for the model_name_or_path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "byod_recipe = \"evaluation/nova/nova_2_0/nova_lite/nova_lite_2_0_p5_48xl_gpu_bring_your_own_dataset_eval\"\n",
    "byod_recipe_job_name = \"eval-byod-nova-lite-2-recipe-job\" # example\n",
    "byod_sm_job_name = \"eval-byod-nova-lite-2\"\n",
    "\n",
    "# this can also be a checkpoint URI of a trained model\n",
    "byod_peft_model_name_or_path = \"nova-lite-2/prod\" # example\n",
    "\n",
    "\n",
    "byod_recipe_overrides = {\n",
    "    \"run\": {\n",
    "        \"name\": byod_recipe_job_name,\n",
    "        \"model_name_or_path\": byod_peft_model_name_or_path,\n",
    "        \"data_s3_path\": \"\",  # For SMTJ, this is \"\"\n",
    "        \"output_s3_path\": f\"{output_path_prefix}/{byod_sm_job_name}\"\n",
    "    },\n",
    "    \"processor\": {\n",
    "        \"lambda_arn\": \"\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bring Your Own Metric Recipe\n",
    "These recipes enable you to bring your own dataset for benchmarking and compare model outputs to reference answers using different types of metrics.\n",
    "\n",
    "When a model is trained using SMTJ, the output from that job includes a manifest.json.  In that manifest.json is a property, \"checkpoint_s3_bucket\".  This property holds the value that will be used for the model_name_or_path.  checkpoint_s3_bucket is the source of truth of for the location of the trained model checkpoint.\n",
    "\n",
    "Note, as our goal is to eval against a trained checkpoint, the `model_name_or_path` hold the value of the checkpoint location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lambda: Preprocessing, Postprocessing\n",
    "Bring your own metrics is unique in that a lambda is executed on each data item - for preprocessing or postprocessing of the \n",
    "eval data.\n",
    "\n",
    "Preprocessing is used to augment, transform, or apply processing on a data item before the model is prompted with that data item.\n",
    "\n",
    "Postprocessing is used augment, transform, calculate a metric, or apply processing after a data item is returned from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lambda: Prerequisites\n",
    "There are additional steps required to execute lambda (appropriate permission issues notwithstanding). For details, see [Evaluating your SageMaker AI-trained model](https://docs.aws.amazon.com/sagemaker/latest/dg/nova-model-evaluation.html)\n",
    "\n",
    "Essentially, there are two layers that must be added to your lambda function\n",
    "- nova-custom-eval-layer.zip - open-source Nova custom evaluation SDK to validate input and output payloads for your custom function\n",
    "- AWSLambdaPowertoolsPythonV3-python312-arm64 - required for pydantic dependency\n",
    "\n",
    "The lambda function can run on x86 or ARM architecture.\n",
    "\n",
    "An example of the lambda is provided in the example folder (custom-eval-metric.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this can also be a checkpoint URI of a trained model\n",
    "byom_recipe = \"evaluation/nova/nova_2_0/nova_lite/nova_lite_2_0_p5_48xl_gpu_bring_your_own_dataset_eval\"\n",
    "byom_recipe_job_name = \"eval-byom-nova-lite-2-recipe-job\" # example\n",
    "byom_sm_job_name = \"eval-byom-nova-lite-2\"\n",
    "\n",
    "# this can also be a checkpoint URI of a trained model\n",
    "byom_peft_model_name_or_path = \"nova-lite-2/prod\" # example\n",
    "\n",
    "lambda_arn = \"arn:aws:lambda:us-east-1:111111111111:function:custom-metric-eval\"\n",
    "\n",
    "# See Code Comments immediately following this\n",
    "byom_recipe_overrides = {\n",
    "    \"run\": {\n",
    "        \"name\": byom_recipe_job_name,\n",
    "        \"model_name_or_path\": byom_peft_model_name_or_path,\n",
    "        \"data_s3_path\": \"\",  # For SMTJ, this is \"\"\n",
    "        \"output_s3_path\": f\"{output_path_prefix}/{byom_sm_job_name}\"\n",
    "    },\n",
    "    \"processor\": {\n",
    "        \"lambda_arn\": f\"{lambda_arn}\",\n",
    "        \"lambda_type\": \"custom_metrics\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code Comments\n",
    "Take a look at the processor section.  As of the writting of the notebook, the base, unmodified github recipe sets this configuration:\n",
    "\n",
    "```\n",
    "processor:\n",
    "  preset_reward_function: \"\" # Do not modify\n",
    "  lambda_arn: \"\" # Provide valid lambda arn to enable custom metrics processing\n",
    "  lambda_type: \"rft\" # or \"custom_metrics\"\n",
    "  preprocessing:\n",
    "    enabled: false\n",
    "  postprocessing:\n",
    "    enabled: true\n",
    "  aggregation: average\n",
    "```\n",
    "\n",
    "So when this eval job executes, it will not call preprocessing (enabled: false), but it will call postprocess (enabled: true).  This \"experiment\" is done by design to illustrate overriding values, or keeping values as is, to see outcomes and to be mindful of what needs to be overridden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select training technique to use\n",
    "This is just a helper that allows chosing of whichever recipe desired.  Change the value of `technique` to one of the keywords indicated in the comment.\n",
    "\n",
    "This will allow running the notebook efficiently.  Change the key, run the cells.  Change the keyword to another technique, run the cells.  Easy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# technique choices of \"GTB\" or \"BYOD\" OR \"BYOM\"\n",
    "technique = \"BYOM\" \n",
    "\n",
    "# recipe = \"\"\n",
    "sm_training_job_name = \"\"\n",
    "recipe_job_name = \"\"\n",
    "training_recipe = \"\"\n",
    "recipe_overrides = {}\n",
    "\n",
    "if technique == \"GTB\":\n",
    "    # General Text Benchmark\n",
    "    print(\"General Text Benchmark\")\n",
    "    training_recipe = gtb_recipe\n",
    "    recipe_overrides = gtb_recipe_overrides\n",
    "    sm_training_job_name = gtb_sm_job_name\n",
    "    recipe_job_name = gtb_recipe_job_name\n",
    "    use_training_input_channel = False\n",
    "elif technique == \"BYOD\":\n",
    "    # Bring your own dataset\n",
    "    print(\"Bring your own dataset\")\n",
    "    training_recipe = byod_recipe\n",
    "    recipe_overrides = byod_recipe_overrides\n",
    "    sm_training_job_name = byod_sm_job_name\n",
    "    recipe_job_name = byod_recipe_job_name\n",
    "    use_training_input_channel = True\n",
    "elif technique == \"BYOM\":\n",
    "    print(\"Bring your own metric\")\n",
    "    # Bring your own metric\n",
    "    training_recipe = byom_recipe\n",
    "    recipe_overrides = byom_recipe_overrides\n",
    "    sm_training_job_name = byom_sm_job_name\n",
    "    recipe_job_name = byom_recipe_job_name\n",
    "    use_training_input_channel = True\n",
    "else:\n",
    "    print(\"*** Issue - evaluation undefined ***\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instance Type and Count\n",
    "\n",
    "P5 instances are optimized for deep learning workloads, providing high-performance GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = \"ml.p5.48xlarge\" \n",
    "instance_count = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Container Image URI\n",
    "This specifies the pre-built container for eval of Nova 2 Lite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri = \"708977205387.dkr.ecr.us-east-1.amazonaws.com/nova-evaluation-repo:SM-TJ-Eval-Beta-latest\"\n",
    "\n",
    "print(f\"image_uri: \\n{image_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output path\n",
    "This path will be used to write model results of the model training.  In this location will be found an output.tar.gz file, containing the `manifest.json` and `step_wise_training_metrics.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "print(f\"recipe_overrides: \\n{json.dumps(recipe_overrides, indent=4)}\\n\")\n",
    "\n",
    "output_s3_path = recipe_overrides[\"run\"][\"output_s3_path\"]\n",
    "\n",
    "print(f\"output_path:\\n{output_s3_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "eval_estimator = PyTorch(\n",
    "    output_path=output_s3_path,\n",
    "    base_job_name=sm_training_job_name,\n",
    "    role=role,\n",
    "    disable_profiler=True,\n",
    "    debugger_hook_config=False,\n",
    "    instance_count=instance_count,\n",
    "    instance_type=instance_type,\n",
    "    training_recipe=training_recipe,\n",
    "    recipe_overrides=recipe_overrides,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    image_uri=image_uri,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuring the Data Channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "eval_input = TrainingInput(\n",
    "    s3_data=test_dataset_s3_path,\n",
    "    distribution=\"FullyReplicated\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start the Training Job\n",
    "This starts the training job with the configured estimator and datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting the train job with our uploaded datasets as input\n",
    "\n",
    "if use_training_input_channel:\n",
    "    print(\"using data input channel\")\n",
    "    eval_estimator.fit(inputs={\"train\": eval_input}, wait=False)\n",
    "else:\n",
    "    print(\"NOT using data input channel\")\n",
    "    eval_estimator.fit(wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_job_name = eval_estimator.latest_training_job.name\n",
    "\n",
    "print(f'Training Job Name:  \\n{training_job_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, Markdown, Image\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region={}#/jobs/{}\">Training Job</a> After About 5 Minutes</b>'.format(\"us-east-1\", training_job_name)))\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/cloudwatch/home?region={}#logStream:group=/aws/sagemaker/TrainingJobs;prefix={};streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After About 5 Minutes</b>'.format(\"us-east-1\", training_job_name)))\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://s3.console.aws.amazon.com/s3/buckets/{}/{}/?region={}&tab=overview\">S3 Output Data</a> After The Training Job Has Completed</b>'.format(bucket_name, training_job_name, \"us-east-1\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ^^ _This will take 20-30 mins in evaluation_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation Artifacts \n",
    "Downloading the artifact from Evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_output = eval_estimator.model_data\n",
    "\n",
    "print(f\"estimator_output: \\n{estimator_output}\\n\")\n",
    "\n",
    "output = '/'.join(estimator_output.split(\"/\")[:-1]) +\"/output.tar.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define consistent local folder names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"tmp\"\n",
    "output_folder = \"eval_output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"output: \\n{output}\\n\")\n",
    "print(f\"folder: \\n{folder}\\n\")\n",
    "print(f\"output_folder: \\n{output_folder}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ./$folder/\n",
    "!mkdir -p ./$folder/$output_folder/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp $output ./$folder/$output_folder/$training_job_name/output.tar.gz\n",
    "!tar -xvzf ./$folder/$output_folder/$training_job_name/output.tar.gz -C ./$folder/$output_folder/$training_job_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the metrics\n",
    "Define a function to plot the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "def plot_metrics(results):\n",
    "    # Extract metrics and their standard errors\n",
    "    metrics = {}\n",
    "    for key, value in results.items():\n",
    "        if not key.endswith(\"_stderr\"):\n",
    "            metrics[key] = {\"value\": value, \"stderr\": results.get(f\"{key}_stderr\", 0)}\n",
    "\n",
    "    # Sort metrics by value for better visualization\n",
    "    sorted_metrics = dict(\n",
    "        sorted(metrics.items(), key=lambda x: x[1][\"value\"], reverse=True)\n",
    "    )\n",
    "\n",
    "    # Prepare data for plotting\n",
    "    labels = list(sorted_metrics.keys())\n",
    "    values = [sorted_metrics[label][\"value\"] for label in labels]\n",
    "    errors = [sorted_metrics[label][\"stderr\"] for label in labels]\n",
    "\n",
    "    # Normalize BLEU score to be on the same scale as other metrics (0-1)\n",
    "    bleu_index = labels.index(\"bleu\") if \"bleu\" in labels else -1\n",
    "    if bleu_index >= 0:\n",
    "        values[bleu_index] /= 100\n",
    "        errors[bleu_index] /= 100\n",
    "\n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # Create bar chart\n",
    "    x = np.arange(len(labels))\n",
    "    bars = ax.bar(\n",
    "        x,\n",
    "        values,\n",
    "        yerr=errors,\n",
    "        align=\"center\",\n",
    "        alpha=0.7,\n",
    "        capsize=5,\n",
    "        color=\"skyblue\",\n",
    "        ecolor=\"black\",\n",
    "    )\n",
    "\n",
    "    # Add labels and title\n",
    "    ax.set_ylabel(\"Score\")\n",
    "    ax.set_title(\"Evaluation Metrics\")\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels, rotation=45, ha=\"right\")\n",
    "    ax.set_ylim(0, 1.0)\n",
    "\n",
    "    # Add value labels on top of bars\n",
    "    for i, bar in enumerate(bars):\n",
    "        height = bar.get_height()\n",
    "        # Convert BLEU back to its original scale for display\n",
    "        display_value = values[i] * 100 if labels[i] == \"bleu\" else values[i]\n",
    "        ax.text(\n",
    "            bar.get_x() + bar.get_width() / 2.0,\n",
    "            height + 0.01,\n",
    "            f\"{display_value:.2f}\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "        )\n",
    "\n",
    "    # Add a note about BLEU\n",
    "    if bleu_index >= 0:\n",
    "        ax.text(\n",
    "            0.5,\n",
    "            -0.15,\n",
    "            \"Note: BLEU score shown as percentage (original: {:.2f})\".format(\n",
    "                values[bleu_index] * 100\n",
    "            ),\n",
    "            transform=ax.transAxes,\n",
    "            ha=\"center\",\n",
    "            fontsize=9,\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a eval job completes, an output.tar.gz is created.  Above, that file is unzipped, and untarred.\n",
    "\n",
    "These results in a folder structure:\n",
    "\n",
    "training-job-name<br>\n",
    "|--> recipe-job-name<br>\n",
    "|--> --> eval_results<br>\n",
    "|--> --> tensorboard_results<br>\n",
    "\n",
    "Take a moment to analyze the folders and content.  In the eval_results folder is a JSON file (results_xxx.json) and JSONL file (inference_output.jsonl)\n",
    "\n",
    "The file results_xxx.json will contain metric results.\n",
    "\n",
    "The file inference_output.jsonl will contain all the model inference results of the input eval dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = f\"{folder}/{output_folder}/{training_job_name}/{recipe_job_name}/eval_results\"\n",
    "\n",
    "print(f\"results_path: \\n{results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to find all json files in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "def find_json_files(path):\n",
    "    return glob.glob(os.path.join(path, \"*.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results_path = find_json_files(results_path)\n",
    "evaluation_results_path = find_json_files(results_path)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the metrics and write the output plot to local storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "with open(evaluation_results_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "fig = plot_metrics(data[\"results\"][\"all\"])\n",
    "training_job_path = f\"./{folder}/{output_folder}/{training_job_name}\"\n",
    "os.makedirs(training_job_path, exist_ok=True)\n",
    "\n",
    "output_file = os.path.join(training_job_path, 'evaluation_metrics.png')\n",
    "\n",
    "fig.savefig(output_file, bbox_inches='tight')\n",
    "\n",
    "print(training_job_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Exploring More Eval Use Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval BYOM Lambda (Deeper) Dive\n",
    "Always curious - it is good to gain an understanding of the event dictionary as input into the preprocessing and postprocessing methods of the lamba.  Also, it would be good to understand the resulting response of the lamba.\n",
    "\n",
    "Use the custom-metric-eval.py lambda, here are example of the 3 afore mentioned data points.\n",
    "\n",
    "- preprocess.json\n",
    "- postprocess.json\n",
    "- result.json\n",
    "\n",
    "Note:  For preprocessing, the lambda can affect the values contained in system, prompt, or gold.  No other additional properties can be added, or removed.\n",
    "\n",
    "The result object of the lambda is dynamic, and can return a variety of information and metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval SMTJ for Batch Inference\n",
    "An interesting observation and feature - an eval SMTJ can also be used to batch inference prompts.  In that case, the model_name_or_path would reflect the base model, not the checkpoint model.   As such, inference can be done across the test dataset.  \n",
    "\n",
    "This would be helpful in a use case such as obtaining a model's reasoning for a given `query` and `response`, any using that reasoning to repopulate an SFT training job (an SFT training schema a reasonContent block for the purpose of improving training by using reasoning inputs)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
