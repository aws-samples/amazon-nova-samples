{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Nova Rubric Based LLM-as-a-Judge Evaluation with Amazon SageMaker\n",
    "\n",
    "This notebook demonstrates how to use Amazon Nova's rubric-based LLM-as-a-Judge methodology to evaluate and compare the outputs of two different large language models using Amazon SageMaker Training Jobs. We'll compare responses from a **Qwen2.5 1.5B Instruct model (Model A)** against a **Qwen2.5 7B Instruct model (Model B)**, both deployed on SageMaker AI.\n",
    "\n",
    "**Overview**\n",
    "\n",
    "The Amazon Nova rubric-based LLM-as-a-Judge approach uses a powerful language model to evaluate the quality of responses from other models by dynamically generating custom evaluation rubrics for each comparison. This method provides:\n",
    "\n",
    "**Objective Comparison**: Systematic evaluation using automatically generated, context-specific rubrics with weighted criteria (accuracy, completeness, clarity, etc.)\n",
    "\n",
    "**Scalable Assessment**: Automated rubric generation and criterion-based scoring across large datasets without manual rubric design\n",
    "\n",
    "**Detailed Metrics**: Win rates, confidence intervals, preference distributions, weighted scores per criterion, and detailed justifications for each evaluation dimension\n",
    "\n",
    "**Cost-Effective**: More efficient than human evaluation for large-scale comparisons while maintaining evaluation transparency through explicit rubric criteria\n",
    "\n",
    "**Adaptive Evaluation**: Rubrics are tailored to each specific question-answer pair, ensuring relevant and context-appropriate assessment criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T14:20:00.576414Z",
     "iopub.status.busy": "2025-06-27T14:20:00.576104Z",
     "iopub.status.idle": "2025-06-27T14:20:00.582402Z",
     "shell.execute_reply": "2025-06-27T14:20:00.581558Z",
     "shell.execute_reply.started": "2025-06-27T14:20:00.576390Z"
    }
   },
   "source": [
    "## Prerequisites\n",
    "\n",
    "* AWS Account with SageMaker and Bedrock access\n",
    "* Appropriate IAM roles and permissions\n",
    "* SageMaker Studio or Jupyter environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T15:22:29.107240Z",
     "iopub.status.busy": "2025-06-27T15:22:29.106656Z",
     "iopub.status.idle": "2025-06-27T15:22:29.114508Z",
     "shell.execute_reply": "2025-06-27T15:22:29.113484Z",
     "shell.execute_reply.started": "2025-06-27T15:22:29.107207Z"
    }
   },
   "source": [
    "## Understanding Amazon Nova LLM-as-a-Judge Evaluation Metrics \n",
    "\n",
    "When using the Amazon Nova LLM-as-a-Judge framework to compare the outputs of two language models, a set of quantitative metrics is generated. These metrics help you objectively assess which model performs better and how reliable the evaluation is.\n",
    "\n",
    "---\n",
    "\n",
    "### Core Preference Metrics\n",
    "\n",
    "- **a_scores**  \n",
    "  The number of times Model A's response was preferred by the judge model over Model B.\n",
    "\n",
    "- **b_scores**  \n",
    "  The number of times Model B's response was preferred by the judge model over Model A.\n",
    "\n",
    "- **ties**  \n",
    "  The number of times the judge found both responses equally good or could not determine a preference.\n",
    "\n",
    "- **inference_error**  \n",
    "  The number of evaluation cases where the judge could not provide a valid judgment due to technical issues, malformed outputs, or other errors.\n",
    "\n",
    "---\n",
    "\n",
    "### Rubric-Specific Metrics\n",
    "- **weighted_a / weighted_b** Aggregate weighted scores for each model calculated by combining criterion-specific scores multiplied by their respective weights. These provide a more nuanced performance measure than simple win counts.\n",
    "\n",
    "- **margin** The difference between weighted scores (typically weighted_b - weighted_a), indicating the magnitude of preference. Negative margins favor Model B, positive margins favor Model A.\n",
    "\n",
    "- **criteria_breakdown** Detailed performance across individual rubric dimensions (e.g., accuracy: 0.7 weight, completeness: 0.2 weight, clarity: 0.1 weight) with justifications for each criterion.\n",
    "\n",
    "---\n",
    "### Statistical Confidence Metrics\n",
    "\n",
    "- **winrate**  \n",
    "  The proportion of valid judgments in which Model B was preferred\n",
    "\n",
    "- **lower_rate**  \n",
    "  The lower bound of the 95% confidence interval for the winrate. This tells you the minimum likely winrate for Model B, accounting for statistical uncertainty.\n",
    "\n",
    "- **upper_rate**  \n",
    "  The upper bound of the 95% confidence interval for the winrate. This tells you the maximum likely winrate for Model B.\n",
    "\n",
    "- **average_weighted_score_a / average_weighted_score_b** Mean weighted scores across all valid evaluations, providing aggregate performance measures that account for rubric criterion weights.\n",
    "\n",
    "- **average_margin** Mean margin across all evaluations, indicating the typical magnitude of preference between models.\n",
    "\n",
    "---\n",
    "\n",
    "### Standard Error Metrics\n",
    "\n",
    "- **a_scores_stderr, b_scores_stderr, ties_stderr, inference_error_stderr, score_stderr**  \n",
    "  These metrics reflect the standard error (uncertainty) of each corresponding count or score. Smaller values indicate more reliable results, while larger values suggest more variability or a need for a larger sample size.\n",
    "\n",
    "---\n",
    "\n",
    "### How to Interpret These Metrics\n",
    "\n",
    "- **Winrate and Confidence Intervals**\n",
    "If the winrate is significantly above 0.5 and the confidence interval does not include 0.5, Model B is statistically favored across rubric criteria.\n",
    "If the winrate is below 0.5 and the confidence interval does not include 0.5, Model A is statistically favored.\n",
    "If the interval includes 0.5, results are inconclusive and may require additional evaluations.\n",
    "\n",
    "\n",
    "- **Weighted Score Analysis**\n",
    "  - Compare average_weighted_score_a vs average_weighted_score_b to understand overall performance accounting for criterion importance.\n",
    "Examine average_margin to assess the typical magnitude of performance differences.\n",
    "Review criteria_breakdown to identify specific strengths and weaknesses (e.g., Model B excels in accuracy but Model A is better at clarity).\n",
    "\n",
    "\n",
    "- **Error Analysis**\n",
    "  - High inference_error or large standard errors indicate possible issues with the evaluation process, rubric generation, or insufficient data.\n",
    "Review individual evaluation justifications to understand why errors occurred.\n",
    "\n",
    "- **Preference Distribution**\n",
    "  - The balance of a_scores, b_scores, and ties provides a direct picture of model performance differences on your evaluation set.\n",
    "Weighted scores provide deeper insight than raw counts by accounting for criterion importance.\n",
    "\n",
    "---\n",
    "\n",
    "### Example Metrics Output\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"a_scores\": 3.0,\n",
    "  \"a_scores_stderr\": 0.02,\n",
    "  \"b_scores\": 7.0,\n",
    "  \"b_scores_stderr\": 0.05,\n",
    "  \"ties\": 0.0,\n",
    "  \"ties_stderr\": 0.0,\n",
    "  \"inference_error\": 1.0,\n",
    "  \"inference_error_stderr\": 0.01,\n",
    "  \"winrate\": 0.70,\n",
    "  \"lower_rate\": 0.40,\n",
    "  \"upper_rate\": 0.909,\n",
    "  \"average_weighted_score_a\": 0.495,\n",
    "  \"average_weighted_score_b\": 0.630,\n",
    "  \"average_margin\": -0.135,\n",
    "  \"error_rate\": 0.091,\n",
    "  \"criteria_weights\": {\n",
    "    \"accuracy\": 0.6,\n",
    "    \"completeness\": 0.25,\n",
    "    \"clarity\": 0.15\n",
    "  }\n",
    "}\n",
    "```\n",
    "---\n",
    "\n",
    "These metrics, generated automatically during rubric-based evaluation, provide a comprehensive, statistically rigorous, and interpretable summary of how two models compare on your chosen dataset. They enable you to make informed decisions about model selection, deployment, and improvement priorities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and Installation\n",
    "\n",
    "Set up the required dependencies and configure the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and Installation\n",
    "\n",
    "Set up the required dependencies and configure the environment.\n",
    "\n",
    "#### IMPORTANT: Ensure that this specific version (2.254.1) of the Sagemaker CLI is used. Nova Customization does not currently support the latest SageMaker v3 CLI!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sagemaker==2.254.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries\n",
    "Import necessary Python packages and set up SageMaker session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.inputs import TrainingInput\n",
    "import boto3\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import boto3\n",
    "import json\n",
    "from sagemaker.huggingface import HuggingFacePredictor\n",
    "\n",
    "\n",
    "# Initialize SageMaker session\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T17:47:53.128111Z",
     "iopub.status.busy": "2025-06-26T17:47:53.127424Z",
     "iopub.status.idle": "2025-06-26T17:47:53.131058Z",
     "shell.execute_reply": "2025-06-26T17:47:53.130420Z",
     "shell.execute_reply.started": "2025-06-26T17:47:53.128079Z"
    }
   },
   "source": [
    "## Model Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy Qwen2.5 1.5B Instruct Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 deploy_model_arg.py Qwen/Qwen2.5-1.5B-Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T19:19:06.459390Z",
     "iopub.status.busy": "2025-11-24T19:19:06.459065Z",
     "iopub.status.idle": "2025-11-24T19:19:06.462447Z",
     "shell.execute_reply": "2025-11-24T19:19:06.461893Z",
     "shell.execute_reply.started": "2025-11-24T19:19:06.459367Z"
    }
   },
   "source": [
    "### Deploy Qwen2.5 7B Instruct Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 deploy_model_arg.py Qwen/Qwen2.5-7B-Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test out the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFacePredictor\n",
    "\n",
    "def generate_response(endpoint_name: str, prompt: str, max_tokens: int = 500, temperature: float = 0.9) -> str:\n",
    "    predictor = HuggingFacePredictor(endpoint_name=endpoint_name)\n",
    "    response = predictor.predict({\n",
    "        \"inputs\": prompt,\n",
    "        \"parameters\": {\n",
    "            \"max_new_tokens\": max_tokens,\n",
    "            \"temperature\": temperature\n",
    "        }\n",
    "    })\n",
    "    return response[0][\"generated_text\"]\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    answer_1_5b = generate_response(\"qwen25-15b-instruct-2025-11-24-22-28-11-675\", \"What is the Grotto at Notre Dame?\")\n",
    "    print(answer_1_5b)\n",
    "\n",
    "    print(\"*******************\")\n",
    "    print(\"*******************\")\n",
    "    answer_7b = generate_response(\"qwen25-7b-instruct-2025-11-24-22-27-28-607\", \"What is the Grotto at Notre Dame?\")\n",
    "    print(answer_7b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T18:08:50.568465Z",
     "iopub.status.busy": "2025-06-26T18:08:50.567730Z",
     "iopub.status.idle": "2025-06-26T18:08:50.571471Z",
     "shell.execute_reply": "2025-06-26T18:08:50.570831Z",
     "shell.execute_reply.started": "2025-06-26T18:08:50.568435Z"
    }
   },
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "squad = load_dataset(\"squad\", split=\"train[:20]\")\n",
    "print(squad[3][\"question\"])\n",
    "print(squad[3][\"answers\"][\"text\"][0])\n",
    "\n",
    "questions = [squad[i][\"question\"] for i in range(7, 14)]\n",
    "\n",
    "print(len(questions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T14:24:25.671531Z",
     "iopub.status.busy": "2025-06-27T14:24:25.670633Z",
     "iopub.status.idle": "2025-06-27T14:24:25.674408Z",
     "shell.execute_reply": "2025-06-27T14:24:25.673720Z",
     "shell.execute_reply.started": "2025-06-27T14:24:25.671498Z"
    }
   },
   "source": [
    "### Generate Evaluation Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "output_path = \"llm_judge.jsonl\"\n",
    "\n",
    "with open(output_path, \"w\") as f:\n",
    "    for q in questions:\n",
    "        try:\n",
    "            response_a = generate_response(\"qwen25-15b-instruct-2025-11-24-22-28-11-675\", q)\n",
    "        except Exception as e:\n",
    "            response_a = f\"[Qwen2.5 generation failed: {e}]\"\n",
    "        try:\n",
    "            response_b = generate_response(\"qwen25-7b-instruct-2025-11-24-22-27-28-607\", q)\n",
    "        except Exception as e:\n",
    "            response_b = f\"[Claude 3.7 generation failed: {e}]\"\n",
    "\n",
    "        row = {\n",
    "            \"prompt\": q,\n",
    "            \"response_A\": response_a,\n",
    "            \"response_B\": response_b\n",
    "        }\n",
    "        f.write(json.dumps(row) + \"\\n\")\n",
    "\n",
    "print(f\"JSONL file created at: {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Training Parameters\n",
    "Set up the necessary parameters for the training job\n",
    "The recipe yaml will be provided as a part of this example notebook under the filename eval_rubric_judge_recipe.yaml.\n",
    "\n",
    "Please enter your bucket name in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please populate parameters\n",
    "\n",
    "your_bucket_name = \"<Your Bucket Name>\"\n",
    "assert your_bucket_name != \"\", \"PLEASE POPULATE YOUR BUCKET NAME ABOVE\"\n",
    "\n",
    "input_s3_uri = \"<INPUT_S3_DATA_PATH>\".format(your_bucket_name)\n",
    "output_s3_uri = \"<OUTPUT_S3_DATA_PATH>\".format(your_bucket_name) # Output data s3 location\n",
    "instance_type = \"ml.p5.48xlarge\"  # this has to be run on a P5 instance\n",
    "job_name = \"rubric-judge-demo\"\n",
    "recipe_path = \"./eval_rubric_judge_recipe.yaml\" # Ensure this is the correct recipe for rubric judge\n",
    "image_uri = \"708977205387.dkr.ecr.us-east-1.amazonaws.com/nova-evaluation-repo:SM-TJ-Eval-V2-latest\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Data to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T15:02:33.242097Z",
     "iopub.status.busy": "2025-06-27T15:02:33.241445Z",
     "iopub.status.idle": "2025-06-27T15:02:33.247763Z",
     "shell.execute_reply": "2025-06-27T15:02:33.246950Z",
     "shell.execute_reply.started": "2025-06-27T15:02:33.242071Z"
    }
   },
   "source": [
    "## Grant S3 Permissions to the SageMaker Execution Role\n",
    "\n",
    "Before proceeding, make sure to grant the Execution Role direct `s3:PutObject` permissions for your S3 bucket prefix.\n",
    "\n",
    "**Steps:**\n",
    "- Go to the Execution Role (e.g., `AmazonSageMaker-ExecutionRole-...`) in the AWS IAM Console.\n",
    "- Attach the following inline policy:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"Effect\": \"Allow\",\n",
    "  \"Action\": [\n",
    "    \"s3:PutObject\",\n",
    "    \"s3:GetObject\",\n",
    "    \"s3:ListBucket\"\n",
    "  ],\n",
    "  \"Resource\": [\n",
    "    \"arn:aws:s3:::my-bucket-east\",\n",
    "    \"arn:aws:s3:::my-bucket-east/*\"\n",
    "  ]\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "def parse_s3_uri(s3_uri):\n",
    "    assert s3_uri.startswith(\"s3://\"), \"Invalid S3 URI\"\n",
    "    parts = s3_uri.replace(\"s3://\", \"\").split(\"/\", 1)\n",
    "    bucket = parts[0]\n",
    "    key = parts[1] if len(parts) > 1 else \"\"\n",
    "    return bucket, key\n",
    "\n",
    "def upload_to_s3(local_path, s3_uri):\n",
    "    \"\"\"\n",
    "    Upload evaluation data to S3 bucket using current role credentials.\n",
    "    \"\"\"\n",
    "    bucket, key = parse_s3_uri(s3_uri)\n",
    "    \n",
    "    s3 = boto3.client(\"s3\")\n",
    "    s3.upload_file(Filename=local_path, Bucket=bucket, Key=key)\n",
    "    print(f\"âœ… Uploaded {local_path} to {s3_uri}\")\n",
    "\n",
    "# Example usage\n",
    "upload_to_s3(\n",
    "    \"llm_judge.jsonl\",\n",
    "    \"s3://{}/datasets/byo-datasets-dev/custom-llm-judge/llm_judge.jsonl\".format(your_bucket_name)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Training Input (Optional)\n",
    "Configure input data source for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "evalInput = TrainingInput(\n",
    " s3_data=input_s3_uri,\n",
    " distribution='FullyReplicated',\n",
    " s3_data_type='S3Prefix'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Amazon Nova LLM-as-a-Judge Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = PyTorch(\n",
    "    output_path=output_s3_uri,\n",
    "    base_job_name=job_name,\n",
    "    role=role,\n",
    "    instance_type=instance_type,\n",
    "    training_recipe=recipe_path,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    image_uri=image_uri,\n",
    "    instance_count=1\n",
    ")\n",
    "\n",
    "estimator.fit({'train': evalInput})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Results From Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import tarfile\n",
    "\n",
    "def download_and_extract_job_output(training_job_name: str, output_s3_uri: str, download_dir: str = \"./output\"):\n",
    "    \"\"\"\n",
    "    Downloads the output.tar.gz of a SageMaker training job and extracts it locally.\n",
    "\n",
    "    Args:\n",
    "        training_job_name (str): Name of the SageMaker training job.\n",
    "        output_s3_uri (str): Base S3 URI where outputs are stored.\n",
    "        download_dir (str): Local directory to extract files into.\n",
    "    \"\"\"\n",
    "    # Build the full S3 path\n",
    "    s3_uri = f\"{output_s3_uri.rstrip('/')}/{training_job_name}/output/output.tar.gz\"\n",
    "    print(f\"Resolved S3 URI: {s3_uri}\")\n",
    "\n",
    "    # Parse bucket and key\n",
    "    def parse_s3_uri(s3_uri):\n",
    "        assert s3_uri.startswith(\"s3://\"), \"Invalid S3 URI\"\n",
    "        parts = s3_uri.replace(\"s3://\", \"\").split(\"/\", 1)\n",
    "        bucket = parts[0]\n",
    "        key = parts[1] if len(parts) > 1 else \"\"\n",
    "        return bucket, key\n",
    "\n",
    "    bucket, key = parse_s3_uri(s3_uri)\n",
    "\n",
    "    # Create S3 client\n",
    "    s3 = boto3.client(\"s3\")\n",
    "\n",
    "    # Create output directory\n",
    "    if not os.path.exists(download_dir):\n",
    "        os.makedirs(download_dir)\n",
    "\n",
    "    local_tar_path = os.path.join(download_dir, \"output.tar.gz\")\n",
    "\n",
    "    # Download file\n",
    "    print(\"Downloading...\")\n",
    "    s3.download_file(bucket, key, local_tar_path)\n",
    "    print(f\"Downloaded to {local_tar_path}\")\n",
    "\n",
    "    # Extract tar.gz\n",
    "    print(\"Extracting...\")\n",
    "    with tarfile.open(local_tar_path, \"r:gz\") as tar:\n",
    "        tar.extractall(path=download_dir)\n",
    "\n",
    "    print(f\"Extracted contents to {download_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_job_name = \"rubric-judge-dmeo\"\n",
    "assert training_job_name != \"\", \"PLEASE POPULATE YOUR TRAINING JOB NAME ABOVE\"\n",
    "\n",
    "download_and_extract_job_output(training_job_name, output_s3_uri)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rubric Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "#example_parquet_file_output = 'output/jmoul-rubric-demo/eval_results/details/rubric_judge_model/2025-12-09T01-39-55.025383+00-00/details_custom|rubric_llm_judge_judge|0_2025-12-09T01-39-55.025383+00-00.parquet'\n",
    "\n",
    "parquet_file_output = \"output/nova-lite-v2-rubric-llm-judge-eval-job/eval_results/details/rubric_judge_model/2025-12-15T18-04-27.835638+00-00/details_custom|rubric_llm_judge_judge|0_2025-12-15T18-04-27.835638+00-00.parquet\"\n",
    "assert parquet_file_output != \"\", \"PLEASE POPULATE YOUR PARQUET FILE LOCATION, LOOK AT EXAMPLE ABOVE\"\n",
    "\n",
    "df = pd.read_parquet(parquet_file_output)\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    metrics = ast.literal_eval(row['metrics'])\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Row {idx}:\")\n",
    "    print(f\"  Preference: {metrics['predictions']}\")\n",
    "    print(f\"  A wins: {metrics['a_scores']}\")\n",
    "    print(f\"  B wins: {metrics['b_scores']}\")\n",
    "    print(f\"  Weighted A: {metrics['weighted_score_A']:.3f}\")\n",
    "    print(f\"  Weighted B: {metrics['weighted_score_B']:.3f}\")\n",
    "    print(f\"  Margin: {metrics['score_margin']:.3f}\")\n",
    "    \n",
    "    # Overall justification\n",
    "    if metrics.get('overall_justification'):\n",
    "        print(f\"\\n  Overall Justification:\")\n",
    "        print(f\"    {metrics['overall_justification']}\")\n",
    "    \n",
    "    # Per-criterion breakdown with justifications\n",
    "    if metrics.get('criteria_breakdown'):\n",
    "        print(f\"\\n  Criteria:\")\n",
    "        for crit_name, crit_data in metrics['criteria_breakdown'].items():\n",
    "            print(f\"\\n    {crit_name}:\")\n",
    "            print(f\"      Score A: {crit_data['score_A']}, Score B: {crit_data['score_B']}\")\n",
    "            print(f\"      Weight: {crit_data['weight']}, Type: {crit_data['type']}\")\n",
    "            print(f\"      Description: {crit_data['description']}\")\n",
    "            if crit_data.get('justification_A'):\n",
    "                print(f\"      Justification A: {crit_data['justification_A']}\")\n",
    "            if crit_data.get('justification_B'):\n",
    "                print(f\"      Justification B: {crit_data['justification_B']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T14:30:03.019249Z",
     "iopub.status.busy": "2025-06-27T14:30:03.018574Z",
     "iopub.status.idle": "2025-06-27T14:30:03.024708Z",
     "shell.execute_reply": "2025-06-27T14:30:03.023844Z",
     "shell.execute_reply.started": "2025-06-27T14:30:03.019225Z"
    }
   },
   "source": [
    "## Results Visualization\n",
    "\n",
    "Based on the evaluation results shown in the uploaded image, here's how to create comprehensive visualizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def plot_llm_judge_results(results):\n",
    "    \"\"\"\n",
    "    Plot LLM judge evaluation results from a JSON file.\n",
    "\n",
    "    Args:\n",
    "        json_file_path (str): Path to the JSON results file\n",
    "        save_plots (bool): Whether to save plots to files\n",
    "        output_dir (str): Directory to save plots (defaults to same dir as JSON file)\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing the plotted data for further analysis\n",
    "    \"\"\"\n",
    "\n",
    "    # Set style\n",
    "    plt.style.use(\"default\")\n",
    "    sns.set_palette(\"husl\")\n",
    "\n",
    "    # Create figure with subplots\n",
    "    fig = plt.figure(figsize=(16, 12))\n",
    "\n",
    "    # 1. Score Distribution Bar Chart\n",
    "    ax1 = plt.subplot(2, 3, 1)\n",
    "    scores = {\n",
    "        \"A Scores\": results[\"a_scores\"],\n",
    "        \"B Scores\": results[\"b_scores\"],\n",
    "        \"Ties\": results[\"ties\"],\n",
    "        \"Inference Errors\": results[\"inference_error\"],\n",
    "    }\n",
    "\n",
    "    bars = ax1.bar(\n",
    "        scores.keys(),\n",
    "        scores.values(),\n",
    "        color=[\"#FF6B6B\", \"#4ECDC4\", \"#45B7D1\", \"#FFA07A\"],\n",
    "    )\n",
    "    ax1.set_title(\"Score Distribution\", fontsize=14, fontweight=\"bold\")\n",
    "    ax1.set_ylabel(\"Count\")\n",
    "\n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, scores.values()):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(\n",
    "            bar.get_x() + bar.get_width() / 2.0,\n",
    "            height + height * 0.01,\n",
    "            f\"{int(value)}\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "    # 2. Win Rate with Confidence Interval\n",
    "    ax2 = plt.subplot(2, 3, 2)\n",
    "    winrate = results[\"winrate\"]\n",
    "    lower_rate = results[\"lower_rate\"]\n",
    "    upper_rate = results[\"upper_rate\"]\n",
    "\n",
    "    # Create horizontal bar for winrate\n",
    "    ax2.barh([\"Win Rate\"], [winrate], color=\"#4ECDC4\", alpha=0.7, height=0.3)\n",
    "\n",
    "    # Add confidence interval\n",
    "    ax2.errorbar(\n",
    "        [winrate],\n",
    "        [\"Win Rate\"],\n",
    "        xerr=[[winrate - lower_rate], [upper_rate - winrate]],\n",
    "        fmt=\"o\",\n",
    "        color=\"black\",\n",
    "        capsize=10,\n",
    "        capthick=2,\n",
    "    )\n",
    "\n",
    "    ax2.set_xlim(0, 1)\n",
    "    ax2.set_xlabel(\"Win Rate\")\n",
    "    ax2.set_title(\"B vs A Win Rate with 95% CI\", fontsize=14, fontweight=\"bold\")\n",
    "    ax2.axvline(\n",
    "        x=0.5, color=\"red\", linestyle=\"--\", alpha=0.7, label=\"50% (No preference)\"\n",
    "    )\n",
    "    ax2.legend()\n",
    "\n",
    "    # Add text annotation\n",
    "    ax2.text(\n",
    "        winrate,\n",
    "        0,\n",
    "        f\"{winrate:.3f}\\n[{lower_rate:.3f}, {upper_rate:.3f}]\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontweight=\"bold\",\n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8),\n",
    "    )\n",
    "\n",
    "    # 3. Preference Pie Chart (excluding inference errors)\n",
    "    ax3 = plt.subplot(2, 3, 3)\n",
    "    total_valid = results[\"a_scores\"] + results[\"b_scores\"] + results[\"ties\"]\n",
    "\n",
    "    if total_valid > 0:\n",
    "        pie_data = [results[\"a_scores\"], results[\"b_scores\"], results[\"ties\"]]\n",
    "        pie_labels = [\"A Preferred\", \"B Preferred\", \"Ties\"]\n",
    "        colors = [\"#FF6B6B\", \"#4ECDC4\", \"#45B7D1\"]\n",
    "\n",
    "        wedges, texts, autotexts = ax3.pie(\n",
    "            pie_data, labels=pie_labels, colors=colors, autopct=\"%1.1f%%\", startangle=90\n",
    "        )\n",
    "\n",
    "        # Make percentage text bold\n",
    "        for autotext in autotexts:\n",
    "            autotext.set_fontweight(\"bold\")\n",
    "            autotext.set_color(\"white\")\n",
    "\n",
    "    ax3.set_title(\n",
    "        \"Preference Distribution\\n(Valid Judgments Only)\",\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "    # 4. Comparison of A vs B Scores\n",
    "    ax4 = plt.subplot(2, 3, 4)\n",
    "    categories = [\"A Scores\", \"B Scores\"]\n",
    "    values = [results[\"a_scores\"], results[\"b_scores\"]]\n",
    "    colors = [\"#FF6B6B\", \"#4ECDC4\"]\n",
    "\n",
    "    bars = ax4.bar(categories, values, color=colors, alpha=0.8)\n",
    "    ax4.set_title(\"A vs B Score Comparison\", fontsize=14, fontweight=\"bold\")\n",
    "    ax4.set_ylabel(\"Score Count\")\n",
    "\n",
    "    # Add value labels\n",
    "    for bar, value in zip(bars, values):\n",
    "        height = bar.get_height()\n",
    "        ax4.text(\n",
    "            bar.get_x() + bar.get_width() / 2.0,\n",
    "            height + height * 0.01,\n",
    "            f\"{int(value)}\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "\n",
    "    # Add difference annotation\n",
    "    diff = abs(values[0] - values[1])\n",
    "    winner = \"A\" if values[0] > values[1] else \"B\"\n",
    "    ax4.text(\n",
    "        0.5,\n",
    "        max(values) * 0.8,\n",
    "        f\"{winner} leads by {int(diff)}\",\n",
    "        ha=\"center\",\n",
    "        transform=ax4.transData,\n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"yellow\", alpha=0.7),\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "    # 5. Win Rate Visualization (Gauge-like)\n",
    "    ax5 = plt.subplot(2, 3, 5)\n",
    "\n",
    "    # Create a semi-circular gauge\n",
    "    theta = np.linspace(0, np.pi, 100)\n",
    "    r = 1\n",
    "\n",
    "    # Background arc\n",
    "    ax5.plot(r * np.cos(theta), r * np.sin(theta), \"lightgray\", linewidth=10)\n",
    "\n",
    "    # Win rate arc\n",
    "    winrate_theta = np.linspace(0, winrate * np.pi, int(winrate * 100))\n",
    "    ax5.plot(\n",
    "        r * np.cos(winrate_theta), r * np.sin(winrate_theta), \"#4ECDC4\", linewidth=10\n",
    "    )\n",
    "\n",
    "    # Add needle\n",
    "    needle_angle = winrate * np.pi\n",
    "    ax5.arrow(\n",
    "        0,\n",
    "        0,\n",
    "        0.8 * np.cos(needle_angle),\n",
    "        0.8 * np.sin(needle_angle),\n",
    "        head_width=0.05,\n",
    "        head_length=0.05,\n",
    "        fc=\"red\",\n",
    "        ec=\"red\",\n",
    "    )\n",
    "\n",
    "    ax5.set_xlim(-1.2, 1.2)\n",
    "    ax5.set_ylim(-0.2, 1.2)\n",
    "    ax5.set_aspect(\"equal\")\n",
    "    ax5.axis(\"off\")\n",
    "    ax5.set_title(\"Win Rate Gauge\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "    # Add labels\n",
    "    ax5.text(-1, -0.1, \"0%\", ha=\"center\", fontweight=\"bold\")\n",
    "    ax5.text(0, -0.1, \"50%\", ha=\"center\", fontweight=\"bold\")\n",
    "    ax5.text(1, -0.1, \"100%\", ha=\"center\", fontweight=\"bold\")\n",
    "    ax5.text(\n",
    "        0,\n",
    "        0.5,\n",
    "        f\"{winrate:.1%}\",\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        fontsize=16,\n",
    "        fontweight=\"bold\",\n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8),\n",
    "    )\n",
    "\n",
    "    # 6. Summary Statistics Table\n",
    "    ax6 = plt.subplot(2, 3, 6)\n",
    "    ax6.axis(\"off\")\n",
    "\n",
    "    # Create summary statistics\n",
    "    total_evaluations = (\n",
    "        results[\"a_scores\"]\n",
    "        + results[\"b_scores\"]\n",
    "        + results[\"ties\"]\n",
    "        + results[\"inference_error\"]\n",
    "    )\n",
    "\n",
    "    summary_data = [\n",
    "        [\"Total Evaluations\", f\"{int(total_evaluations)}\"],\n",
    "        [\"A Scores\", f\"{int(results['a_scores'])}\"],\n",
    "        [\"B Scores\", f\"{int(results['b_scores'])}\"],\n",
    "        [\"Ties\", f\"{int(results['ties'])}\"],\n",
    "        [\"Inference Errors\", f\"{int(results['inference_error'])}\"],\n",
    "        [\"Win Rate (B vs A)\", f\"{results['winrate']:.3f}\"],\n",
    "        [\"95% CI Lower\", f\"{results['lower_rate']:.3f}\"],\n",
    "        [\"95% CI Upper\", f\"{results['upper_rate']:.3f}\"],\n",
    "        [\n",
    "            \"Error Rate\",\n",
    "            (\n",
    "                f\"{results['inference_error']/total_evaluations:.1%}\"\n",
    "                if total_evaluations > 0\n",
    "                else \"0%\"\n",
    "            ),\n",
    "        ],\n",
    "    ]\n",
    "\n",
    "    # Create table\n",
    "    table = ax6.table(\n",
    "        cellText=summary_data,\n",
    "        colLabels=[\"Metric\", \"Value\"],\n",
    "        cellLoc=\"left\",\n",
    "        loc=\"center\",\n",
    "        colWidths=[0.6, 0.4],\n",
    "    )\n",
    "\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1, 2)\n",
    "\n",
    "    # Style the table\n",
    "    for i in range(len(summary_data) + 1):\n",
    "        for j in range(2):\n",
    "            cell = table[(i, j)]\n",
    "            if i == 0:  # Header\n",
    "                cell.set_facecolor(\"#4ECDC4\")\n",
    "                cell.set_text_props(weight=\"bold\", color=\"white\")\n",
    "            else:\n",
    "                cell.set_facecolor(\"#f0f0f0\" if i % 2 == 0 else \"white\")\n",
    "                if j == 1:  # Value column\n",
    "                    cell.set_text_props(weight=\"bold\")\n",
    "\n",
    "    ax6.set_title(\"Summary Statistics\", fontsize=14, fontweight=\"bold\", pad=20)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example evaluations path below\n",
    "# evaluation_results_path = \"./output/jmoul-rubric-demo/eval_results/results_2025-12-09T01-39-55.025383+00-00.json\"\n",
    "\n",
    "evaluation_results_path = \"output/nova-lite-v2-rubric-llm-judge-eval-job/eval_results/results_2025-12-15T18-04-27.835638+00-00.json\"\n",
    "assert evaluation_results_path != \"\", \"PLEASE POPULATE YOUR EVALUATIONS RESULTS PATH ABOVE\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "with open(evaluation_results_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "fig = plot_llm_judge_results(data[\"results\"][\"all\"])\n",
    "\n",
    "output_file = os.path.join(\"./\", \"evaluation_metrics.png\")\n",
    "fig.savefig(output_file, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T14:49:23.956157Z",
     "iopub.status.busy": "2025-06-27T14:49:23.955800Z",
     "iopub.status.idle": "2025-06-27T14:49:23.962180Z",
     "shell.execute_reply": "2025-06-27T14:49:23.961415Z",
     "shell.execute_reply.started": "2025-06-27T14:49:23.956132Z"
    }
   },
   "source": [
    "- **Model B (Qwen2.5 7B Instruct)** demonstrates superior performance with a 70% win rate against **Model A (Qwen2.5 1.5B Instruct)**, though there's notable variability across different question types.\n",
    "\n",
    "- **Aggregate Statistics**  \n",
    "  - Total Evaluations: 11 (7 detailed datapoints provided)\n",
    "  - Valid Judgments: 10 (excluding 1 inference error)\n",
    "  - Win Distribution: B scored 7 wins vs A's 3 wins\n",
    "  - Preference Rate: 70% preferred B, 30% preferred A\n",
    "  - 95% Confidence Interval: [0.400, 0.909] - indicating statistical confidence in B's superiority\n",
    "  - Error Rate: 9.1% (1 inference error out of 11 evaluations)\n",
    "\n",
    "**Detailed Datapoint Analysis**   \n",
    "\n",
    "From the 5 valid evaluations in your detailed data:\n",
    "\n",
    "- **Model B(Qwen2.5 7B Instruct) Wins (3 cases - 60%)**\n",
    "  - Row 0: Notre Dame student newspapers - B provided more accurate, complete information with specific examples vs A's unsupported number\n",
    "  - Row 3: Common Sense publication year - B correctly identified 1916 vs A's incorrect 1879\n",
    "  - Row 6: Notre Dame's oldest structure - B accurately identified western facade and southern spire vs A's confused response\n",
    "- **Model A(Qwen2.5 1.5B Instruct) Wins (2 cases - 40%)**\n",
    "  - Row 1: Congregation of Holy Cross headquarters - A correctly explained decentralized structure vs B's inaccurate single location claim\n",
    "  - Row 2: Primary seminary information - A provided detailed, accurate explanation vs B's completely unrelated content\n",
    "\n",
    "**Weighted Performance Metrics**\n",
    "- Average Weighted Score A: 0.495\n",
    "- Average Weighted Score B: 0.630\n",
    "- Average Margin (B-A): -0.135 (negative indicates B's advantage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T14:53:46.318232Z",
     "iopub.status.busy": "2025-06-27T14:53:46.317515Z",
     "iopub.status.idle": "2025-06-27T14:53:46.324514Z",
     "shell.execute_reply": "2025-06-27T14:53:46.323623Z",
     "shell.execute_reply.started": "2025-06-27T14:53:46.318184Z"
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated a complete **Amazon Nova Rubric-Based LLM-as-a-Judge** evaluation pipeline using Amazon SageMaker AI. The methodology provides:\n",
    "\n",
    "**Scalable Evaluation**: Automated rubric-based comparison of multiple models with dynamically generated evaluation criteria\n",
    "\n",
    "**Statistical Rigor**: Confidence intervals and significance testing with weighted scoring across multiple rubric dimensions (accuracy, completeness, clarity)\n",
    "\n",
    "**Cost Efficiency**: Reduced need for human evaluation through automated rubric generation and criterion-specific scoring\n",
    "\n",
    "**Actionable Insights**: Clear metrics for model selection with detailed justifications per evaluation criterion and weighted performance analysis\n",
    "\n",
    "The 70% win rate with a confidence interval not crossing the 50% threshold suggests statistically meaningful superiority for the 7B model, though the relatively small sample size (11 evaluations) means continued testing would strengthen these conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
