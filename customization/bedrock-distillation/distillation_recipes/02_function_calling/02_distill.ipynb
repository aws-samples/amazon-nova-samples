{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d096b3f",
   "metadata": {},
   "source": [
    "# Amazon Bedrock Model Distillation for function_calling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b237369",
   "metadata": {},
   "source": [
    "> ⚠️ **CRITICAL COST WARNING**\n",
    ">\n",
    "> This notebook uses Amazon Bedrock Provisioned Throughput (PT) which has significant cost implications. As of this writing, a PT endpoints are required in order to make inferences to a distilled model.\n",
    ">\n",
    "> - Provisioned units are billed for the entire month regardless of actual usage\n",
    "> - You cannot cancel provisioned capacity until the month ends\n",
    "> - \n",
    "> \n",
    "> **Before running this notebook:**\n",
    "> 1. Please read and understand the cost implications of [creating a Provisioned Throughput endpoint](https://docs.aws.amazon.com/bedrock/latest/userguide/prov-throughput.html?utm_source=chatgpt.com)\n",
    "> 2. Check current pricing at [AWS Bedrock Pricing](https://aws.amazon.com/bedrock/pricing/)\n",
    ">\n",
    "> 💡 **Best Practice**: Delete or release provisioned capacity when not actively needed to avoid unnecessary charges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf2edd1",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "After completing this notebook, you will be able to:\n",
    "1. Implement advanced model distillation techniques using Amazon Bedrock's Distilation APIs\n",
    "2. Configure and optimize teacher-student model architectures for citation tasks\n",
    "3. Monitor and evaluate distillation performance metrics\n",
    "4. Deploy and manage production-grade distilled models\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Model distillation is an advanced knowledge transfer technique that enables the creation of efficient, production-ready models by distilling knowledge from larger foundation models into smaller, specialized ones. This notebook demonstrates enterprise-grade implementation of model distillation in Amazon Bedrock, focusing on citation generation use cases.\n",
    "\n",
    "### Setup and Prerequisites\n",
    "**By now you should have already ran notebook `01_prepare_data.ipynb` to prepare all of the data for subsequent steps.**\n",
    "\n",
    "Before proceeding, ensure you have:\n",
    "\n",
    "- An active AWS account with appropriate permissions\n",
    "- Amazon Bedrock access enabled in your preferred region\n",
    "- An S3 bucket for storing training data and output\n",
    "- Training data in JSONL format\n",
    "- Sufficient service quota to use Provisioned Throughput in Bedrock\n",
    "- An IAM role with the following permissions:\n",
    "\n",
    "IAM Policy:\n",
    "```json\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:PutObject\",\n",
    "                \"s3:ListBucket\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::YOUR_DISTILLATION_OUTPUT_BUCKET\",\n",
    "                \"arn:aws:s3:::YOUR_DISTILLATION_OUTPUT_BUCKET/*\",\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"bedrock:CreateModelCustomizationJob\",\n",
    "                \"bedrock:GetModelCustomizationJob\",\n",
    "                \"bedrock:ListModelCustomizationJobs\",\n",
    "                \"bedrock:StopModelCustomizationJob\"\n",
    "            ],\n",
    "            \"Resource\": \"arn:aws:bedrock:YOUR_REGION:YOUR_ACCOUNT_ID:model-customization-job/*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "Trust Relationship:\n",
    "```json\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": [\n",
    "                    \"bedrock.amazonaws.com\"\n",
    "                ]\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\",\n",
    "            \"Condition\": {\n",
    "                \"StringEquals\": {\n",
    "                    \"aws:SourceAccount\": \"YOUR_ACCOUNT_ID\"\n",
    "                },\n",
    "                \"ArnLike\": {\n",
    "                    \"aws:SourceArn\": \"arn:aws:bedrock:YOUR_REGION:YOUR_ACCOUNT_ID:model-customization-job/*\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "First, let's set up our environment and import required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9296e2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upgrade boto3 \n",
    "%pip install --upgrade pip --quiet\n",
    "%pip install boto3 --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063248b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc786809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "skip_dir = os.path.dirname(parent_dir)\n",
    "sys.path.append(skip_dir)\n",
    "\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "from botocore.exceptions import ClientError\n",
    "from utils import create_s3_bucket, upload_training_data_to_s3, delete_s3_bucket_and_contents, \\\n",
    "create_model_distillation_role_and_permissions, delete_role_and_attached_policies, delete_distillation_buckets\n",
    "\n",
    "# Create Bedrock client\n",
    "bedrock_client = boto3.client(service_name=\"bedrock\",region_name='us-east-1')\n",
    "\n",
    "# Create runtime client for inference\n",
    "bedrock_runtime = boto3.client(service_name='bedrock-runtime',region_name='us-east-1')\n",
    "\n",
    "# Region and accountID\n",
    "session = boto3.session.Session(region_name='us-east-1')\n",
    "region =  'us-east-1' # session.region_name\n",
    "sts_client = session.client(service_name='sts',region_name='us-east-1')\n",
    "account_id = sts_client.get_caller_identity()['Account']\n",
    "\n",
    "# define bucket you want to create and upload the dataset to:\n",
    "BUCKET_NAME= '<BUCKET_NAME>' # Replace by your bucket name\n",
    "DATA_PREFIX = 'function_calling_distillation' # Replace by your defined prefix\n",
    "\n",
    "# configure teacher and student model\n",
    "teacher_model = \"us.amazon.nova-premier-v1:0\"\n",
    "# teacher_model = \"us.amazon.nova-pro-v1:0\"\n",
    "student_model = \"amazon.nova-lite-v1:0:300k\"\n",
    "# student_model = \"amazon.nova-micro-v1:0:128k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48760820",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(teacher_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bea0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate unique names for the job and model\n",
    "distillation_dataset_prompt_only = 'training/bedrock_training_data_prompt_only.jsonl'\n",
    "distillation_dataset_tool_config = 'training/bedrock_training_data_tool_config.jsonl'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce95f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure models and IAM role\n",
    "role_name, role_arn = create_model_distillation_role_and_permissions(bucket_name=BUCKET_NAME, account_id=account_id)\n",
    "\n",
    "# Set maximum response length\n",
    "max_response_length = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30233e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating training data bucket\n",
    "create_s3_bucket(bucket_name=BUCKET_NAME)\n",
    "\n",
    "# Specify S3 locations\n",
    "training_data_s3_uri_prompt_only = upload_training_data_to_s3(BUCKET_NAME, distillation_dataset_prompt_only, prefix=DATA_PREFIX)\n",
    "training_data_s3_uri_tool_config = upload_training_data_to_s3(BUCKET_NAME, distillation_dataset_tool_config, prefix=DATA_PREFIX)\n",
    "output_path = f\"s3://{BUCKET_NAME}/{DATA_PREFIX}/outputs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e9bb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(role_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd8eebe",
   "metadata": {},
   "source": [
    "# Starting the Distillation Job\n",
    "\n",
    "With our environment configured and data prepared, we'll initiate the distillation process. This section covers:\n",
    "\n",
    "1. Job Configuration\n",
    "   - Model selection and parameters\n",
    "   - Resource allocation\n",
    "   - Output settings\n",
    "\n",
    "2. Performance Optimization\n",
    "   - Response length tuning\n",
    "   - Batch size configuration\n",
    "   - Resource utilization\n",
    "\n",
    "3. Monitoring Setup\n",
    "   - Metrics configuration\n",
    "   - Logging settings\n",
    "   - Alert thresholds\n",
    "\n",
    "We'll use the `create_model_customization_job` API with production-optimized settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec649fd",
   "metadata": {},
   "source": [
    "Before running this cell, wait a few moments to ensure the iam role has been persisted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ea0394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit distillation for the prompt only dataset\n",
    "current_dt = datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "\n",
    "# response = bedrock_client.create_model_customization_job(\n",
    "#     jobName=f\"distill-fn-calling-prompt-only-{current_dt}\",\n",
    "#     customModelName=f\"distilled-fn-calling-prompt-only-{current_dt}\",\n",
    "#     roleArn=role_arn,\n",
    "#     baseModelIdentifier=student_model,\n",
    "#     customizationType=\"DISTILLATION\",\n",
    "#     trainingDataConfig={\n",
    "#         \"s3Uri\": training_data_s3_uri_prompt_only\n",
    "#     },\n",
    "#     outputDataConfig={\n",
    "#         \"s3Uri\": output_path\n",
    "#     },\n",
    "#     customizationConfig={\n",
    "#         \"distillationConfig\": {\n",
    "#             \"teacherModelConfig\": {\n",
    "#                 \"teacherModelIdentifier\": teacher_model,\n",
    "#                 \"maxResponseLengthForInference\": max_response_length \n",
    "#             }\n",
    "#         }\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811dd77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record the distillation job arn\n",
    "# prompt_only_job_arn = response['jobArn']\n",
    "# print(\"prompt_only_job_arn\", prompt_only_job_arn)\n",
    "\n",
    "# # print job status\n",
    "# job_status = bedrock_client.get_model_customization_job(jobIdentifier=prompt_only_job_arn)[\"status\"]\n",
    "# print(job_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a10e11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit distillation for the tool config dataset\n",
    "response = bedrock_client.create_model_customization_job(\n",
    "    jobName=f\"fn-calling-t-prem-s-lite-{current_dt}\",\n",
    "    customModelName=f\"fn-calling-t-prem-s-lite-{current_dt}\",\n",
    "    roleArn=role_arn,\n",
    "    baseModelIdentifier=student_model,\n",
    "    customizationType=\"DISTILLATION\",\n",
    "    trainingDataConfig={\n",
    "        \"s3Uri\": training_data_s3_uri_tool_config\n",
    "    },\n",
    "    outputDataConfig={\n",
    "        \"s3Uri\": output_path\n",
    "    },\n",
    "    customizationConfig={\n",
    "        \"distillationConfig\": {\n",
    "            \"teacherModelConfig\": {\n",
    "                \"teacherModelIdentifier\": teacher_model,\n",
    "                \"maxResponseLengthForInference\": max_response_length \n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab6aa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record the distillation job arn\n",
    "tool_config_job_arn = response['jobArn']\n",
    "print(\"tool_config_job_arn\", tool_config_job_arn)\n",
    "\n",
    "# print job status\n",
    "job_status = bedrock_client.get_model_customization_job(jobIdentifier=tool_config_job_arn)[\"status\"]\n",
    "print(job_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503e1de0",
   "metadata": {},
   "source": [
    "A distillation job of this size (15000 records) can take a number of hours to complete. Return to this notebook once that job has completed by running the above cell.\n",
    "\n",
    "Once distillation is complete, we're ready to deploy to our PT endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0da8a8e",
   "metadata": {},
   "source": [
    "⚠️ **Please understand the cost considerations for Provisioned Throughput (PT) endpoints before proceeding**\n",
    "\n",
    "\n",
    "# Deploying the Distilled Model\n",
    "\n",
    "Now we'll deploy our distilled model to a custom model deployment. We'll do this by grabbing the model name from the distillation job and creating an endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a050d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get custom model id for the prompt-on model\n",
    "# custom_model_arn_prompt_only = bedrock_client.get_model_customization_job(jobIdentifier=prompt_only_job_arn)['outputModelArn']\n",
    "\n",
    "# get the custom model id for tool config model\n",
    "custom_model_arn_tool_config = bedrock_client.get_model_customization_job(jobIdentifier=tool_config_job_arn)['outputModelArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3b0cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deploy custom prompt-on model\n",
    "# response = bedrock_client.create_custom_model_deployment(**{\n",
    "#     \"modelDeploymentName\": f\"deployment-custom-fn-nova-prompt-only\",\n",
    "#     \"modelArn\": custom_model_arn_prompt_only,\n",
    "# })\n",
    "# custom_model_deployment_arn_prompt_only = response['customModelDeploymentArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ad1681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deploy custom prompt-on model\n",
    "response = bedrock_client.create_custom_model_deployment(**{\n",
    "    \"modelDeploymentName\": f\"deployment-custom-fn-nova-tool-config\",\n",
    "    \"modelArn\": custom_model_arn_tool_config,\n",
    "})\n",
    "custom_model_deployment_arn_tool_config = response['customModelDeploymentArn']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca969f2e",
   "metadata": {},
   "source": [
    "Store the provisioned model endpoint ARN for subsequent inference operations and resource cleanup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6038780a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store custom_model_deployment_arn_prompt_only\n",
    "%store custom_model_deployment_arn_tool_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb8a025",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "This notebook has demonstrated advanced implementation techniques for model distillation in Amazon Bedrock, with a focus on citation generation use cases. \n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next notebook ([03_batch_inference.ipynb](03_batch_inference.ipynb)), we'll explore:\n",
    "1. Implementing batch inference with the distilled model\n",
    "2. Evaluating citation accuracy and performance metrics\n",
    "3. Optimizing throughput and latency\n",
    "4. Monitoring production workloads\n",
    "\n",
    "For additional resources:\n",
    "- [Amazon Bedrock Model Distillation Documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/model-distillation.html)\n",
    "- [Best Practices for Production Deployments](https://docs.aws.amazon.com/bedrock/latest/userguide/best-practices.html)\n",
    "- [Advanced Monitoring and Optimization](https://docs.aws.amazon.com/bedrock/latest/userguide/monitoring-customization.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef1bf4b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "function-calling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
