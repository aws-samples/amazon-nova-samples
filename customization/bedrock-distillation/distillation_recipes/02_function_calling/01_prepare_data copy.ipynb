{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "280d3606",
   "metadata": {},
   "source": [
    "# Model Distillation for Function Calling\n",
    "\n",
    "This notebook is part of a series demonstrating advanced model distillation techniques for creating specialized, function-calling-aware models. The goal is to distill the knowledge from a large language model (Amazon Nova Premier) into a smaller, more efficient model while maintaining high-quality function calling capabilities.\n",
    "\n",
    "## Learning Objectives\n",
    "- Prepare training data for function calling model distillation\n",
    "- Design structured output formats for consistent function parameter generation\n",
    "- Implement function selection and parameter extraction\n",
    "- Create evaluation datasets for measuring function calling accuracy\n",
    "\n",
    "## Dataset: Berkeley Function Calling Leaderboard (BFCL) V2 Live\n",
    "We use the Berkeley Function Calling Leaderboard (BFCL) V2 Live dataset as our base dataset. This dataset is particularly suitable for function-calling model training because:\n",
    "\n",
    "1. Contains 2,251 question-function-answer pairs total\n",
    "2. Provides diverse function calling scenarios:\n",
    "   - 258 simple calls\n",
    "   - 1,053 multiple parameter calls\n",
    "   - 16 parallel function calls\n",
    "   - 24 parallel multiple parameter calls\n",
    "   - 882 irrelevance detection cases\n",
    "   - 18 relevance detection cases\n",
    "3. Offers complexity with an average of 3 function choices per entry (maximum 37)\n",
    "4. Includes parameter diversity with an average of 4 parameters per function (maximum 28)\n",
    "\n",
    "The dataset is processed and stored in optimized formats for efficient model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3adadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upgrade boto3 \n",
    "%pip install --upgrade pip --quiet\n",
    "%pip install boto3 --upgrade --quiet\n",
    "%pip install bcfl-eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdea093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72c875c",
   "metadata": {},
   "source": [
    "We need to set the project root so results are put in the correct location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2b61383",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['BFCL_PROJECT_ROOT'] = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d244e4eb",
   "metadata": {},
   "source": [
    "If you're running this on your own machine, enter your AWS access keys in an .env file.\n",
    "Uncomment the below cell to copy down an example .env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b125616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # set up environment file\n",
    "# %cp $(python -c \"import bfcl_eval; print(bfcl_eval.__path__[0])\")/.env.example $BFCL_PROJECT_ROOT/.env\n",
    "# # Fill in necessary values in `.env`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8378ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download sample data\n",
    "# %cp $(python -c \"import bfcl_eval, pathlib; print(pathlib.Path(bfcl_eval.__path__[0]) / 'test_case_ids_to_generate.json.example')\") $BFCL_PROJECT_ROOT/test_case_ids_to_generate.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01188219",
   "metadata": {},
   "source": [
    "For this example we're going to use a combination of v3_simple, v3_multiple, v3_live_relevance, and v3_irrelevance to train and evaluate with. For more information on these categories and their intents, please visit the [official BFCL documentation](https://huggingface.co/datasets/gorilla-llm/Berkeley-Function-Calling-Leaderboard)\n",
    "\n",
    "Let's move these to our local directory so we can begin preparing the data for distillation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3d505320",
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir questions\n",
    "%cp $(python -c \"import bfcl_eval, pathlib; print(pathlib.Path(bfcl_eval.__path__[0]) / 'data' / 'BFCL_v3_simple.json')\") ./questions/BFCL_v3_simple.json\n",
    "%cp $(python -c \"import bfcl_eval, pathlib; print(pathlib.Path(bfcl_eval.__path__[0]) / 'data' / 'BFCL_v3_multiple.json')\") ./questions/BFCL_v3_multiple.json\n",
    "%cp $(python -c \"import bfcl_eval, pathlib; print(pathlib.Path(bfcl_eval.__path__[0]) / 'data' / 'BFCL_v3_irrelevance.json')\") ./questions/BFCL_v3_irrelevance.json\n",
    "%cp $(python -c \"import bfcl_eval, pathlib; print(pathlib.Path(bfcl_eval.__path__[0]) / 'data' / 'BFCL_v3_live_relevance.json')\") ./questions/BFCL_v3_live_relevance..json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2b47ba",
   "metadata": {},
   "source": [
    "Now will grab the corresponding answers. For the simple and multiple datasets, we are provided possible answers and we'll use these for our mix-in labels.\n",
    "\n",
    "Per the BFCL documentation, the correct answer for any question in the `BFCL_v3_irrelevance` datset is an empty list of functions, as these are design specifically to test the model's ability to correctly identify zero possible functions that are relevant.\n",
    "\n",
    "The correct answer for any question in the `BFCL_v3_live_relevance` dataset is \"at least one\" function call returned.\n",
    "\n",
    "Here's an excerpt from [their documentation](https://huggingface.co/datasets/gorilla-llm/Berkeley-Function-Calling-Leaderboard):\n",
    "\n",
    "> **Irrelevance Detection (875):** The scenario where none of the function choices provided are relevant to the user query and none should be invoked. We expect the model to not output a function call; the model can either output a message explaining why the function provided are not relevant or simply output a non-function call response (e.g., an empty list).\n",
    "\n",
    "> **Relevance Detection (41):** The opposite of irrelevance detection. The scenario where at least one of the function choices provided are relevant to the user query and should be invoked, but the way the user prompt or the function doc is stated means that there could be infinitely many correct function calls and impossible to use a pre-defined possible answer set to evaluate. We expect the model to output some function call (one or multiple) that is relevant to the user query; we don't check for the correctness of the function call in this category (eg, correct parameter value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d625a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir answers\n",
    "%cp $(python -c \"import bfcl_eval, pathlib; print(pathlib.Path(bfcl_eval.__path__[0]) / 'data' / 'possible_answer' / 'BFCL_v3_simple.json')\") ./answers/BFCL_v3_simple.json\n",
    "%cp $(python -c \"import bfcl_eval, pathlib; print(pathlib.Path(bfcl_eval.__path__[0]) / 'data' / 'possible_answer' / 'BFCL_v3_multiple.json')\") ./answers/BFCL_v3_multiple.json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662fd953",
   "metadata": {},
   "source": [
    "To make things a bit cleaner, we'll create answer files for the relevance and irrelevance datasets as well. This will make it easier to combine for our training dataset as our mix-in will require a few ground truth examples.\n",
    "We'll emulate the ground truth response structure the BFCL team is using, and return this for irrelevance answers:\n",
    "```json\n",
    "{\"id\": \"irrelevance_13\", \"ground_truth\": []}\n",
    "```\n",
    "\n",
    "and this for relevance answers, by picking a random function name from the list. Note that they \"don't check for the correctness of the function call in this category (eg, correct parameter value).\" so we'll use placeholders for the actual ground truth answers. Remember we're just doing this for labeled mix-in training data to hint our teacher model during distillation:\n",
    "```json\n",
    "{\"id\": \"live_relevance_5-5-0\", \"ground_truth\": [{\"get_copyright_info\": {\"copyright_content\": [\"The specific content that is claimed to be copyrighted.\"], \"copyright_holder\": [\"The name of the individual or organization that holds the copyright.\"], \"confidence_score\": [0.8]}}]}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340f0048",
   "metadata": {},
   "source": [
    "### Steps to build training data set:\n",
    "\n",
    "\n",
    "* using pandas, pick a random 50% of the records from the question dataset and put those into a subdirectory called `training`. Put the remaining in a subdirectory called `eval`.\n",
    "* for the training data, create a directory called `mix_in` that will hold our labeled questions we'll use in our training dataset. Put 10% of the record for each dataset into this folder and keep the rest where they are.\n",
    "* Build bedrock invoke api prompt with tool calling, using the questions\n",
    "* for the mix-in data, attach ground truth answers to these by looking up the answer in the answer dataset and entering an assistant response with the assistant's correct answer.\n",
    "* Combine all of the training data, including mix-in data, into a single `jsonl` file and in a format compatible with what's required for the bedrock distillation service format and save in the training directory with `.jsonl` extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97899e15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bfde9269",
   "metadata": {},
   "source": [
    "Let's take a look at teach of the files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75783a55",
   "metadata": {},
   "source": [
    "For our distillation, we're going to use a 10% mix-in of labeled data to hint to the teacher model what we would expect a good response to look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ab6d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "skip_dir = os.path.dirname(parent_dir)\n",
    "sys.path.append(skip_dir)\n",
    "from utils import read_jsonl_to_dataframe\n",
    "\n",
    "splits = {'train': 'squad_v2/train-00000-of-00001.parquet', 'validation': 'squad_v2/validation-00000-of-00001.parquet'}\n",
    "df_train = pd.read_parquet(\"hf://datasets/rajpurkar/squad_v2/\" + splits[\"train\"])\n",
    "df_eval = pd.read_parquet(\"hf://datasets/rajpurkar/squad_v2/\" + splits[\"validation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ee8fe9",
   "metadata": {},
   "source": [
    "## Advanced System Prompt Engineering\n",
    "\n",
    "This section implements a specialized system prompt for [Amazon Nova Foundation models](https://docs.aws.amazon.com/nova/latest/userguide/prompting.html). The prompt engineering focuses on:\n",
    "\n",
    "1. **Function Selection**: Accurate identification of the appropriate function to call\n",
    "2. **Parameter Extraction**: Precise extraction of required parameters from user queries\n",
    "3. **Structured Output**: JSON-based response format for consistent parsing\n",
    "4. **Edge Case Handling**: Explicit handling of irrelevant queries and parameter validation\n",
    "\n",
    "### System Prompt JSON Schema\n",
    "The system prompt leverages the following formatting to support function calling with parameters. In the following cells we'll build helper functions to parse these out to measure function calling accuracy. This style of prompting is specific to Amazon Nova and will provide the best accuracy for function calling use cases.\n",
    "- **Function Selection Logic**: Accurate identification of the most appropriate function\n",
    "- **Parameter Validation**: Ensuring all required parameters are properly extracted\n",
    "- **Schema Compliance**: Adherence to function signature requirements\n",
    "- **Extensibility**: Support for complex nested parameters and multiple function calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277ad88d",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "  \"query\": \"What's the weather like in San Francisco today?\",\n",
    "  \"function_call\": {\n",
    "    \"name\": \"get_weather\",\n",
    "    \"parameters\": {\n",
    "      \"location\": \"San Francisco\",\n",
    "      \"unit\": \"celsius\",\n",
    "      \"date\": \"today\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574f3c74",
   "metadata": {},
   "source": [
    "## Data Processing Pipeline Implementation\n",
    "\n",
    "The evaluation of distilled models using Bedrock native tools requires 3 different datasets, all using our squad dataset.\n",
    "1. **Distillation dataset.** This dataset will be used to fine-tune Nova Lite. Here we're using a 10% mix-in, so 10% of the records will include ground-truth answers and the rest will not. For more information on these best practices please visit [our documentation on this topic](https://docs.aws.amazon.com/nova/latest/userguide/custom-distill-prepare.html)\n",
    "2. **Batch Inference dataset.** This will be the hold out of records we'll use for evaluating each model's performance. We'll use this dataset in Batch Inference to get each model's responses.\n",
    "3. **Labeled dataset.** Using the same records from our batch inference dataset, we'll create a labeled dataset that includes the correct answers. We'll use this in our Evaluation job to measure each model's response to the ground-truth answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4943339e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_answer_structure(answers_dict):\n",
    "    \"\"\"\n",
    "    Parse different formats of answer dictionaries and extract text and start positions.\n",
    "    Returns lists of texts and start positions.\n",
    "    \"\"\"\n",
    "    # Case 1: NumPy arrays with direct keys\n",
    "    if 'text' in answers_dict and isinstance(answers_dict['text'], np.ndarray):\n",
    "        texts = answers_dict['text'].tolist()\n",
    "        starts = answers_dict['answer_start'].tolist()\n",
    "        \n",
    "    # Case 2: Lists or single values with direct keys\n",
    "    elif 'text' in answers_dict:\n",
    "        texts = answers_dict['text'] if isinstance(answers_dict['text'], list) else [answers_dict['text']]\n",
    "        starts = answers_dict['answer_start'] if isinstance(answers_dict['answer_start'], list) else [answers_dict['answer_start']]\n",
    "        \n",
    "    # Case 4: String JSON that needs parsing (handled in calling function)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown answer format: {answers_dict}\")\n",
    "        \n",
    "    return texts, starts\n",
    "\n",
    "def create_xml_answer(row, no_answer_text='I could not find an exact answer to the question.'):\n",
    "    \"\"\"\n",
    "    takes a pandas df row and parses the 'answers' column XML answers\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Handle answers as string (JSON) if needed\n",
    "        answers_dict = row['answers']\n",
    "        if isinstance(answers_dict, str):\n",
    "            import json\n",
    "            answers_dict = json.loads(answers_dict)\n",
    "            \n",
    "        # Parse answer structure using our helper function\n",
    "        texts, starts = parse_answer_structure(answers_dict)\n",
    "        context = row['context']\n",
    "        \n",
    "        # Split context into sentences more accurately\n",
    "        sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\\s', context)\n",
    "        \n",
    "        # Build XML structure\n",
    "        xml_parts = ['<answer>']\n",
    "        \n",
    "        if len(texts) > 0:\n",
    "            for i, (text, start) in enumerate(zip(texts, starts)):\n",
    "                xml_parts.append('<answer_part>')\n",
    "                xml_parts.append('<text>')\n",
    "                xml_parts.append(str(text))\n",
    "                xml_parts.append('</text>')\n",
    "                xml_parts.append('<sources>')\n",
    "                \n",
    "                # Find the sentence containing the answer based on the start position\n",
    "                char_count = 0\n",
    "                source_sentence = \"No relevant source found\"\n",
    "                for sentence in sentences:\n",
    "                    sentence_len = len(sentence) + 1  # +1 for the space after sentence\n",
    "                    if char_count <= int(start) < (char_count + sentence_len):\n",
    "                        source_sentence = sentence.strip()\n",
    "                        break\n",
    "                    char_count += sentence_len\n",
    "                \n",
    "                xml_parts.append(f'<source>{source_sentence}</source>')\n",
    "                xml_parts.append('</sources>')\n",
    "                xml_parts.append('</answer_part>')\n",
    "        \n",
    "            xml_parts.append('</answer>')\n",
    "        else: # use no answer text\n",
    "            xml_parts.append(f\"<answer_part>\\n<text>\\n{no_answer_text}\\n</text>\\n</answer_part></answer>\")\n",
    "        return '\\n'.join(xml_parts)\n",
    "    except Exception as e:\n",
    "        return f\"<answer>\\n<error>Error generating XML: {str(e)}</error>\\n</answer>\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5355fa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bedrock_payload(row, model_type=\"conversation\", system_prompt=None, include_answer=False, additional_params=None):\n",
    "    \"\"\"\n",
    "    Create a payload dictionary for Amazon Bedrock batch inference API requests.\n",
    "    Batch inference uses the invoke api.\n",
    "    \n",
    "    Args:\n",
    "        row: A row from the pandas DataFrame containing context, question, and optionally answers\n",
    "        model_type: The type of model payload to create (\"conversation\" or \"invoke\")\n",
    "        system_prompt: The system message to include (for conversation-based models)\n",
    "        include_answer: Whether to include the answer in the conversation (for evaluation)\n",
    "        additional_params: Dictionary of additional parameters to include in the payload\n",
    "    \n",
    "    Returns:\n",
    "        dict: A formatted payload dictionary ready for Bedrock batch inference API\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract needed information\n",
    "        context = row['context']\n",
    "        question = row['question']\n",
    "        \n",
    "        # Create the user prompt with context and question\n",
    "        user_prompt = f\"\"\"<context>{context}</context> <question>{question}</question>\"\"\"\n",
    "        \n",
    "        # Get the answer if needed\n",
    "        assistant_response = create_xml_answer(row) if include_answer else None\n",
    "        \n",
    "        # Create appropriate payload based on model_type\n",
    "        if model_type == \"conversation\":\n",
    "            \n",
    "            payload = {\n",
    "                \"schemaVersion\": \"bedrock-conversation-2024\",\n",
    "                \"system\": [{\"text\": system_prompt}] if system_prompt else [],\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [{\"text\": user_prompt}]\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            # Add assistant response if needed (for evaluation)\n",
    "            if include_answer and assistant_response:\n",
    "                payload[\"messages\"].append({\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": [{\"text\": assistant_response}]\n",
    "                })\n",
    "                \n",
    "        elif model_type == \"invoke\":\n",
    "            # For basic invoke request (non-conversation models like Titan, etc.)\n",
    "            payload = {\n",
    "                \"system\": [{\"text\": system_prompt}] if system_prompt else [],\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [{\"text\": user_prompt}]\n",
    "                    }\n",
    "                ],\n",
    "                \"inferenceConfig\":{ \n",
    "                    # \"maxTokens\": int, \n",
    "                    \"temperature\": .1, \n",
    "                    \"topP\": .9, \n",
    "                    \"topK\": 50, \n",
    "                    \"stopSequences\": ['</answer>']\n",
    "                }\n",
    "            }\n",
    "            if include_answer and assistant_response:\n",
    "                payload[\"messages\"].append({\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": [{\"text\": assistant_response}]\n",
    "                })\n",
    "            \n",
    "            # Add optional parameters specific to invoke requests\n",
    "            if additional_params:\n",
    "                payload.update(additional_params)\n",
    "                \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model_type: {model_type}\")\n",
    "            \n",
    "        # Add any additional parameters passed\n",
    "        if additional_params and model_type == \"conversation\":\n",
    "            # For conversation models, additional params might need to be added at the root level\n",
    "            for key, value in additional_params.items():\n",
    "                if key not in payload:\n",
    "                    payload[key] = value\n",
    "                    \n",
    "        return payload\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating payload for row: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2aa386",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch_inf_record(row, system_prompt, include_answer=False):\n",
    "    \"\"\"\n",
    "    Takes a pandas df row and creates a jsonl record for batch inference\n",
    "    \"\"\"\n",
    "    conversation = create_bedrock_payload(\n",
    "                                row=row, \n",
    "                                system_prompt=system_prompt, \n",
    "                                model_type=\"invoke\", \n",
    "                                additional_params={},\n",
    "                                include_answer=include_answer)\n",
    "    return {\n",
    "        \"recordId\": row['id'],\n",
    "        \"modelInput\": conversation\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d64f297",
   "metadata": {},
   "source": [
    "### Including non-answers\n",
    "Any citations use case will need to support scenarios where a correct answer is not possible given the passages available.\n",
    "To this end, we'll make half of our training dataset include non-answers, and half will include examples with answers.\n",
    "Bedrock distillation jobs can have a maximum of 15,000 records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe80bb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to create a new column\n",
    "# Filter for empty answers\n",
    "empty_answers_df = df_train[df_train['answers'].apply(lambda x: \n",
    "    len(x['text']) == 0 and len(x['answer_start']) == 0)]\n",
    "\n",
    "# Filter for rows with actual answers\n",
    "with_answers_df = df_train[df_train['answers'].apply(lambda x: len(x['text']) > 0)]\n",
    "\n",
    "# take 7500 of each dataframe and combine to use in distillation. \n",
    "df_train_revised = pd.concat([\n",
    "    empty_answers_df.sample(n=7500, random_state=42), \n",
    "    with_answers_df.sample(n=7500, random_state=42)], ignore_index=True) # max 15k for bedrock distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41416c80",
   "metadata": {},
   "source": [
    "As stated earlier, it is a best practice to include a ground truth answer for ~10% of the total training set. We will take a random sample of 10% and use our `create_bedrock_payload` with include_anwer set to True. The remaining 90% we leave out the ground truth answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39bf45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_count = len(df_train_revised)\n",
    "ground_truth_included = 0.1 * row_count\n",
    "\n",
    "# here we'll take 10% of our training data set and add the answers\n",
    "training_data_with_gt_df = df_train_revised.sample(n=int(ground_truth_included), random_state=17)\n",
    "\n",
    "# next we'll drop our ground truth examples so as not to mix with our labels excluding answers.\n",
    "training_data_without_gt_df = df_train_revised.drop(training_data_with_gt_df.index)\n",
    "\n",
    "# next we'll build our training data with ground truth\n",
    "training_data_with_gt_df['conversation'] = training_data_with_gt_df.apply(lambda row: create_bedrock_payload(row=row, model_type=\"conversation\", system_prompt=system_prompt, include_answer=True), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6e344d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# then we'll build our training data without ground truth\n",
    "training_data_without_gt_df['conversation'] = training_data_without_gt_df.apply(lambda row: create_bedrock_payload(row=row, model_type=\"conversation\", system_prompt=system_prompt, include_answer=False), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf64168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'll concatenate the dataframes\n",
    "final_training_dataset = pd.concat([training_data_with_gt_df, training_data_without_gt_df], axis=0, ignore_index=True).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10b3bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we'll output to .jsonl to use in distillation job\n",
    "final_training_dataset['conversation'].to_json('distillation_data.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e4b86c",
   "metadata": {},
   "source": [
    "## Batch Inference Dataset Creation\n",
    "\n",
    "Now that our distillation data set is created, we'll move on to creating our batch inference dataset.\n",
    "Because we'll also be using the same dataset (with labeled answers) for our evaluation jobs, Bedrock Evaluations will only handle a maxium of 1000 records.\n",
    "We'll use 500 total rows our data set, as this is a sufficient number for evaluation and the right balance between processing time and proper evaluation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7469d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_empty_answers_df = df_eval[df_eval['answers'].apply(lambda x: \n",
    "    len(x['text']) == 0 and len(x['answer_start']) == 0)]\n",
    "\n",
    "# Filter for rows with actual answers\n",
    "eval_with_answers_df = df_eval[df_eval['answers'].apply(lambda x: len(x['text']) > 0)]\n",
    "\n",
    "batch_inf_df = pd.concat([\n",
    "    eval_empty_answers_df.sample(n=250, random_state=15), \n",
    "    eval_with_answers_df.sample(n=250, random_state=15)], ignore_index=True)\n",
    "\n",
    "\n",
    "batch_inf_df.apply(lambda row: create_batch_inf_record(row, system_prompt), axis=1).to_json('batch_inf_data.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fd681e",
   "metadata": {},
   "source": [
    "## Labeled Dataset for BYOI Bedrock Evaluation\n",
    "This section creates a labeled dataset by applying our `create_batch_inf_record` method on each row and setting `include_answer` to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d625d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_inf_df.apply(lambda row: create_batch_inf_record(row, system_prompt=system_prompt, include_answer=True), axis=1).to_json('labeled_data.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ba4edf",
   "metadata": {},
   "source": [
    "### Datasets Created\n",
    "By now you should see 3 `.jsonl` files:\n",
    "1. distillation_data.jsonl. This is the dataset we'll use for distillation.\n",
    "2. batch_inf_data.jsonl. This is the dataset we'll use to run inference on all of our models, including our distilled one.\n",
    "3. labeled_data.jsonl. This is the dataset we'll use to evaluate each model's performance against the ground truth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4faab9",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "Proceed to [02_distill.ipynb](02_distill.ipynb) to:\n",
    "1. Submit a distillation job using our distillation dataset\n",
    "2. Create a provisioned throughput endpoint to hose our distilled model.\n",
    "\n",
    "You can now move on to `02_distill.ipynb` where we'll kick of our distillation job!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03e8fcd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "function-calling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
