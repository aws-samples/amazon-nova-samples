{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation and Performance Analysis\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook represents the final stage in our model distillation journey, where we evaluate the performance of our distilled model against the original model. We leverage Amazon Bedrock's RAG Evaluation capabilities with Bring Your Own Inference (BYOI) support to conduct a comprehensive assessment of model quality and citation capabilities.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand:\n",
    "- How to structure and format evaluation datasets for BYOI evaluation\n",
    "- Advanced evaluation metrics for assessing RAG system performance\n",
    "- Techniques for analyzing citation quality and knowledge transfer effectiveness\n",
    "\n",
    "## Evaluation Metrics Deep Dive\n",
    "\n",
    "Our evaluation framework uses several sophisticated metrics designed for RAG systems:\n",
    "\n",
    "### Citation Quality Metrics\n",
    "- **Citation Coverage**: Evaluates how comprehensively the model utilizes available context. This metric helps identify if the model is under-utilizing or over-relying on certain passages.\n",
    "\n",
    "### Response Quality Metrics\n",
    "- **Correctness**: Assesses factual accuracy by comparing generated content against ground truth responses and source documents.\n",
    "- **Completeness**: Measures response thoroughness relative to the question's requirements and available context.\n",
    "- **Faithfulness**: Evaluates how well responses align with provided context, detecting potential hallucinations or unsupported claims.\n",
    "- **Helpfulness**: Analyzes practical utility by considering factors like clarity, relevance, and actionability.\n",
    "- **Logical Coherence**: Examines response consistency and reasoning quality, particularly important for complex queries.\n",
    "\n",
    "> **Advanced Note**: These metrics are calculated using specialized evaluator models that perform semantic analysis rather than simple string matching, enabling nuanced assessment of model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Ensure you have completed the previous notebooks in this sequence:\n",
    "1. `01_prepare_data.ipynb`: Data preparation and formatting\n",
    "2. `02_distill.ipynb`: Model distillation process\n",
    "3. `03_batch_inference.ipynb`: Batch inference implementation\n",
    "\n",
    "Additional requirements:\n",
    "- An active AWS account with appropriate permissions\n",
    "- Amazon Bedrock access enabled in your preferred region ([Enable Bedrock models](https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html))\n",
    "- An S3 bucket for storing evaluation data and results\n",
    "- An IAM role with necessary permissions for S3 and Bedrock ([IAM setup guide](https://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html))\n",
    "- RAG system outputs formatted according to the BYOI specification\n",
    "\n",
    "> **Important**: The evaluation process requires access to Amazon Bedrock evaluator models. Ensure these are enabled in your account and you have sufficient [quotas](https://docs.aws.amazon.com/bedrock/latest/userguide/quotas.html) for your evaluation workload."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "Let's set up our evaluation pipeline by first importing required dependencies and configuring our environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upgrade Boto3\n",
    "!pip install --upgrade boto3 matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries and setup environment\n",
    "import boto3\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "print(boto3.__version__)\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "skip_dir = os.path.dirname(parent_dir)\n",
    "sys.path.append(skip_dir)\n",
    "from utils import read_jsonl_to_dataframe, upload_training_data_to_s3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate Evaluation Results\n",
    "We'll use a helper script called `eval_results.py` to process all of the results files we have now and compare the model outputs to the ground truth for each question.\n",
    "\n",
    "This will generate evaluation_results.csv which we'll plot to analyze later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python eval_results.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load CSV into DataFrame\n",
    "function_calling_results = pd.read_csv('evaluation_results.csv')\n",
    "function_calling_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bar chart for specific columns\n",
    "columns_to_plot = ['prompting_approach', 'model_type', 'model_name', 'overall_accuracy']\n",
    "\n",
    "# Sort by overall_accuracy in descending order\n",
    "df_sorted = function_calling_results.sort_values(by='overall_accuracy', ascending=False)\n",
    "\n",
    "ax = df_sorted[columns_to_plot].plot(kind='bar', figsize=(10, 6))\n",
    "plt.title('Function Calling Accuracy by Model (BFCL Dataset)')\n",
    "plt.xlabel('Eval')\n",
    "plt.ylabel('Values')\n",
    "\n",
    "# Create custom x-axis labels combining prompting_approach, model_type, and model_name\n",
    "labels = [f\"{row['prompting_approach']}\\n{row['model_type']}\\n{row['model_name']}\" \n",
    "          for _, row in df_sorted.iterrows()]\n",
    "plt.xticks(range(len(df_sorted)), labels, rotation=45, ha='right')\n",
    "\n",
    "# Add value labels on bars\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how to:\n",
    "\n",
    "1. **Structure Evaluation Data**: Format RAG system outputs for comprehensive evaluation using the BYOI specification\n",
    "2. **Configure Evaluation Jobs**: Set up secure IAM roles and configure evaluation parameters\n",
    "3. **Execute Evaluations**: Run parallel evaluations of multiple models using Amazon Bedrock\n",
    "4. **Analyze Results**: Interpret evaluation metrics to assess model performance\n",
    "\n",
    "This completes our four-notebook series on model distillation for citation-aware RAG systems. Through this series, we've covered:\n",
    "- Data preparation and formatting\n",
    "- Model distillation techniques\n",
    "- Batch inference implementation\n",
    "- Comprehensive model evaluation\n",
    "\n",
    "For more information, explore:\n",
    "- [Amazon Bedrock Evaluation Documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/evaluation.html)\n",
    "- [RAG Best Practices Guide](https://docs.aws.amazon.com/bedrock/latest/userguide/rag-best-practices.html)\n",
    "- [Advanced Model Evaluation Techniques](https://docs.aws.amazon.com/bedrock/latest/userguide/evaluation-metrics.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "02_function_calling-tDu3jLPM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
