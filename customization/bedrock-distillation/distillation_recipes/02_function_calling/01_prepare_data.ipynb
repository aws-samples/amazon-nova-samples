{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "280d3606",
   "metadata": {},
   "source": [
    "# Model Distillation for Function Calling\n",
    "\n",
    "This notebook is part of a series demonstrating advanced model distillation techniques for creating specialized, function-calling-aware models. The goal is to distill the knowledge from a large language model (Amazon Nova Premier) into a smaller, more efficient model while maintaining high-quality function calling capabilities.\n",
    "\n",
    "## Learning Objectives\n",
    "- Prepare training data for function calling model distillation\n",
    "- Design structured output formats for consistent function parameter generation\n",
    "- Implement function selection and parameter extraction\n",
    "- Create evaluation datasets for measuring function calling accuracy\n",
    "\n",
    "## Dataset: Berkeley Function Calling Leaderboard (BFCL) V2 Live\n",
    "We use the Berkeley Function Calling Leaderboard (BFCL) V2 Live dataset as our base dataset. This dataset is particularly suitable for function-calling model training because:\n",
    "\n",
    "1. Contains 2,251 question-function-answer pairs total\n",
    "2. Provides diverse function calling scenarios:\n",
    "   - 258 simple calls\n",
    "   - 1,053 multiple parameter calls\n",
    "   - 16 parallel function calls\n",
    "   - 24 parallel multiple parameter calls\n",
    "   - 882 irrelevance detection cases\n",
    "   - 18 relevance detection cases\n",
    "3. Offers complexity with an average of 3 function choices per entry (maximum 37)\n",
    "4. Includes parameter diversity with an average of 4 parameters per function (maximum 28)\n",
    "\n",
    "The dataset is processed and stored in optimized formats for efficient model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3adadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upgrade boto3 \n",
    "%pip install --upgrade pip --quiet\n",
    "%pip install boto3 --upgrade --quiet\n",
    "%pip install bcfl-eval --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdea093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72c875c",
   "metadata": {},
   "source": [
    "We need to set the project root so results are put in the correct location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b61383",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['BFCL_PROJECT_ROOT'] = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d244e4eb",
   "metadata": {},
   "source": [
    "If you're running this on your own machine, enter your AWS access keys in an .env file.\n",
    "Uncomment the below cell to copy down an example .env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b125616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # set up environment file\n",
    "# %cp $(python -c \"import bfcl_eval; print(bfcl_eval.__path__[0])\")/.env.example $BFCL_PROJECT_ROOT/.env\n",
    "# # Fill in necessary values in `.env`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8378ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download sample data\n",
    "# %cp $(python -c \"import bfcl_eval, pathlib; print(pathlib.Path(bfcl_eval.__path__[0]) / 'test_case_ids_to_generate.json.example')\") $BFCL_PROJECT_ROOT/test_case_ids_to_generate.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01188219",
   "metadata": {},
   "source": [
    "For this example we're going to use a combination of v3_simple, v3_multiple, v3_live_relevance, and v3_irrelevance to train and evaluate with. For more information on these categories and their intents, please visit the [official BFCL documentation](https://huggingface.co/datasets/gorilla-llm/Berkeley-Function-Calling-Leaderboard)\n",
    "\n",
    "Let's move these to our local directory so we can begin preparing the data for distillation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d505320",
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir questions\n",
    "%cp $(python -c \"import bfcl_eval, pathlib; print(pathlib.Path(bfcl_eval.__path__[0]) / 'data' / 'BFCL_v3_simple.json')\") ./questions/BFCL_v3_simple.json\n",
    "%cp $(python -c \"import bfcl_eval, pathlib; print(pathlib.Path(bfcl_eval.__path__[0]) / 'data' / 'BFCL_v3_multiple.json')\") ./questions/BFCL_v3_multiple.json\n",
    "%cp $(python -c \"import bfcl_eval, pathlib; print(pathlib.Path(bfcl_eval.__path__[0]) / 'data' / 'BFCL_v3_irrelevance.json')\") ./questions/BFCL_v3_irrelevance.json\n",
    "%cp $(python -c \"import bfcl_eval, pathlib; print(pathlib.Path(bfcl_eval.__path__[0]) / 'data' / 'BFCL_v3_live_relevance.json')\") ./questions/BFCL_v3_live_relevance.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2b47ba",
   "metadata": {},
   "source": [
    "Now will grab the corresponding answers. For the simple and multiple datasets, we are provided possible answers and we'll use these for our mix-in labels.\n",
    "\n",
    "Per the BFCL documentation, the correct answer for any question in the `BFCL_v3_irrelevance` datset is an empty list of functions, as these are design specifically to test the model's ability to correctly identify zero possible functions that are relevant.\n",
    "\n",
    "The correct answer for any question in the `BFCL_v3_live_relevance` dataset is \"at least one\" function call returned.\n",
    "\n",
    "Here's an excerpt from [their documentation](https://huggingface.co/datasets/gorilla-llm/Berkeley-Function-Calling-Leaderboard):\n",
    "\n",
    "> **Irrelevance Detection (875):** The scenario where none of the function choices provided are relevant to the user query and none should be invoked. We expect the model to not output a function call; the model can either output a message explaining why the function provided are not relevant or simply output a non-function call response (e.g., an empty list).\n",
    "\n",
    "> **Relevance Detection (41):** The opposite of irrelevance detection. The scenario where at least one of the function choices provided are relevant to the user query and should be invoked, but the way the user prompt or the function doc is stated means that there could be infinitely many correct function calls and impossible to use a pre-defined possible answer set to evaluate. We expect the model to output some function call (one or multiple) that is relevant to the user query; we don't check for the correctness of the function call in this category (eg, correct parameter value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d625a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir answers\n",
    "%cp $(python -c \"import bfcl_eval, pathlib; print(pathlib.Path(bfcl_eval.__path__[0]) / 'data' / 'possible_answer' / 'BFCL_v3_simple.json')\") ./answers/BFCL_v3_simple.json\n",
    "%cp $(python -c \"import bfcl_eval, pathlib; print(pathlib.Path(bfcl_eval.__path__[0]) / 'data' / 'possible_answer' / 'BFCL_v3_multiple.json')\") ./answers/BFCL_v3_multiple.json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662fd953",
   "metadata": {},
   "source": [
    "To make things a bit cleaner, we'll manually create answer files for the relevance and irrelevance datasets as well. This will make it easier to combine for our training dataset as our mix-in will require a few ground truth examples.\n",
    "We'll emulate the ground truth response structure the BFCL team is using, and return this for irrelevance answers:\n",
    "```json\n",
    "{\"id\": \"irrelevance_13\", \"ground_truth\": []}\n",
    "```\n",
    "\n",
    "and this for relevance answers, by picking a random function name from the list. Note that they \"don't check for the correctness of the function call in this category (eg, correct parameter value).\" so we'll use placeholders for the actual ground truth answers. Remember we're just doing this for labeled mix-in training data to hint our teacher model during distillation:\n",
    "```json\n",
    "{\"id\": \"live_relevance_5-5-0\", \"ground_truth\": [{\"get_copyright_info\": {\"copyright_content\": [\"The specific content that is claimed to be copyrighted.\"], \"copyright_holder\": [\"The name of the individual or organization that holds the copyright.\"], \"confidence_score\": [0.8]}}]}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77af866",
   "metadata": {},
   "source": [
    "Let's process the irrelevance answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900d3dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "irrelevance_answers = []\n",
    "with open(\"questions/BFCL_v3_irrelevance.json\", \"r\") as samples:\n",
    "    for sample in samples.readlines():\n",
    "        id = json.loads(sample)['id']\n",
    "        answer = {\n",
    "            \"id\": id,\n",
    "            \"ground_truth\": []\n",
    "        }\n",
    "        irrelevance_answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ac68e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"answers/BFCL_v3_irrelevance.json\", \"w\") as output_file:\n",
    "    for answer in irrelevance_answers:\n",
    "        json.dump(answer, output_file)\n",
    "        output_file.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2450473f",
   "metadata": {},
   "source": [
    "Now, lets process the relevance answers. We don't have an answer file for this already, but remember, BFCL counts any non-zero answer as correct. We'll use a random function from the list of functions provided in the example.\n",
    "\n",
    "To create answers with the right data types, we'll build a help function to account for all of the different answer scenarios in the relevance dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48eb343c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_answer(function: dict) -> dict:\n",
    "    function_params = function['parameters']['properties']\n",
    "    required_params = function['parameters']['required']\n",
    "    param_values = {}\n",
    "\n",
    "\n",
    "    for p in function_params.keys():\n",
    "        data_type = function_params[p]['type']\n",
    "        # check if default availablt and use that\n",
    "        if 'default' in function_params[p].keys():\n",
    "            param_values[p] = [function_params[p]['default']]\n",
    "            # print(param_values)\n",
    "\n",
    "        if p in required_params:\n",
    "            if data_type == 'string':\n",
    "                if 'enum' in function_params[p].keys():\n",
    "                    enums = function_params[p]['enum']\n",
    "                    # print(\"found enums\", function_params[p]['enum'])\n",
    "                    param_values[p] = [enums[random.randint(0,len(enums)-1)]]\n",
    "                else:\n",
    "                    param_values[p] = ['test parameter value']\n",
    "                # param_values[p] = ['test string']\n",
    "            \n",
    "        # else, create a value based on the data type\n",
    "\n",
    "    answer = {\n",
    "        function['name']: param_values\n",
    "    }\n",
    "    return answer\n",
    "    # print(\"answer\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e838b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "relevance_answers = []\n",
    "with open(\"questions/BFCL_v3_live_relevance.json\", \"r\") as samples:\n",
    "    for sample in samples.readlines():\n",
    "        s = json.loads(sample)\n",
    "        id = s['id']\n",
    "\n",
    "        chosen_function = s['function'][random.randint(0,len(s['function'])-1)]\n",
    "        relevance_answers.append({\n",
    "            \"id\": id,\n",
    "            \"ground_truth\": [generate_answer(chosen_function)]\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c278e6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"answers/BFCL_v3_live_relevance.json\", \"w\") as output_file:\n",
    "    for answer in relevance_answers:\n",
    "        json.dump(answer, output_file)\n",
    "        output_file.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340f0048",
   "metadata": {},
   "source": [
    "## Data Preparation Steps\n",
    "\n",
    "1. **Data Splitting**\n",
    "   - Split the BFCL question datasets randomly:\n",
    "     - 50% into `training` directory\n",
    "     - 50% into `eval` directory\n",
    "   \n",
    "2. **Mix-in Data Creation**\n",
    "   - From the training data:\n",
    "     - Create `mix_in` subdirectory\n",
    "     - Move 10% of records into mix_in\n",
    "     - Keep 90% in training\n",
    "   \n",
    "3. **Ground Truth Integration**\n",
    "   - For mix-in data:\n",
    "     - Look up corresponding answers in answer dataset\n",
    "     - Add as ground truth assistant responses\n",
    "   \n",
    "4. **Prompt Engineering**\n",
    "   - Build Bedrock invoke API prompts with tool calling functionality\n",
    "   \n",
    "5. **Data Consolidation**\n",
    "   - Combine all training data (including mix-in)\n",
    "   - Format as JSONL for Bedrock distillation service\n",
    "   - Save in training directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97899e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs('training', exist_ok=True)\n",
    "os.makedirs('eval', exist_ok=True)\n",
    "\n",
    "# Load and combine all question datasets\n",
    "question_files = [\n",
    "    'questions/BFCL_v3_simple.json',\n",
    "    'questions/BFCL_v3_multiple.json',\n",
    "    'questions/BFCL_v3_irrelevance.json',\n",
    "    'questions/BFCL_v3_live_relevance.json'  # Note the double dot in filename\n",
    "]\n",
    "\n",
    "all_questions = []\n",
    "for file in question_files:\n",
    "    print(\"reading... \", file)\n",
    "    with open(file, 'r') as f:\n",
    "        for answer in f.readlines():\n",
    "            # print(question)\n",
    "            all_questions.append(json.loads(answer))\n",
    "        # questions = json.load(contents)\n",
    "        # all_questions.extend(questions)\n",
    "\n",
    "# Convert to DataFrame for easier manipulation\n",
    "df_questions = pd.DataFrame(all_questions)\n",
    "\n",
    "# Randomly split into training (50%) and eval (50%)\n",
    "df_train = df_questions.sample(frac=0.5, random_state=42)\n",
    "df_eval = df_questions.drop(df_train.index)\n",
    "\n",
    "# Save splits to respective directories\n",
    "df_train.to_json('training/questions.json', orient='records', indent=2)\n",
    "df_eval.to_json('eval/questions.json', orient='records', indent=2)\n",
    "\n",
    "print(f\"Training set size: {len(df_train)}\")\n",
    "print(f\"Evaluation set size: {len(df_eval)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5355fa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mix_in directory\n",
    "os.makedirs('training/mix_in', exist_ok=True)\n",
    "\n",
    "# Select 10% of training data for mix-in\n",
    "df_mix_in = df_train.sample(frac=0.1, random_state=42)\n",
    "df_train_remaining = df_train.drop(df_mix_in.index)\n",
    "\n",
    "# Save mix-in and remaining training data\n",
    "df_mix_in.to_json('training/mix_in/questions.json', orient='records', indent=2)\n",
    "df_train_remaining.to_json('training/questions.json', orient='records', indent=2)\n",
    "\n",
    "print(f\"Mix-in set size: {len(df_mix_in)}\")\n",
    "print(f\"Remaining training set size: {len(df_train_remaining)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2aa386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load answer datasets\n",
    "answer_files = {\n",
    "    'simple': 'answers/BFCL_v3_simple.json',\n",
    "    'multiple': 'answers/BFCL_v3_multiple.json',\n",
    "    'relevance': 'answers/BFCL_v3_live_relevance.json',\n",
    "    'irrelevance': 'answers/BFCL_v3_irrelevance.json'\n",
    "}\n",
    "\n",
    "all_answers = []\n",
    "for dataset_type, file in answer_files.items():\n",
    "    with open(file, 'r') as f:\n",
    "        for answer in f.readlines():\n",
    "            all_answers.append(json.loads(answer))\n",
    "        # all_answers.update({a['id']: a['ground_truth'] for a in answers})\n",
    "\n",
    "# Convert to DataFrame for easier manipulation\n",
    "df_all_answers = pd.DataFrame(all_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c342f158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add ground truth answers to mix-in data\n",
    "# Merge the dataframes on the 'id' column\n",
    "df_mix_in = df_mix_in.merge(\n",
    "    df_all_answers[['id', 'ground_truth']], \n",
    "    on='id', \n",
    "    how='left'\n",
    ").rename(columns={'answer': 'ground_truth'})\n",
    "\n",
    "df_mix_in.to_json('training/mix_in/questions_with_answers.json', orient='records', indent=2)\n",
    "\n",
    "print(f\"Added ground truth answers to {len(df_mix_in)} mix-in records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d659634",
   "metadata": {},
   "source": [
    "By now we should have our mix-in record with answers. We'll be combining these with the delta for training records without answers to form our final distillation training data set. However, we still have to form the dataset to work with bedrock along with our Nova prompt.\n",
    "\n",
    "We'll begin the final formatting now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23c8b23",
   "metadata": {},
   "source": [
    "First we'll start with our system prompt, as this will contain the tools available for the agent to call. We're following the best practices laid out here for agent calling: https://docs.aws.amazon.com/bedrock/latest/userguide/distillation-prepare-datasets.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1939410",
   "metadata": {},
   "source": [
    "## Prepare distillation training data with Prompt-Only Function Calling\n",
    "This is a prompting-only approach to tool calling that relies entirely on the system prompt to provide the available tools to the model to pick from. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c60cdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt for function calling\n",
    "def create_sys_prompt(tools) -> str:\n",
    "\n",
    "    SYSTEM_PROMPT = f\"\"\"You are an expert in composing functions. You are given a question and a set of possible functions. Based on the question, you will need to make one or more function/tool calls to achieve the purpose.\n",
    "If none of the functions can be used, point it out. If the given question lacks the parameters required by the function, also point it out.\n",
    "You should only return the function calls in your response.\n",
    "\n",
    "If you decide to invoke any of the function(s), you MUST put it in the format of [func_name1(params_name1=params_value1, params_name2=params_value2...), func_name2(params)]\n",
    "You SHOULD NOT include any other text in the response.\n",
    "\n",
    "At each turn, you should try your best to complete the tasks requested by the user within the current turn. Continue to output functions to call until you have fulfilled the user's request to the best of your ability. Once you have no more functions to call, the system will consider the current turn complete and proceed to the next turn or task.\n",
    "\"\"\"\n",
    "\n",
    "    default_system_prompt = (\n",
    "        SYSTEM_PROMPT\n",
    "        + \"\"\"\n",
    "    Here is a list of functions in JSON format that you can invoke.\\n{tools}\\n\n",
    "    \"\"\"\n",
    "    )\n",
    "    return default_system_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b2a179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper method for transforming list of functions from BFCL data set to bedrock tool spec\n",
    "from typing import List\n",
    "\n",
    "def transform_to_toolspec(input_data: List):\n",
    "    \"\"\"\n",
    "    Transform function calling format to toolSpec format.\n",
    "    \n",
    "    Args:\n",
    "        input_data (list): List of function definitions in the input format\n",
    "        \n",
    "    Returns:\n",
    "        list: List of function definitions in the toolSpec format\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    \n",
    "    for func in input_data:\n",
    "        # Extract the parameters object\n",
    "        parameters = func.get(\"parameters\", {})\n",
    "        \n",
    "        # Create the toolSpec structure\n",
    "        toolspec_item = {\n",
    "            \"toolSpec\": {\n",
    "                \"name\": func[\"name\"],\n",
    "                \"description\": func[\"description\"],\n",
    "                \"inputSchema\": {\n",
    "                    \"json\": {\n",
    "                        \"type\": \"object\",  # Convert \"dict\" to \"object\"\n",
    "                        \"properties\": parameters.get(\"properties\", {}),\n",
    "                        \"required\": parameters.get(\"required\", [])\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        result.append(toolspec_item)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ccd7e7",
   "metadata": {},
   "source": [
    "Here we'll be sure to fine-tune with the system prompt used for BFCL. If your evaluation framework is using a specific system prompt that represents your business, you would want to include that prompt in your fine-tuning so the model tuned to your business. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4e922a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_jsonl_record(row, use_tool_config=False,batch_inf_format=False):\n",
    "    \"\"\"\n",
    "    creates a jsonl record for bedrock distillation or batch inference formats\n",
    "    \"\"\"\n",
    "    \n",
    "    conversation = {}\n",
    "    \n",
    "    if use_tool_config:\n",
    "        conversation = {\n",
    "            \"schemaVersion\": \"bedrock-conversation-2024\",\n",
    "            \"system\": [\n",
    "                {\n",
    "                    \"text\": \"\"\"You are an expert in composing functions. You are given a question and a set of possible functions. Based on the question, you will need to make one or more function/tool calls to achieve the purpose.\n",
    "If none of the functions can be used, point it out. If the given question lacks the parameters required by the function, also point it out.\n",
    "You should only return the function calls in your response.\n",
    "\n",
    "If you decide to invoke any of the function(s), you MUST put it in the format of [func_name1(params_name1=params_value1, params_name2=params_value2...), func_name2(params)]\n",
    "You SHOULD NOT include any other text in the response.\n",
    "\n",
    "At each turn, you should try your best to complete the tasks requested by the user within the current turn. Continue to output functions to call until you have fulfilled the user's request to the best of your ability. Once you have no more functions to call, the system will consider the current turn complete and proceed to the next turn or task.\"\"\"\n",
    "                }\n",
    "            ],\n",
    "            \"messages\": [{\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": [\n",
    "                            {\n",
    "                                \"text\": row['question'][0][0]['content']\n",
    "                            }\n",
    "                            ]\n",
    "                        }],\n",
    "            \"toolConfig\": {\"tools\": transform_to_toolspec(row['function'])}\n",
    "        }\n",
    "    else:\n",
    "        conversation = {\n",
    "            \"schemaVersion\": \"bedrock-conversation-2024\",\n",
    "            \"system\": [\n",
    "                {\n",
    "                    \"text\": create_sys_prompt(tools=row['function'])\n",
    "                }\n",
    "            ],\n",
    "            \"messages\": [{\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": [\n",
    "                            {\n",
    "                                \"text\": row['question'][0][0]['content']\n",
    "                            }\n",
    "                            ]\n",
    "                        }],\n",
    "        }\n",
    "\n",
    "    if 'ground_truth' in row.keys():\n",
    "        conversation['messages'].append({\n",
    "                            \"role\": \"assistant\",\n",
    "                            \"content\": [\n",
    "                            {\n",
    "                                \"text\": f\"{json.dumps(row['ground_truth'])}\"\n",
    "                            }\n",
    "                            ]\n",
    "                        })\n",
    "    \n",
    "    if batch_inf_format:\n",
    "        return {\n",
    "            \"recordId\": row['id'],\n",
    "            \"modelInput\": conversation\n",
    "        }\n",
    "    else:\n",
    "        return conversation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faf61a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process regular training data without labels\n",
    "records = []\n",
    "with open('training/questions.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "    for item in train_data:\n",
    "        # print(item)\n",
    "        record = create_jsonl_record(\n",
    "            row=item\n",
    "        )\n",
    "        records.append(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a4bf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process mix-in data with ground truth answers\n",
    "with open('training/mix_in/questions_with_answers.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "    for item in train_data:\n",
    "        # print(item)\n",
    "        record = create_jsonl_record(\n",
    "            row=item\n",
    "        )\n",
    "        records.append(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a259327a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save combined training data as JSONL\n",
    "with open('training/bedrock_training_data_prompt_only.jsonl', 'w') as f:\n",
    "    for record in records:\n",
    "        f.write(json.dumps(record) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0298b700",
   "metadata": {},
   "source": [
    "## Prepare distillation training data with Tool Config\n",
    "This method is alternative to the prompt-only approach, where we include a seperate inference parameter called tool_config, and put our list of available tools in this configuration instead of the system prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d51eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process regular training data without labels\n",
    "records_tools_use = []\n",
    "with open('training/questions.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "    for item in train_data:\n",
    "        # print(item)\n",
    "        record = create_jsonl_record(\n",
    "            row=item,\n",
    "            use_tool_config=True\n",
    "        )\n",
    "        records_tools_use.append(record)\n",
    "\n",
    "# Process mix-in data with ground truth answers\n",
    "with open('training/mix_in/questions_with_answers.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "    for item in train_data:\n",
    "        # print(item)\n",
    "        record = create_jsonl_record(\n",
    "            row=item,\n",
    "            use_tool_config=True\n",
    "        )\n",
    "        records_tools_use.append(record)\n",
    "\n",
    "# Save combined training data as JSONL\n",
    "with open('training/bedrock_training_data_tool_config.jsonl', 'w') as f:\n",
    "    for record in records_tools_use:\n",
    "        f.write(json.dumps(record) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df17466b",
   "metadata": {},
   "source": [
    "## Prepare Evaluation Data\n",
    "Now we'll prepare our evaluation data set that we set aside at the beginning. we'll prepare  the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7aabe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepare eval data for prompt only formatting\n",
    "# records_prompt_only = []\n",
    "# with open('eval/questions.json', 'r') as f:\n",
    "#     eval_data = json.load(f)\n",
    "#     for item in eval_data:\n",
    "#         # print(item)\n",
    "#         record = create_jsonl_record(\n",
    "#             row=item,\n",
    "#             batch_inf_format=True\n",
    "#         )\n",
    "#         records_prompt_only.append(record)\n",
    "\n",
    "# # Save combined training data as JSONL\n",
    "# with open('eval/bedrock_eval_prompt_only.jsonl', 'w') as f:\n",
    "#     for record in records_prompt_only:\n",
    "#         f.write(json.dumps(record) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ec3bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepare eval data for tool config formatting\n",
    "# records_tool_config = []\n",
    "# with open('eval/questions.json', 'r') as f:\n",
    "#     eval_data = json.load(f)\n",
    "#     for item in eval_data:\n",
    "#         # print(item)\n",
    "#         record = create_jsonl_record(\n",
    "#             row=item,\n",
    "#             use_tool_config=True,\n",
    "#             batch_inf_format=True\n",
    "#         )\n",
    "#         records_tool_config.append(record)\n",
    "\n",
    "# # Save combined training data as JSONL\n",
    "# with open('eval/bedrock_eval_tool_config.jsonl', 'w') as f:\n",
    "#     for record in records_tool_config:\n",
    "#         f.write(json.dumps(record) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d1cf25",
   "metadata": {},
   "source": [
    "Let's also generate the answers for these eval questions so we'll be ready to evaluate the model response compared to the ground truth answer from BFCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eea5086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through questions in eval/questions.jsonl and find the corresponding answer and put into a jsonl file\n",
    "answer_files = [\n",
    "    'answers/BFCL_v3_simple.json',\n",
    "    'answers/BFCL_v3_multiple.json',\n",
    "    'answers/BFCL_v3_irrelevance.json',\n",
    "    'answers/BFCL_v3_live_relevance.json'  # Note the double dot in filename\n",
    "]\n",
    "\n",
    "# all_answers = []\n",
    "# for file in answer_files:\n",
    "#     print(\"reading... \", file)\n",
    "#     with open(file, 'r') as f:\n",
    "#         for answer in f.readlines():\n",
    "#             # print(question)\n",
    "#             all_answers.append(json.loads(answer))\n",
    "#         # questions = json.load(contents)\n",
    "#         # all_questions.extend(questions)\n",
    "\n",
    "# # Convert to DataFrame for easier manipulation\n",
    "# df_answers = pd.DataFrame(all_answers)\n",
    "# df_answers.to_json('eval/answers.json', orient='records', indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f81aee5",
   "metadata": {},
   "source": [
    "Here's we'll format our list of eval questions in the format specified for BFCL when running specific test cases: https://github.com/ShishirPatil/gorilla/blob/main/berkeley-function-call-leaderboard/README.md#selecting-specific-test-cases-with---run-ids\n",
    "\n",
    "We'll use this final in our final evaluation for our customized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9932c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def in_training_data_set(id, training_dataset_filepath) -> bool:\n",
    "    \"\"\"Checks if id is in training data so as to exclude from evaluation\"\"\"\n",
    "    return id in Path(training_dataset_filepath).read_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7650ce83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store categorized IDs\n",
    "from collections import defaultdict\n",
    "categorized_ids = defaultdict(list)\n",
    "\n",
    "# Process each answer file\n",
    "for file in answer_files:\n",
    "    print(f\"Reading... {file}\")\n",
    "    if os.path.exists(file):\n",
    "        with open(file, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    try:\n",
    "                        answer_data = json.loads(line)\n",
    "                        record_id = answer_data.get('id')\n",
    "                        if in_training_data_set(record_id, 'training/bedrock_training_data_tool_config.jsonl'):\n",
    "                            print(f\"Excluding record {record_id} from evaluation.\")\n",
    "                        else:\n",
    "                            if record_id:\n",
    "                                # Determine category based on ID prefix\n",
    "                                if record_id.startswith('simple_'):\n",
    "                                    categorized_ids['simple'].append(record_id)\n",
    "                                elif record_id.startswith('multiple_'):\n",
    "                                    categorized_ids['multiple'].append(record_id)\n",
    "                                elif record_id.startswith('live_relevance'):\n",
    "                                    categorized_ids['live_relevance'].append(record_id)\n",
    "                                elif record_id.startswith('irrelevance'): \n",
    "                                    categorized_ids['irrelevance'].append(record_id)\n",
    "                                    pass\n",
    "                                else:\n",
    "                                    # Handle any other categories\n",
    "                                    prefix = record_id.split('_')[0]\n",
    "                                    categorized_ids[prefix].append(record_id)\n",
    "                                \n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f\"Error parsing JSON in {file}: {e}\")\n",
    "    else:\n",
    "        print(f\"Warning: File {file} not found\")\n",
    "\n",
    "# Convert defaultdict to regular dict and sort IDs within each category\n",
    "result = {}\n",
    "for category, ids in categorized_ids.items():\n",
    "    result[category] = sorted(ids)\n",
    "\n",
    "# Write to JSON file\n",
    "output_file = 'test_case_ids_to_generate.json' # This will be used with BFCL\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(result, f, indent=2, sort_keys=True)\n",
    "\n",
    "print(f\"\\nCategorized IDs written to: {output_file}\")\n",
    "print(f\"Categories found: {list(result.keys())}\")\n",
    "\n",
    "for category, ids in result.items():\n",
    "    print(f\"  {category}: {len(ids)} IDs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404b30b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answers = []\n",
    "# with open('eval/questions.json', 'r') as f:\n",
    "#     eval_data = json.load(f)\n",
    "#     for row in eval_data:\n",
    "#         question_id = row['id']\n",
    "#         gt_answer = df_answers[df_answers['id'] == question_id]['ground_truth'].values\n",
    "#         answer_record = {'id': question_id, 'answer': gt_answer[0]}\n",
    "#         answers.append(answer_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac759b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"eval/answers.json\", \"w\") as output_file:\n",
    "#     for answer in answers:\n",
    "#         json.dump(answer, output_file)\n",
    "#         output_file.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195f4332",
   "metadata": {},
   "source": [
    "## Wrap-up\n",
    "We've now prepared two datasets for use in Bedrock Distillation, one using the prompt-only approach to tool calling, the other using the toolConfig parameter.\n",
    "You should also now have evaluation datasets with our hold out data we can use for making inferences to bedrock for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574f3c74",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that we have prepared our training data for the Bedrock distillation service, you can proceed to:\n",
    "\n",
    "1. **Model Training**: Use the generated `bedrock_training_data.jsonl` file to train your distilled model using the [Bedrock Model Distillation service](https://docs.aws.amazon.com/bedrock/latest/userguide/custom-models.html)\n",
    "\n",
    "2. **Evaluation**: Use the data in the `eval` directory to assess your model's performance on:\n",
    "   - Function selection accuracy\n",
    "   - Parameter extraction quality\n",
    "   - Handling of irrelevant queries\n",
    "   - Response format consistency\n",
    "\n",
    "3. **Fine-tuning**: Based on evaluation results, you may want to:\n",
    "   - Adjust the mix-in percentage (currently 10%)\n",
    "   - Modify the system prompt\n",
    "   - Enhance the training data with additional examples\n",
    "\n",
    "For more information on model distillation best practices, refer to the [Amazon Bedrock documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/custom-models-distillation.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17334b4f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "02_function_calling-tDu3jLPM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
