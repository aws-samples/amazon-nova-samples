{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "280d3606",
   "metadata": {},
   "source": [
    "# Model Distillation for Function Calling\n",
    "\n",
    "This notebook is part of a series demonstrating advanced model distillation techniques for creating specialized, function-calling-aware models. The goal is to distill the knowledge from a large language model (Amazon Nova Premier) into a smaller, more efficient model while maintaining high-quality function calling capabilities.\n",
    "\n",
    "## Learning Objectives\n",
    "- Prepare training data for function calling model distillation\n",
    "- Design structured output formats for consistent function parameter generation\n",
    "- Implement function selection and parameter extraction\n",
    "- Create evaluation datasets for measuring function calling accuracy\n",
    "\n",
    "## Dataset: Berkeley Function Calling Leaderboard (BFCL) V2 Live\n",
    "We use the Berkeley Function Calling Leaderboard (BFCL) V2 Live dataset as our base dataset. This dataset is particularly suitable for function-calling model training because:\n",
    "\n",
    "1. Contains 2,251 question-function-answer pairs total\n",
    "2. Provides diverse function calling scenarios:\n",
    "   - 258 simple calls\n",
    "   - 1,053 multiple parameter calls\n",
    "   - 16 parallel function calls\n",
    "   - 24 parallel multiple parameter calls\n",
    "   - 882 irrelevance detection cases\n",
    "   - 18 relevance detection cases\n",
    "3. Offers complexity with an average of 3 function choices per entry (maximum 37)\n",
    "4. Includes parameter diversity with an average of 4 parameters per function (maximum 28)\n",
    "\n",
    "The dataset is processed and stored in optimized formats for efficient model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3adadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upgrade boto3 \n",
    "%pip install --upgrade pip --quiet\n",
    "%pip install boto3 --upgrade --quiet\n",
    "%pip install bcfl-eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdea093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72c875c",
   "metadata": {},
   "source": [
    "We need to set the project root so results are put in the correct location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2b61383",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['BFCL_PROJECT_ROOT'] = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d244e4eb",
   "metadata": {},
   "source": [
    "If you're running this on your own machine, enter your AWS access keys in an .env file.\n",
    "Uncomment the below cell to copy down an example .env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b125616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # set up environment file\n",
    "# %cp $(python -c \"import bfcl_eval; print(bfcl_eval.__path__[0])\")/.env.example $BFCL_PROJECT_ROOT/.env\n",
    "# # Fill in necessary values in `.env`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8378ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download sample data\n",
    "# %cp $(python -c \"import bfcl_eval, pathlib; print(pathlib.Path(bfcl_eval.__path__[0]) / 'test_case_ids_to_generate.json.example')\") $BFCL_PROJECT_ROOT/test_case_ids_to_generate.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01188219",
   "metadata": {},
   "source": [
    "For this example we're going to use a combination of v3_simple, v3_multiple, v3_live_relevance, and v3_irrelevance to train and evaluate with. For more information on these categories and their intents, please visit the [official BFCL documentation](https://huggingface.co/datasets/gorilla-llm/Berkeley-Function-Calling-Leaderboard)\n",
    "\n",
    "Let's move these to our local directory so we can begin preparing the data for distillation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3d505320",
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir questions\n",
    "%cp $(python -c \"import bfcl_eval, pathlib; print(pathlib.Path(bfcl_eval.__path__[0]) / 'data' / 'BFCL_v3_simple.json')\") ./questions/BFCL_v3_simple.json\n",
    "%cp $(python -c \"import bfcl_eval, pathlib; print(pathlib.Path(bfcl_eval.__path__[0]) / 'data' / 'BFCL_v3_multiple.json')\") ./questions/BFCL_v3_multiple.json\n",
    "%cp $(python -c \"import bfcl_eval, pathlib; print(pathlib.Path(bfcl_eval.__path__[0]) / 'data' / 'BFCL_v3_irrelevance.json')\") ./questions/BFCL_v3_irrelevance.json\n",
    "%cp $(python -c \"import bfcl_eval, pathlib; print(pathlib.Path(bfcl_eval.__path__[0]) / 'data' / 'BFCL_v3_live_relevance.json')\") ./questions/BFCL_v3_live_relevance..json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2b47ba",
   "metadata": {},
   "source": [
    "Now will grab the corresponding answers. For the simple and multiple datasets, we are provided possible answers and we'll use these for our mix-in labels.\n",
    "\n",
    "Per the BFCL documentation, the correct answer for any question in the `BFCL_v3_irrelevance` datset is an empty list of functions, as these are design specifically to test the model's ability to correctly identify zero possible functions that are relevant.\n",
    "\n",
    "The correct answer for any question in the `BFCL_v3_live_relevance` dataset is \"at least one\" function call returned.\n",
    "\n",
    "Here's an excerpt from [their documentation](https://huggingface.co/datasets/gorilla-llm/Berkeley-Function-Calling-Leaderboard):\n",
    "\n",
    "> **Irrelevance Detection (875):** The scenario where none of the function choices provided are relevant to the user query and none should be invoked. We expect the model to not output a function call; the model can either output a message explaining why the function provided are not relevant or simply output a non-function call response (e.g., an empty list).\n",
    "\n",
    "> **Relevance Detection (41):** The opposite of irrelevance detection. The scenario where at least one of the function choices provided are relevant to the user query and should be invoked, but the way the user prompt or the function doc is stated means that there could be infinitely many correct function calls and impossible to use a pre-defined possible answer set to evaluate. We expect the model to output some function call (one or multiple) that is relevant to the user query; we don't check for the correctness of the function call in this category (eg, correct parameter value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d625a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir answers\n",
    "%cp $(python -c \"import bfcl_eval, pathlib; print(pathlib.Path(bfcl_eval.__path__[0]) / 'data' / 'possible_answer' / 'BFCL_v3_simple.json')\") ./answers/BFCL_v3_simple.json\n",
    "%cp $(python -c \"import bfcl_eval, pathlib; print(pathlib.Path(bfcl_eval.__path__[0]) / 'data' / 'possible_answer' / 'BFCL_v3_multiple.json')\") ./answers/BFCL_v3_multiple.json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662fd953",
   "metadata": {},
   "source": [
    "To make things a bit cleaner, we'll create answer files for the relevance and irrelevance datasets as well. This will make it easier to combine for our training dataset as our mix-in will require a few ground truth examples.\n",
    "We'll emulate the ground truth response structure the BFCL team is using, and return this for irrelevance answers:\n",
    "```json\n",
    "{\"id\": \"irrelevance_13\", \"ground_truth\": []}\n",
    "```\n",
    "\n",
    "and this for relevance answers, by picking a random function name from the list. Note that they \"don't check for the correctness of the function call in this category (eg, correct parameter value).\" so we'll use placeholders for the actual ground truth answers. Remember we're just doing this for labeled mix-in training data to hint our teacher model during distillation:\n",
    "```json\n",
    "{\"id\": \"live_relevance_5-5-0\", \"ground_truth\": [{\"get_copyright_info\": {\"copyright_content\": [\"The specific content that is claimed to be copyrighted.\"], \"copyright_holder\": [\"The name of the individual or organization that holds the copyright.\"], \"confidence_score\": [0.8]}}]}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340f0048",
   "metadata": {},
   "source": [
    "## Data Preparation Steps\n",
    "\n",
    "1. **Data Splitting**\n",
    "   - Split the BFCL question datasets randomly:\n",
    "     - 50% into `training` directory\n",
    "     - 50% into `eval` directory\n",
    "   \n",
    "2. **Mix-in Data Creation**\n",
    "   - From the training data:\n",
    "     - Create `mix_in` subdirectory\n",
    "     - Move 10% of records into mix_in\n",
    "     - Keep 90% in training\n",
    "   \n",
    "3. **Ground Truth Integration**\n",
    "   - For mix-in data:\n",
    "     - Look up corresponding answers in answer dataset\n",
    "     - Add as ground truth assistant responses\n",
    "   \n",
    "4. **Prompt Engineering**\n",
    "   - Build Bedrock invoke API prompts with tool calling functionality\n",
    "   \n",
    "5. **Data Consolidation**\n",
    "   - Combine all training data (including mix-in)\n",
    "   - Format as JSONL for Bedrock distillation service\n",
    "   - Save in training directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97899e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs('training', exist_ok=True)\n",
    "os.makedirs('eval', exist_ok=True)\n",
    "\n",
    "# Load and combine all question datasets\n",
    "question_files = [\n",
    "    'questions/BFCL_v3_simple.json',\n",
    "    'questions/BFCL_v3_multiple.json',\n",
    "    'questions/BFCL_v3_irrelevance.json',\n",
    "    'questions/BFCL_v3_live_relevance.json'  # Note the double dot in filename\n",
    "]\n",
    "\n",
    "all_questions = []\n",
    "for file in question_files:\n",
    "    with open(file, 'r') as f:\n",
    "        questions = json.load(f)\n",
    "        all_questions.extend(questions)\n",
    "\n",
    "# Convert to DataFrame for easier manipulation\n",
    "df_questions = pd.DataFrame(all_questions)\n",
    "\n",
    "# Randomly split into training (50%) and eval (50%)\n",
    "df_train = df_questions.sample(frac=0.5, random_state=42)\n",
    "df_eval = df_questions.drop(df_train.index)\n",
    "\n",
    "# Save splits to respective directories\n",
    "df_train.to_json('training/questions.json', orient='records', indent=2)\n",
    "df_eval.to_json('eval/questions.json', orient='records', indent=2)\n",
    "\n",
    "print(f\"Training set size: {len(df_train)}\")\n",
    "print(f\"Evaluation set size: {len(df_eval)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5355fa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mix_in directory\n",
    "os.makedirs('training/mix_in', exist_ok=True)\n",
    "\n",
    "# Select 10% of training data for mix-in\n",
    "df_mix_in = df_train.sample(frac=0.1, random_state=42)\n",
    "df_train_remaining = df_train.drop(df_mix_in.index)\n",
    "\n",
    "# Save mix-in and remaining training data\n",
    "df_mix_in.to_json('training/mix_in/questions.json', orient='records', indent=2)\n",
    "df_train_remaining.to_json('training/questions.json', orient='records', indent=2)\n",
    "\n",
    "print(f\"Mix-in set size: {len(df_mix_in)}\")\n",
    "print(f\"Remaining training set size: {len(df_train_remaining)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2aa386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load answer datasets\n",
    "answer_files = {\n",
    "    'simple': 'answers/BFCL_v3_simple.json',\n",
    "    'multiple': 'answers/BFCL_v3_multiple.json'\n",
    "}\n",
    "\n",
    "all_answers = {}\n",
    "for dataset_type, file in answer_files.items():\n",
    "    with open(file, 'r') as f:\n",
    "        answers = json.load(f)\n",
    "        all_answers.update({a['id']: a['ground_truth'] for a in answers})\n",
    "\n",
    "# Add empty list answers for irrelevance cases\n",
    "irrelevance_ids = df_mix_in[df_mix_in['id'].str.contains('irrelevance', na=False)]['id']\n",
    "for id in irrelevance_ids:\n",
    "    all_answers[id] = []\n",
    "\n",
    "# Add placeholder answers for relevance cases\n",
    "relevance_ids = df_mix_in[df_mix_in['id'].str.contains('live_relevance', na=False)]\n",
    "for _, row in relevance_ids.iterrows():\n",
    "    # Extract a random function from available choices\n",
    "    if row['function_list']:\n",
    "        func = np.random.choice(row['function_list'])\n",
    "        all_answers[row['id']] = [{func: {\"placeholder\": \"value\"}}]\n",
    "\n",
    "# Add ground truth answers to mix-in data\n",
    "df_mix_in['ground_truth'] = df_mix_in['id'].map(all_answers)\n",
    "df_mix_in.to_json('training/mix_in/questions_with_answers.json', orient='records', indent=2)\n",
    "\n",
    "print(f\"Added ground truth answers to {len(df_mix_in)} mix-in records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4943339e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt for function calling\n",
    "SYSTEM_PROMPT = \"\"\"You are an AI assistant that helps users by calling appropriate functions based on their requests. When a function is relevant:\n",
    "1. Select the most appropriate function from the available choices\n",
    "2. Extract required parameters from the user's query\n",
    "3. Return a JSON response with the function call details\n",
    "4. If no functions are relevant, return an empty list []\n",
    "\n",
    "Format your response as a JSON object with 'function_call' containing 'name' and 'parameters'.\"\"\"\n",
    "\n",
    "# Create JSONL records for all training data\n",
    "def create_training_records():\n",
    "    records = []\n",
    "    \n",
    "    # Process regular training data\n",
    "    with open('training/questions.json', 'r') as f:\n",
    "        train_data = json.load(f)\n",
    "        for item in train_data:\n",
    "            record = create_batch_inf_record(\n",
    "                row={'id': item['id'], 'context': '', 'question': item['question']},\n",
    "                system_prompt=SYSTEM_PROMPT,\n",
    "                include_answer=False\n",
    "            )\n",
    "            records.append(record)\n",
    "    \n",
    "    # Process mix-in data with ground truth answers\n",
    "    with open('training/mix_in/questions_with_answers.json', 'r') as f:\n",
    "        mix_in_data = json.load(f)\n",
    "        for item in mix_in_data:\n",
    "            record = create_batch_inf_record(\n",
    "                row={\n",
    "                    'id': item['id'],\n",
    "                    'context': '',\n",
    "                    'question': item['question'],\n",
    "                    'answers': {'text': json.dumps(item['ground_truth'])}\n",
    "                },\n",
    "                system_prompt=SYSTEM_PROMPT,\n",
    "                include_answer=True\n",
    "            )\n",
    "            records.append(record)\n",
    "    \n",
    "    # Save combined training data as JSONL\n",
    "    with open('training/training_data.jsonl', 'w') as f:\n",
    "        for record in records:\n",
    "            f.write(json.dumps(record) + '\\n')\n",
    "    \n",
    "    print(f\"Created {len(records)} training records in JSONL format\")\n",
    "\n",
    "# Generate the training data\n",
    "create_training_records()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574f3c74",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that we have prepared our training data for the Bedrock distillation service, you can proceed to:\n",
    "\n",
    "1. **Model Training**: Use the generated `training_data.jsonl` file to train your distilled model using the [Bedrock Model Distillation service](https://docs.aws.amazon.com/bedrock/latest/userguide/custom-models.html)\n",
    "\n",
    "2. **Evaluation**: Use the data in the `eval` directory to assess your model's performance on:\n",
    "   - Function selection accuracy\n",
    "   - Parameter extraction quality\n",
    "   - Handling of irrelevant queries\n",
    "   - Response format consistency\n",
    "\n",
    "3. **Fine-tuning**: Based on evaluation results, you may want to:\n",
    "   - Adjust the mix-in percentage (currently 10%)\n",
    "   - Modify the system prompt\n",
    "   - Enhance the training data with additional examples\n",
    "\n",
    "For more information on model distillation best practices, refer to the [Amazon Bedrock documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/custom-models-distillation.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
